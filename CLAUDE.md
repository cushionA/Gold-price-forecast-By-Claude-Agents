# Gold Price Prediction Model - Autonomous Agent System v5

## Startup Mode

The orchestrator (you) first runs `git pull` and reads `shared/state.json` on startup.

### Auto-Detection

```
1. git pull to get latest state
2. Check shared/state.json status
3. User instructions take priority if provided

status == "not_started"            â†’ Start from Phase 0
status == "in_progress"            â†’ Resume from resume_from agent
status == "waiting_training"       â†’ Fetch Kaggle training results â†’ evaluate
status == "waiting_user_input"     â†’ Waiting for user action. Await instructions
status == "paused_max_iterations"  â†’ Check pending_tasks, resume from resume_from
status == "completed"              â†’ Output final report
```

### User Instruction Examples

```
"Start the project"                â†’ Fresh start
"Resume from where we left off"    â†’ Auto-resume per state.json
"Resume real_rate from attempt 3"  â†’ Overwrite state.json, resume from specified point
"Build only the vix submodel"      â†’ Execute vix only
"Proceed to meta-model"            â†’ Jump to Phase 3
"Check training results"           â†’ Fetch Kaggle results â†’ evaluate
```

---

## Mission

Build a regression model to predict next-day gold return (%).
Increase information via 9 key features Ã— submodels, then integrate with a meta-model.

## Core Concepts

- Meta-model directly processes key features
- Submodels supplement context, state, and characteristics
- **Submodels do NOT predict gold prices** â€” they extract latent patterns (persistence, regime, mean-reversion probability, etc.) from the feature's underlying dynamics using unsupervised, semi-supervised, or supervised learning
- Submodel quality is measured by whether the output provides statistically significant information gain to the meta-model (Gate 2/3), not by the submodel's own loss
- Actively leverage multi-country data to maximize training samples

---

## Key Features (9)

| # | Feature | Source | Importance | Note |
|---|---------|--------|------------|------|
| 1 | Real Interest Rate (10Y TIPS) | FRED: DFII10 | Strongest negative correlation | Yield Curve(DGS10)ã¨ç›¸é–¢é«˜ã„ã€‚VIFæ³¨æ„ |
| 2 | Dollar Index (DXY) | Yahoo: DX-Y.NYB | USD-denominated inverse correlation | |
| 3 | VIX | FRED: VIXCLS | Risk-off indicator | |
| 4 | Gold Technicals | Yahoo: GC=F, GLD | Momentum / mean-reversion | |
| 5 | Cross-Asset | Yahoo: SI=F, HG=F, ^GSPC | Relative attractiveness | |
| 6 | Yield Curve | FRED: DGS10, DGS2 | Policy expectations | Real Rate(DFII10)ã¨ç›¸é–¢é«˜ã„ã€‚VIFæ³¨æ„ |
| 7 | ETF Flows (proxy) | Yahoo: GLD | Investor demand | çœŸã®ETFç´”æµå…¥å‡ºé¡ã¯å–å¾—å›°é›£ã€‚GLD volume + shares outstandingå¤‰åŒ–ç‡ã‚’ä»£ç†æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨ |
| 8 | Inflation Expectation | FRED: T10YIE | Real asset demand driver | 10Y Breakeven Inflation Rateã€‚TIPSåˆ©å›ã‚Šã«å«ã¾ã‚Œã‚‹ãŒç‹¬ç«‹ã—ãŸå¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ |
| 9 | CNY Demand Proxy | Yahoo: CNY=X | China demand (~30% of gold demand) | ä¸Šæµ·ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã®ä»£ç†ã¨ã—ã¦CNY/USDç‚ºæ›¿ã‚’ä½¿ç”¨ |

---

## Execution Architecture: Claude Code + Kaggle Separation

### Why Separate?

```
Claude Code = Local PC process. Stops when PC shuts down.
Kaggle      = Cloud. Training continues even when PC is off.
Git         = Bridge between the two.
```

### Role Assignment

| Task | Execution | PC Required? |
|------|-----------|-------------|
| Requirements (entrance) | Claude Code | Yes |
| Research (researcher) | Claude Code | Yes |
| Design (architect) | Claude Code | Yes |
| Data fetching (builder_data) | Claude Code | Yes |
| Data check (datachecker) | Claude Code | Yes |
| **Training script generation** (builder_model) | Claude Code | Yes |
| **Training execution** | **Kaggle** | **No** |
| Evaluation (evaluator) | Claude Code | Yes |
| Improvement planning | Claude Code | Yes |

### Typical Workflow

```
1. [PC on] Claude Code: Design â†’ Data fetch â†’ Check â†’ Training script generation
2. [PC on] Claude Code: Submit training Notebook via Kaggle API
3. [PC off OK] Kaggle: Cloud training execution (minutes to ~30min)
4. [PC on] Claude Code: "Resume from where we left off"
   â†’ git pull â†’ Fetch Kaggle results â†’ Evaluate â†’ Next iteration
```

---

## Kaggle Integration

### Kaggle Notebook Structure

builder_model auto-generates the following Notebook:

```
notebooks/
  â””â”€â”€ {feature}_{attempt}/
      â”œâ”€â”€ kernel-metadata.json    â† Kaggle API config
      â””â”€â”€ train.ipynb             â† Jupyter Notebook (self-contained training script)
```

**é‡è¦**: å…¨ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã§çµ±ä¸€Notebookã€Œ**Gold Model Training**ã€ï¼ˆID: `bigbigzabuton/gold-model-training`ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã®Notebookã«ã¯FRED_API_KEYãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚

### kernel-metadata.json

**CRITICAL: All notebooks MUST include the submodel dataset reference**

```json
{
  "id": "bigbigzabuton/gold-{feature}-{attempt}",
  "title": "Gold {Feature} Model - Attempt {attempt}",
  "code_file": "train.ipynb",
  "language": "python",
  "kernel_type": "notebook",
  "is_private": true,
  "enable_gpu": true,
  "enable_internet": true,
  "dataset_sources": ["bigbigzabuton/gold-prediction-submodels"],
  "competition_sources": [],
  "kernel_sources": []
}
```

**Notes**:
- `dataset_sources` MUST include `"bigbigzabuton/gold-prediction-submodels"` for all submodels and meta-models
- This dataset contains all pre-computed submodel outputs (vix.csv, technical.csv, etc.)
- Kaggle mounts this at `/kaggle/input/gold-prediction-submodels/`
- Missing this dataset will cause immediate runtime failure

### train.ipynb Design Principles

Training notebooks must be **self-contained** Jupyter Notebooks. No dependency on Claude Code or Kaggle:

```python
"""
Gold Prediction SubModel Training - {feature} Attempt {attempt}
Self-contained: Data fetch â†’ Preprocessing â†’ Training â†’ Evaluation â†’ Save results, all in this file
"""

# === 1. Libraries ===
import torch, torch.nn as nn, pandas as pd, numpy as np
import json, os
from datetime import datetime

# === 2. Data Fetching (direct API calls) ===
# Embed builder_data's verified data fetching code as-is
def fetch_data():
    ...
    return train_data, val_data, test_data

# === 3. Model Definition ===
# Implement architect's design directly
class SubModel(nn.Module):
    ...

# === 4. Training Loop ===
def train(model, train_data, val_data, config):
    ...
    return model, metrics

# === 5. Optuna HPO ===
def run_hpo(train_data, val_data):
    ...
    return best_params, best_value

# === 6. Main Execution ===
if __name__ == "__main__":
    train_data, val_data, test_data = fetch_data()
    best_params, _ = run_hpo(train_data, val_data)

    model = SubModel(**best_params)
    model, metrics = train(model, train_data, val_data, best_params)

    # Generate submodel output
    output = model.transform(full_data)

    # === 7. Save Results (Kaggle output) ===
    output.to_csv("submodel_output.csv")
    torch.save(model.state_dict(), "model.pt")

    with open("training_result.json", "w") as f:
        json.dump({
            "feature": "{feature}",
            "attempt": {attempt},
            "timestamp": datetime.now().isoformat(),
            "best_params": best_params,
            "metrics": metrics,
            "output_shape": list(output.shape),
            "output_columns": list(output.columns),
        }, f, indent=2)

    print("Training complete!")
```

### Kaggle API Commands

After builder_model generates the training script, the orchestrator executes:

```bash
# 1. Submit training Notebook
kaggle kernels push -p notebooks/{feature}_{attempt}/

# 2. Check status
kaggle kernels status {KAGGLE_USERNAME}/gold-{feature}-{attempt}
# â†’ "running" / "complete" / "error"

# 3. Fetch results (after complete)
kaggle kernels output {KAGGLE_USERNAME}/gold-{feature}-{attempt} \
  -p data/submodel_outputs/{feature}/

# 4. Place fetched files in designated paths
mv data/submodel_outputs/{feature}/submodel_output.csv \
   data/submodel_outputs/{feature}.csv
mv data/submodel_outputs/{feature}/model.pt \
   models/submodels/{feature}/model.pt
cp data/submodel_outputs/{feature}/training_result.json \
   logs/training/{feature}_{attempt}.json
```

### Training Wait State Management

Update state.json after submitting training so the PC can be shut down:

```json
{
  "status": "waiting_training",
  "resume_from": "evaluator",
  "kaggle_kernel": "{KAGGLE_USERNAME}/gold-{feature}-{attempt}",
  "submitted_at": "2025-01-22T10:00:00",
  "current_feature": "real_rate",
  "current_attempt": 1
}
```

Orchestrator behavior on resume:

```
1. git pull
2. Read state.json â†’ status == "waiting_training"
3. Check training completion via kaggle kernels status
   â†’ "running" â†’ Notify user "Training still in progress"
   â†’ "error" â†’ Fetch error log, return to builder_model
   â†’ "complete" â†’ Fetch results and pass to evaluator
```

### Full Automation System

The project supports two execution modes:

#### ğŸ¤– Full Auto Mode (Default)

**Orchestrator automatically starts background monitoring after Kaggle submission.**

```python
# Auto-started by orchestrator_kaggle_handler.py
# Features:
- Checks kernel status every 1 minute (max 3 hours)
- Downloads results when complete
- Commits & pushes to Git
- Automatically runs evaluator (inline, no Claude Code restart)
- Decides next action (attempt+1 / next feature / done)
- Error handling: auto-retry (network) / auto-skip (OOM)
- state.json auto-update for seamless resume
```

Orchestrator usage:
```python
from scripts.orchestrator_kaggle_handler import KaggleSubmissionHandler

handler = KaggleSubmissionHandler()
# Full auto mode (default)
handler.submit_and_exit(
    notebook_path='notebooks/real_rate_1/',
    feature='real_rate',
    attempt=1,
    auto_mode=True  # â† default
)
# â†’ Submits to Kaggle
# â†’ Starts auto_resume_after_kaggle.py in background
# â†’ Exits orchestrator (PC can be closed)
```

Manual script start (if needed):
```bash
python scripts/auto_resume_after_kaggle.py
```

#### ğŸ‘¤ Manual Mode

**User manually evaluates results after Kaggle completes.**

Orchestrator usage:
```python
handler.submit_and_exit(
    notebook_path='notebooks/real_rate_1/',
    feature='real_rate',
    attempt=1,
    auto_mode=False  # â† manual mode
)
# â†’ Submits to Kaggle
# â†’ NO background monitoring
# â†’ User says "Resume from where we left off" when ready
```

#### Command-line Usage

```bash
# Full auto mode (default)
python scripts/orchestrator_kaggle_handler.py notebooks/real_rate_1/ real_rate 1

# Manual mode (no auto-monitoring)
python scripts/orchestrator_kaggle_handler.py notebooks/real_rate_1/ real_rate 1 --manual

# Manual monitoring start (if auto-start failed)
python scripts/auto_resume_after_kaggle.py
```

#### Auto-Clean & Resume (`scripts/auto_clean_and_resume.py`)

Cleans context and resumes after evaluation:

```python
from scripts.auto_clean_and_resume import AutoCleanResume

handler = AutoCleanResume()
handler.execute_and_exit(
    feature='real_rate',
    attempt=1,
    decision='attempt+1'  # or 'no_further_improvement', 'success'
)
# â†’ Commits evaluation results
# â†’ Cleans Claude Code context
# â†’ Launches new session with fresh context
# â†’ Exits current session
```

#### Full Automation Flow (ğŸ¤– Auto Mode)

```
[PC on] builder_model: Generate Kaggle Notebook
  â†“
orchestrator: KaggleSubmissionHandler.submit_and_exit(auto_mode=True)
  - kaggle kernels push
  - Start auto_resume_after_kaggle.py (background)
  - git commit & push
  - exit(0)  â† orchestrator terminates
  â†“
[PC off OK] auto_resume_after_kaggle.py monitors every 1 min
  â†“
[Kaggle complete] auto_resume_after_kaggle.py detects completion
  - kaggle kernels output (download results)
  - git commit & push
  - run_evaluator_inline() (NO Claude Code restart)
  - Gate 1/2/3 evaluation
  - handle_evaluation_decision()
    â†’ success: move to next feature
    â†’ attempt+1: set resume_from=architect
    â†’ no_further_improvement: move to next feature
  - state.json auto-update
  - git commit & push
  â†“
[Continue] User says "Resume from where we left off"
  - orchestrator reads state.json
  - resumes from designated agent (architect/entrance)
  â†“
[Loop continues with fresh context]
```

#### Manual Flow (ğŸ‘¤ Manual Mode)

```
[PC on] builder_model: Generate Kaggle Notebook
  â†“
orchestrator: KaggleSubmissionHandler.submit_and_exit(auto_mode=False)
  - kaggle kernels push
  - git commit & push
  - NO background monitoring
  â†“
[Wait] User checks Kaggle web UI
  â†“
[Kaggle complete] User says "Resume from where we left off"
  - orchestrator fetches results
  - evaluator runs (Gate 1/2/3)
  - user reviews and continues manually
```

#### Benefits

âœ… **Zero manual intervention** in auto mode
âœ… **PC can be closed** during Kaggle training (monitor runs in background)
âœ… **Memory efficient** (context cleared after each evaluation)
âœ… **Error resilient** (3-hour timeout, error notifications)
âœ… **Git persistence** (all state saved, resumable anytime)

#### Configuration

No additional configuration needed. Scripts auto-detect:
- Project root from script location
- Kaggle kernel ID from state.json
- Feature/attempt from state.json
- Resume point from state.json

---

## Agent Architecture

```
Orchestrator (Sonnet)
  â”‚  * git commit & push after each agent completes
  â”‚  * Training submission/result fetching done directly by orchestrator
  â”‚
  â”œâ”€ entrance (Opus)          Initial requirements definition
  â”œâ”€ researcher (Sonnet)      Research (subject to fact-checking)
  â”œâ”€ architect (Opus)         Fact-check â†’ Design doc â†’ HP search space
  â”œâ”€ builder_data (Sonnet)    Data fetching & preprocessing
  â”œâ”€ datachecker (Haiku)      Standardized 7-step check
  â”œâ”€ builder_model (Sonnet)   PyTorch training script generation (for Kaggle) + Notebook validation
  â””â”€ evaluator (Opus)         Gate 1/2/3 â†’ Loop control â†’ Improvement plan
```

### Token Consumption

| Phase | Token Consumption |
|-------|-------------------|
| Design & code generation (Claude Code) | Yes |
| Kaggle training submission (API call) | Minimal |
| **Kaggle training execution** | **None (PC can be off)** |
| Result fetching & evaluation (Claude Code) | Yes |

---

## Workflow

### Phase 0: Environment Setup (first time only)

```
1. git init & remote setup (user prepares in advance)
2. Library installation:
   pip install torch pandas numpy scikit-learn xgboost optuna \
       yfinance fredapi matplotlib scipy statsmodels kaggle python-dotenv
3. Credential verification (stop with waiting_user_input if any are missing):
   a. Local environment variables (.env â†’ auto-loaded via python-dotenv):
      - FRED_API_KEY â†’ set in .env?
      - KAGGLE_USERNAME â†’ set in .env?
      - KAGGLE_API_TOKEN â†’ set in .env? (environment variable format, DO NOT use ~/.kaggle/kaggle.json)
   b. Kaggle CLI authentication:
      - KAGGLE_API_TOKEN is set and kaggle kernels list succeeds?
      - Note: Use environment variables ONLY. Delete ~/.kaggle/kaggle.json if it exists.
   c. Kaggle Secrets (required for API calls inside Kaggle Notebooks):
      - https://www.kaggle.com/settings â†’ Secrets â†’ FRED_API_KEY registered?
      - * User must configure this in browser
4. Create shared code:
   - src/submodel_base.py
   - src/data_fetcher.py
   - src/evaluation.py
   - src/utils.py
   - src/kaggle_runner.py  â† Common Kaggle submission/retrieval functions
5. Fetch base data:
   - Gold price GC=F â†’ data/raw/gold.csv
   - Raw data for 9 key features â†’ data/raw/
   - Target variable (next-day return %) â†’ data/processed/target.csv
6. git commit & push "phase0: environment setup and base data"
```

### Phase 1: Baseline Construction

```
1. Prepare direct input data for 9 key features â†’ data/processed/base_features.csv
2. Split data into train/val/test (70/15/15, time-series order)
3. Train XGBoost baseline (local execution, small enough to finish immediately)
4. Record baseline score in shared/baseline_score.json
5. Freeze base_features schema â†’ shared/schema_freeze.json
   (columns, dtypes, date_range, row_count â€” used for Gate 2/3 validation)
6. git commit & push "phase1: baseline (DA=xx%, Sharpe=x.xx)"
```

### Phase 1.5: Smoke Test

```
1. Run full pipeline once with real_rate (simplified version)
   - entrance â†’ researcher â†’ architect â†’ builder_data â†’ datachecker
   - builder_model â†’ Generate Kaggle Notebook (Optuna 5 trials)
   - Kaggle submission â†’ Fetch results â†’ evaluator (Gate 1 only)
2. Verify Kaggle integration works without errors
3. git commit & push "smoke_test: pipeline with kaggle verified"
```

### Phase 2: Submodel Construction Loop

```
[PC on] entrance/evaluator â†’ researcher â†’ architect â†’
        builder_data â†’ datachecker â†’ builder_model (script generation)
        â†’ Kaggle submission â†’ git push
[PC off OK] Kaggle training in progress
[PC on] "Resume" â†’ Fetch Kaggle results â†’ evaluator â†’ (loop or next)
```

### Phase 3: Meta-Model Construction

```
1. architect: Analyze all submodel output formats â†’ Select architecture
2. builder_model: Generate Kaggle Notebook
3. Kaggle training execution
4. evaluator: Evaluate against final target metrics
5. Improvement loop
```

---

## Phase 2 Pipeline Detail

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ entrance (first time) / evaluator (2nd+)     â”‚
â”‚  â†’ Requirements in current_task.json         â”‚
â”‚  â†’ git commit                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ researcher (Sonnet)                          â”‚
â”‚  â†’ Report in docs/research/                  â”‚
â”‚  â†’ git commit                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ architect (Opus)                             â”‚
â”‚  â†’ Fact-check                                â”‚
â”‚  â†’ Fail â†’ researcher re-investigation        â”‚
â”‚  â†’ Design doc in docs/design/                â”‚
â”‚  â†’ git commit                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ builder_data (Sonnet)                        â”‚
â”‚  â†’ Save data in data/                        â”‚
â”‚  â†’ git commit                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ datachecker (Haiku) 7-step standardized checkâ”‚
â”‚  â†’ REJECT â†’ Return to builder_data           â”‚
â”‚    (no attempt consumed, max 3 times)        â”‚
â”‚  â†’ 3 REJECTs â†’ Return to architect           â”‚
â”‚  â†’ PASS â†’ git commit, proceed               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ builder_model (Sonnet)                       â”‚
â”‚  â†’ Generate self-contained train.py          â”‚
â”‚  â†’ Generate kernel-metadata.json             â”‚
â”‚  â†’ **RUN VALIDATION** (scripts/validate_notebook.py) â”‚
â”‚    â€¢ Syntax check                            â”‚
â”‚    â€¢ Typo detection (.UPPER() etc)           â”‚
â”‚    â€¢ Compatibility warnings (SHAP+XGBoost)   â”‚
â”‚    â€¢ Dataset reference check                 â”‚
â”‚    â€¢ kernel-metadata.json validation         â”‚
â”‚  â†’ FAIL â†’ Fix and re-validate                â”‚
â”‚  â†’ PASS â†’ git commit                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestrator: Kaggle Submission              â”‚
â”‚  â†’ kaggle kernels push                       â”‚
â”‚  â†’ Update state.json to "waiting_training"   â”‚
â”‚  â†’ git push                                  â”‚
â”‚  â†’ â˜… User can shut down PC â˜…                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (On PC restart) Orchestrator: Fetch Results  â”‚
â”‚  â†’ git pull                                  â”‚
â”‚  â†’ Check via kaggle kernels status           â”‚
â”‚  â†’ Fetch via kaggle kernels output           â”‚
â”‚  â†’ git commit                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ evaluator (Opus)                             â”‚
â”‚  â†’ Gate 1 â†’ 2 â†’ 3                           â”‚
â”‚  â†’ Pass â†’ Append to completed, next submodel â”‚
â”‚  â†’ Fail â†’ attempt+1, improvement plan, loop  â”‚
â”‚  â†’ No improvement possible â†’ next submodel   â”‚
â”‚  â†’ git commit & push                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Loop Control

### Attempt Consumption Rules

```
Consumed (+1): evaluator completes Gate evaluation â†’ fail
Not consumed:  datachecker return / architect return / researcher re-investigation
```

### Termination Conditions

1. Gate 3 pass
2. No improvement possible (3 consecutive delta<0.001 or evaluator declaration)
3. attempt >= 5 (paused, can be resumed later)

---

## Tech Stack

| Purpose | Library |
|---------|---------|
| Submodels & meta-model | PyTorch |
| HP optimization | Optuna |
| Data processing | pandas, numpy |
| Gate 3 baseline comparison | XGBoost |
| Evaluation | scikit-learn |
| Cloud training | Kaggle API |

### HP Management Roles

| Role | Owner |
|------|-------|
| Search space design | architect |
| Search execution | builder_model (Optuna in train.py) |
| Result evaluation | evaluator |
| Range adjustment | evaluator â†’ architect |

### Meta-Model Selection Criteria

architect analyzes input formats and selects optimal model:

| Input Format | Recommended |
|-------------|-------------|
| Continuous only <30 dims | MLP |
| Continuous + categorical | FT-Transformer / TabNet |
| Time-series patterns important | GRU + MLP |
| >50 dims, few samples | MLP + strong Dropout / dim reduction |

---

## Multi-Country Data Strategy

Actively leverage large-scale data. Maximizing training samples via multi-country data is key to performance improvement.

| Feature | Extended Data | Expected Samples | Note |
|---------|--------------|-----------------|------|
| Real Interest Rate | G10 countries | ~50,000 (realistic) | FREDã§ã®åˆ©ç”¨å¯èƒ½ã‚·ãƒªãƒ¼ã‚ºã¯é™å®šçš„ã€‚architectè¦ç¢ºèª |
| DXY | 12 currency pairs | ~60,000 | |
| VIX | Country-specific vol indices | ~25,000 | |
| Cross-Asset | Multi-commodity correlations | ~30,000 | |

Rules:
- Limited to G10 + major developed countries
- Do not use levels; use changes directly
- Normalize volatility by domestic long-term average
- architect must verify actual FRED series availability before design (not all G10 have TIPS-equivalent data)

---

## Evaluation Framework

### Data Split
- train/val/test = 70/15/15 (time-series order, no shuffle)
- HPO and model selection use train+val only
- Gate 3 and final meta-model evaluation use test set

### Gate 1: Standalone Quality
Overfit ratio <1.5 (same epoch comparison), no all-NaN/constant output, no leak indicators

### Gate 2: Information Gain
MI total increase >5% (sum-based, not mean), VIF <10, rolling correlation std <0.15

### Gate 3: Ablation (any one of the following)
Direction accuracy +0.5%, Sharpe +0.05 (after transaction costs), MAE -0.01%

### Sharpe Calculation Rules
- Transaction cost: 5bps per trade (one-way) deducted from strategy returns
- Direction sign: returns exactly 0 are excluded from direction accuracy calculation (np.sign(0) = 0 problem)

### Meta-Model Final Targets

| Metric | Target |
|--------|--------|
| Direction Accuracy | > 56% |
| High-Confidence Direction Accuracy | > 60% |
| MAE | < 0.75% |
| Sharpe Ratio (after costs) | > 0.8 |

---

## Git Persistence

### Branch Strategy

**develop branch (daily work):**
- All agent cycles: entrance â†’ researcher â†’ ... â†’ evaluator
- Trials, debugging, file organization
- Orchestrator auto-commits after each agent

**main branch (milestones only):**
- Submodel completion (Gate 3 PASS)
- Meta-model completion
- Environment setup completion
- Critical feature additions

### Commit Rules

**On develop branch** (orchestrator runs after each agent):

```bash
git add -A && git commit && git push origin develop
```

Commit messages:
```
entrance done     â†’ "entrance: {feature} attempt {N}"
researcher done   â†’ "research: {feature} attempt {N}"
architect done    â†’ "design: {feature} attempt {N}"
builder_data done â†’ "data: {feature} attempt {N}"
datachecker done  â†’ "datacheck: {feature} attempt {N} - {PASS/REJECT}"
builder_model done â†’ "model: {feature} attempt {N} - notebook generated"
kaggle submit     â†’ "kaggle: {feature} attempt {N} - submitted"
kaggle fetch      â†’ "kaggle: {feature} attempt {N} - results fetched"
evaluator done    â†’ "eval: {feature} attempt {N} - gate{N} {pass/fail}"
cleanup/refactor  â†’ "cleanup: {description}" or "refactor: {description}"
```

**On main branch** (manual or automated on Gate 3 PASS):

```bash
git checkout main
git merge develop --no-ff -m "feat: complete {feature} submodel (metrics)"
git push origin main
git checkout develop
```

Merge commit messages (Conventional Commits):
```
feat: complete {feature} submodel (Gate 3 PASS, Sharpe +X.XX, MAE -X.XX)
feat: complete meta-model (DA=XX%, Sharpe=X.XX)
feat: add {feature_name}
refactor: {major_refactoring}
```

**Current branch:** Check `shared/state.json` â†’ `git_branch` field

See `docs/knowledge/GIT_WORKFLOW.md` for detailed workflow.

---

## Shared Workspace

```
shared/
  â”œâ”€â”€ state.json              Progress state, resume point, Kaggle state
  â”œâ”€â”€ current_task.json       Current iteration requirements
  â”œâ”€â”€ improvement_queue.json  Improvement task queue
  â””â”€â”€ completed.json          Completed submodel records
```

---

## Project Structure

```
gold-prediction-agent/
â”œâ”€â”€ CLAUDE.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .claude/agents/           7 agents
â”œâ”€â”€ shared/                   State management
â”œâ”€â”€ src/                      Shared code
â”‚   â”œâ”€â”€ submodel_base.py
â”‚   â”œâ”€â”€ data_fetcher.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ kaggle_runner.py      â† Kaggle API operations
â”œâ”€â”€ notebooks/                â† Kaggle Notebooks (auto-generated)
â”‚   â””â”€â”€ {feature}_{attempt}/
â”‚       â”œâ”€â”€ kernel-metadata.json
â”‚       â””â”€â”€ train.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ multi_country/
â”‚   â””â”€â”€ submodel_outputs/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ submodels/{feature}/
â”‚   â””â”€â”€ meta/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ research/
â”‚   â””â”€â”€ design/
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ datacheck/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ iterations/
â”‚   â””â”€â”€ training/
â””â”€â”€ config/settings.yaml
```

---

## Prohibited Actions

1. No random splits (time-series splits only)
2. No future information leakage
3. No data with >5 day delay
4. Submodels must NOT directly predict gold prices
5. No hardcoded API keys or credentials (including train.py)
6. Do not execute processes requiring paid services or new API keys without explicit user approval

## File Generation Policy

**DO NOT create the following files** (they are redundant and waste resources):

1. âŒ `notebooks/**/README.md` - Duplicates design documents in `docs/design/`
2. âŒ `data/**/metadata.json` or `*_metadata.json` - Not used after datachecker passes
3. âŒ `docs/data/*_summary.md` - Duplicates design documents
4. âŒ `temp_*/` directories - Use proper output directories instead

**DO create**:
- âœ… `logs/evaluation/*_summary.md` - User needs these for review
- âœ… Design docs in `docs/design/`
- âœ… Research reports in `docs/research/`
- âœ… Evaluation logs in `logs/evaluation/*.json`

All agents must follow this policy to avoid creating unnecessary files.

---

## APIs

| API | Purpose | Auth | Local | Inside Kaggle Notebook |
|-----|---------|------|-------|----------------------|
| yfinance | Price data | None | â€” | â€” |
| fredapi | Economic indicators | FRED_API_KEY | .env | Kaggle Secrets |
| kaggle CLI | Training submission/retrieval | KAGGLE_API_TOKEN | .env | â€” (CLI is local only) |
| CNN Fear & Greed | Risk indicator | None | â€” | â€” |
| CBOE Put/Call | Risk indicator | None | â€” | â€” |
| GPR Index | Geopolitical indicator | None | â€” | â€” |

### Credential Management Principles

- Local: .env file (gitignored) â†’ auto-loaded via python-dotenv
- Kaggle CLI: KAGGLE_USERNAME + KAGGLE_API_TOKEN env vars (via .env)
- **CRITICAL**: DO NOT use ~/.kaggle/kaggle.json. Delete it if exists. Use environment variables ONLY.
- Kaggle Notebook: Kaggle Secrets (configured in browser)
- Inside train.py: os.environ['FRED_API_KEY'] (fail immediately with KeyError)
- **Never use hardcoded values or fallback defaults**
