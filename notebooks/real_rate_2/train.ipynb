{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel Training - real_rate Attempt 2\n",
    "\n",
    "**Feature**: real_rate (DFII10 - 10Y TIPS Yield)  \n",
    "**Attempt**: 2  \n",
    "**Architecture**: GRU Autoencoder (Sequence-to-Sequence)  \n",
    "**Generated by**: builder_model agent  \n",
    "**Date**: 2026-02-14\n",
    "\n",
    "## Key Changes from Attempt 1\n",
    "\n",
    "- Architecture: MLP → **GRU Autoencoder**\n",
    "- Latent dimensions: 4 → **2** (tighter compression)\n",
    "- Dropout: 0.13 → **0.3-0.5** (stronger regularization)\n",
    "- Weight decay: 1e-6 → **1e-4 to 1e-2** (100-10000x stronger)\n",
    "- Window size: 20 → **40-80** (longer context)\n",
    "- Optuna trials: 5 → **20** (better exploration)\n",
    "- **New**: First-difference postprocessing to break autocorr >0.99\n",
    "\n",
    "## Expected Improvements\n",
    "\n",
    "- Overfit ratio: 2.69 → **1.1-1.4**\n",
    "- Autocorrelation: >0.995 → **<0.5** (after differencing)\n",
    "- Gate 2 MI increase: maintained at **15-20%**\n",
    "- Gate 3: Direction accuracy **+0.3-0.5%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration (CPU for this task)\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Data Fetching (Self-Contained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_real_rate_features():\n",
    "    \"\"\"\n",
    "    Self-contained data fetching and feature engineering.\n",
    "    Embedded from scripts/fetch_real_rate_features.py\n",
    "    Returns: DataFrame with 8 engineered features\n",
    "    \"\"\"\n",
    "    # Install dependencies if needed\n",
    "    try:\n",
    "        from fredapi import Fred\n",
    "    except ImportError:\n",
    "        print(\"Installing fredapi...\")\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"fredapi\"], check=True)\n",
    "        from fredapi import Fred\n",
    "    \n",
    "    try:\n",
    "        import yfinance as yf\n",
    "    except ImportError:\n",
    "        print(\"Installing yfinance...\")\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"yfinance\"], check=True)\n",
    "        import yfinance as yf\n",
    "    \n",
    "    # === Load API key from Kaggle Secrets ===\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        api_key = secrets.get_secret(\"FRED_API_KEY\")\n",
    "        print(\"Loaded FRED_API_KEY from Kaggle Secrets\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load FRED_API_KEY from Kaggle Secrets: {e}\")\n",
    "        # Fallback to environment variable (for local testing)\n",
    "        api_key = os.environ.get('FRED_API_KEY')\n",
    "        if api_key is None:\n",
    "            raise RuntimeError(\n",
    "                \"FRED_API_KEY not found. Please add it to Kaggle Secrets or environment.\"\n",
    "            )\n",
    "    \n",
    "    fred = Fred(api_key=api_key)\n",
    "    \n",
    "    # === Fetch DFII10 from FRED ===\n",
    "    print(\"Fetching DFII10 from FRED...\")\n",
    "    # Fetch from 2013-06-01 to allow 252-day rolling windows before schema start (2015-01-30)\n",
    "    series = fred.get_series('DFII10', observation_start='2013-06-01')\n",
    "    \n",
    "    df = pd.DataFrame({'level': series})\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.index.name = 'Date'\n",
    "    \n",
    "    # Drop NaN rows (holidays)\n",
    "    df = df.dropna()\n",
    "    print(f\"FRED data shape after dropping NaN: {df.shape}\")\n",
    "    \n",
    "    # === Align with gold trading days ===\n",
    "    print(\"Fetching gold trading days from yfinance...\")\n",
    "    gold = yf.download('GC=F', start='2013-06-01', progress=False)\n",
    "    gold_dates = pd.DatetimeIndex(gold.index)\n",
    "    \n",
    "    # Reindex to gold trading days and forward-fill (max 5 days)\n",
    "    df = df.reindex(gold_dates)\n",
    "    df['level'] = df['level'].ffill(limit=5)\n",
    "    df = df.dropna()\n",
    "    print(f\"Data shape after alignment to gold trading days: {df.shape}\")\n",
    "    \n",
    "    # === Feature Engineering ===\n",
    "    print(\"Computing 8 hand-crafted features...\")\n",
    "    \n",
    "    # Feature 2: Daily change\n",
    "    df['change_1d'] = df['level'].diff()\n",
    "    \n",
    "    # Rolling std for normalization (60-day window)\n",
    "    rolling_std_60d = df['change_1d'].rolling(60, min_periods=60).std()\n",
    "    \n",
    "    # Feature 3: Velocity 20-day (normalized)\n",
    "    df['velocity_20d'] = (df['level'] - df['level'].shift(20)) / rolling_std_60d\n",
    "    \n",
    "    # Feature 4: Velocity 60-day (normalized)\n",
    "    df['velocity_60d'] = (df['level'] - df['level'].shift(60)) / rolling_std_60d\n",
    "    \n",
    "    # Feature 5: Acceleration (change in 20-day velocity)\n",
    "    df['accel_20d'] = df['velocity_20d'] - df['velocity_20d'].shift(20)\n",
    "    \n",
    "    # Feature 6: Rolling std 20-day\n",
    "    df['rolling_std_20d'] = df['change_1d'].rolling(20, min_periods=20).std()\n",
    "    \n",
    "    # Feature 7: Regime percentile (252-day rolling percentile rank)\n",
    "    def percentile_rank(x):\n",
    "        if len(x) < 2:\n",
    "            return np.nan\n",
    "        return percentileofscore(x, x.iloc[-1]) / 100.0\n",
    "    \n",
    "    df['regime_percentile'] = df['level'].rolling(252, min_periods=252).apply(percentile_rank, raw=False)\n",
    "    \n",
    "    # Feature 8: Autocorrelation of daily changes (60-day window, lag 1)\n",
    "    def rolling_autocorr(x):\n",
    "        if len(x) < 2:\n",
    "            return np.nan\n",
    "        try:\n",
    "            return pd.Series(x).autocorr(lag=1)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    df['autocorr_20d'] = df['change_1d'].rolling(60, min_periods=60).apply(rolling_autocorr, raw=False)\n",
    "    \n",
    "    # Drop rows with NaN from rolling calculations\n",
    "    initial_rows = len(df)\n",
    "    df = df.dropna()\n",
    "    print(f\"Dropped {initial_rows - len(df)} rows due to rolling window NaN\")\n",
    "    \n",
    "    # === Align to schema date range ===\n",
    "    schema_start = '2015-01-30'\n",
    "    schema_end = '2025-02-12'\n",
    "    \n",
    "    # First check if we have data covering the schema range\n",
    "    if df.index.min() > pd.Timestamp(schema_start):\n",
    "        raise ValueError(f\"Insufficient data buffer. First valid date: {df.index.min()}, \"\n",
    "                        f\"but schema starts at {schema_start}\")\n",
    "    \n",
    "    df = df.loc[schema_start:schema_end]\n",
    "    \n",
    "    # Verify NO NaN in final output\n",
    "    if df.isna().any().any():\n",
    "        nan_count = df.isna().sum().sum()\n",
    "        print(f\"WARNING: {nan_count} NaN values remain in schema range\")\n",
    "        print(\"Dropping rows with NaN...\")\n",
    "        df = df.dropna()\n",
    "    \n",
    "    print(f\"Final shape after schema alignment: {df.shape}\")\n",
    "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"NaN count: {df.isna().sum().sum()}\")\n",
    "    \n",
    "    # Verify we have 8 feature columns\n",
    "    feature_cols = ['level', 'change_1d', 'velocity_20d', 'velocity_60d',\n",
    "                    'accel_20d', 'rolling_std_20d', 'regime_percentile', 'autocorr_20d']\n",
    "    assert all(col in df.columns for col in feature_cols), \"Missing feature columns\"\n",
    "    \n",
    "    return df[feature_cols]\n",
    "\n",
    "# Fetch data\n",
    "print(\"=\" * 60)\n",
    "print(\"Real Rate Feature Fetching - Attempt 2\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "full_data = fetch_real_rate_features()\n",
    "print(f\"\\nFetched data shape: {full_data.shape}\")\n",
    "print(f\"Columns: {list(full_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Dataset Class for GRU Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for GRU autoencoder input.\n",
    "    Returns windows with shape [seq_len, n_features] (NOT flattened).\n",
    "    \n",
    "    This is different from Attempt 1 MLP which flattened to [seq_len * n_features].\n",
    "    GRU processes the temporal sequence natively.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_array, window_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_array: numpy array of shape [total_samples, n_features]\n",
    "            window_size: int, length of sliding window\n",
    "        \"\"\"\n",
    "        self.data = torch.FloatTensor(data_array)\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return window with shape [window_size, n_features]\n",
    "        # NO flattening (unlike MLP Attempt 1)\n",
    "        window = self.data[idx:idx + self.window_size]\n",
    "        return window\n",
    "\n",
    "print(\"Dataset class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: GRU Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealRateGRUAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-based autoencoder for real rate temporal dynamics.\n",
    "    \n",
    "    Input: [batch, seq_len, n_features=8]\n",
    "    Latent: [batch, latent_dim=2]\n",
    "    Output: [batch, seq_len, n_features=8]\n",
    "    \n",
    "    Changes from Attempt 1 MLP:\n",
    "    - Sequential processing (no flattening)\n",
    "    - Tighter bottleneck (latent_dim=2 vs 4)\n",
    "    - Separate dropout layers (GRU dropout only between layers)\n",
    "    - Decoder GRU reconstructs sequence from compressed state\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=8, gru_hidden_dim=32, gru_num_layers=1,\n",
    "                 latent_dim=2, dropout=0.3, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        self.gru_num_layers = gru_num_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # --- ENCODER ---\n",
    "        # GRU dropout only applies between layers; for single layer, set to 0\n",
    "        # This is a PyTorch gotcha: dropout parameter is ignored for num_layers=1\n",
    "        gru_dropout = dropout if gru_num_layers > 1 else 0.0\n",
    "        \n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=n_features,\n",
    "            hidden_size=gru_hidden_dim,\n",
    "            num_layers=gru_num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=gru_dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Post-GRU dropout (applies to final hidden state)\n",
    "        self.encoder_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Compress to latent space\n",
    "        encoder_output_dim = gru_hidden_dim * self.num_directions\n",
    "        self.encoder_fc = nn.Linear(encoder_output_dim, latent_dim)\n",
    "        \n",
    "        # --- DECODER ---\n",
    "        self.decoder_fc = nn.Linear(latent_dim, gru_hidden_dim)\n",
    "        self.decoder_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=gru_hidden_dim,\n",
    "            hidden_size=gru_hidden_dim,\n",
    "            num_layers=1,  # Always 1 layer for decoder\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.decoder_output = nn.Linear(gru_hidden_dim, n_features)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode sequential input to latent space.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, n_features]\n",
    "        Returns:\n",
    "            z: [batch, latent_dim]\n",
    "        \"\"\"\n",
    "        # GRU encoding\n",
    "        _, hidden = self.encoder_gru(x)\n",
    "        # hidden: [num_layers*num_dir, batch, gru_hidden_dim]\n",
    "        \n",
    "        # Take the last layer's hidden state\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward final hidden states\n",
    "            h_forward = hidden[-2]   # [batch, gru_hidden_dim]\n",
    "            h_backward = hidden[-1]  # [batch, gru_hidden_dim]\n",
    "            h = torch.cat([h_forward, h_backward], dim=-1)\n",
    "        else:\n",
    "            h = hidden[-1]  # [batch, gru_hidden_dim]\n",
    "        \n",
    "        # Dropout + compress to latent\n",
    "        h = self.encoder_dropout(h)\n",
    "        z = torch.tanh(self.encoder_fc(h))\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        Decode latent representation to sequence.\n",
    "        \n",
    "        Args:\n",
    "            z: [batch, latent_dim]\n",
    "            seq_len: int (original sequence length to reconstruct)\n",
    "        Returns:\n",
    "            reconstruction: [batch, seq_len, n_features]\n",
    "        \"\"\"\n",
    "        # Expand latent to GRU input dimension\n",
    "        h = torch.relu(self.decoder_fc(z))\n",
    "        h = self.decoder_dropout(h)\n",
    "        \n",
    "        # Repeat across time steps\n",
    "        decoder_input = h.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Decode sequence\n",
    "        decoder_output, _ = self.decoder_gru(decoder_input)\n",
    "        reconstruction = self.decoder_output(decoder_output)\n",
    "        return reconstruction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass for training.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, n_features]\n",
    "        Returns:\n",
    "            reconstruction: [batch, seq_len, n_features]\n",
    "            z: [batch, latent_dim]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        z = self.encode(x)\n",
    "        reconstruction = self.decode(z, seq_len)\n",
    "        return reconstruction, z\n",
    "    \n",
    "    def transform(self, x):\n",
    "        \"\"\"\n",
    "        Generate latent features for inference (no gradient).\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, n_features]\n",
    "        Returns:\n",
    "            z: [batch, latent_dim]\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            z = self.encode(x)\n",
    "        return z\n",
    "\n",
    "print(\"GRU Autoencoder model class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, verbose=False):\n",
    "    \"\"\"\n",
    "    Train the GRU autoencoder with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: RealRateGRUAutoencoder instance\n",
    "        train_loader: DataLoader for training\n",
    "        val_loader: DataLoader for validation\n",
    "        config: dict with training hyperparameters\n",
    "        verbose: bool, whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        model: trained model (best checkpoint restored)\n",
    "        metrics: dict with training metrics\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        patience=7,\n",
    "        factor=0.5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    max_epochs = config.get('max_epochs', 150)\n",
    "    early_stop_patience = config.get('early_stop_patience', 15)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # === TRAIN ===\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, _ = model(batch)\n",
    "            loss = F.mse_loss(reconstruction, batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (essential for GRU stability)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # === VALIDATE ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                reconstruction, _ = model(batch)\n",
    "                loss = F.mse_loss(reconstruction, batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Overfit ratio for monitoring\n",
    "        overfit_ratio = val_loss / (train_loss + 1e-10)\n",
    "        \n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}, \"\n",
    "                  f\"overfit_ratio={overfit_ratio:.3f}\")\n",
    "        \n",
    "        # === EARLY STOPPING ===\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_train_loss = train_loss  # Record train_loss at same epoch for fair comparison\n",
    "            patience_counter = 0\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    # Restore best weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, {\n",
    "        'train_loss': best_train_loss,\n",
    "        'val_loss': best_val_loss,\n",
    "        'overfit_ratio': best_val_loss / (best_train_loss + 1e-10),\n",
    "        'epochs_trained': epoch + 1\n",
    "    }\n",
    "\n",
    "print(\"Training function defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Optuna HPO Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hpo(train_data, val_data, n_trials=20, timeout=1800):\n",
    "    \"\"\"\n",
    "    Run Optuna hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        train_data: numpy array [n_samples, n_features]\n",
    "        val_data: numpy array [n_samples, n_features]\n",
    "        n_trials: number of Optuna trials\n",
    "        timeout: timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "        best_params: dict with best hyperparameters\n",
    "        best_value: best validation loss\n",
    "        n_completed: number of completed trials\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        # === HYPERPARAMETER SEARCH SPACE ===\n",
    "        window_size = trial.suggest_categorical('window_size', [40, 60, 80])\n",
    "        gru_hidden_dim = trial.suggest_categorical('gru_hidden_dim', [16, 32, 64])\n",
    "        gru_num_layers = trial.suggest_categorical('gru_num_layers', [1, 2])\n",
    "        dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n",
    "        bidirectional = trial.suggest_categorical('bidirectional', [True, False])\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "        \n",
    "        # === CREATE DATASETS ===\n",
    "        train_dataset = SlidingWindowDataset(train_data, window_size)\n",
    "        val_dataset = SlidingWindowDataset(val_data, window_size)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,  # Shuffle batches (temporal order within window preserved)\n",
    "            drop_last=True  # Avoid batch size 1\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        # === CREATE MODEL ===\n",
    "        model = RealRateGRUAutoencoder(\n",
    "            n_features=8,\n",
    "            gru_hidden_dim=gru_hidden_dim,\n",
    "            gru_num_layers=gru_num_layers,\n",
    "            latent_dim=2,  # FIXED at 2\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional\n",
    "        ).to(device)\n",
    "        \n",
    "        # === TRAIN ===\n",
    "        config = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'max_epochs': 150,\n",
    "            'early_stop_patience': 15\n",
    "        }\n",
    "        \n",
    "        _, metrics = train_model(model, train_loader, val_loader, config, verbose=False)\n",
    "        \n",
    "        # Prune if overfitting is severe (>2.0)\n",
    "        if metrics['overfit_ratio'] > 2.0:\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        return metrics['val_loss']\n",
    "    \n",
    "    # === RUN STUDY ===\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=TPESampler(seed=42),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=15)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    \n",
    "    return study.best_params, study.best_value, len(study.trials)\n",
    "\n",
    "print(\"HPO function defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MAIN EXECUTION: Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# === 1. PREPARE DATA ===\n",
    "print(\"\\n[1/7] Preparing data...\")\n",
    "\n",
    "# Split into train/val/test (70/15/15, chronological)\n",
    "n = len(full_data)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "train_df = full_data.iloc[:train_end]\n",
    "val_df = full_data.iloc[train_end:val_end]\n",
    "test_df = full_data.iloc[val_end:]\n",
    "\n",
    "print(f\"Train: {len(train_df)} samples ({train_df.index.min()} to {train_df.index.max()})\")\n",
    "print(f\"Val:   {len(val_df)} samples ({val_df.index.min()} to {val_df.index.max()})\")\n",
    "print(f\"Test:  {len(test_df)} samples ({test_df.index.min()} to {test_df.index.max()})\")\n",
    "\n",
    "# === 2. STANDARDIZE FEATURES ===\n",
    "print(\"\\n[2/7] Standardizing features (fit on train only)...\")\n",
    "\n",
    "# Compute mean/std from train split ONLY\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "# Apply to all splits\n",
    "train_data = ((train_df - train_mean) / train_std).values\n",
    "val_data = ((val_df - train_mean) / train_std).values\n",
    "test_data = ((test_df - train_mean) / train_std).values\n",
    "full_data_normalized = ((full_data - train_mean) / train_std).values\n",
    "\n",
    "print(f\"Train mean range: [{train_mean.min():.4f}, {train_mean.max():.4f}]\")\n",
    "print(f\"Train std range:  [{train_std.min():.4f}, {train_std.max():.4f}]\")\n",
    "\n",
    "# === 3. HYPERPARAMETER OPTIMIZATION ===\n",
    "print(\"\\n[3/7] Running Optuna HPO (20 trials, 30 min timeout)...\")\n",
    "print(\"This may take 15-25 minutes...\")\n",
    "\n",
    "best_params, best_value, n_completed = run_hpo(\n",
    "    train_data,\n",
    "    val_data,\n",
    "    n_trials=20,\n",
    "    timeout=1800\n",
    ")\n",
    "\n",
    "print(f\"\\nOptuna completed {n_completed} trials\")\n",
    "print(f\"Best validation loss: {best_value:.6f}\")\n",
    "print(f\"Best hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# === 4. TRAIN FINAL MODEL ===\n",
    "print(\"\\n[4/7] Training final model with best hyperparameters...\")\n",
    "\n",
    "window_size = best_params['window_size']\n",
    "batch_size = best_params['batch_size']\n",
    "\n",
    "# Create datasets with best window size\n",
    "train_dataset = SlidingWindowDataset(train_data, window_size)\n",
    "val_dataset = SlidingWindowDataset(val_data, window_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# Create model with best hyperparameters\n",
    "final_model = RealRateGRUAutoencoder(\n",
    "    n_features=8,\n",
    "    gru_hidden_dim=best_params['gru_hidden_dim'],\n",
    "    gru_num_layers=best_params['gru_num_layers'],\n",
    "    latent_dim=2,\n",
    "    dropout=best_params['dropout'],\n",
    "    bidirectional=best_params['bidirectional']\n",
    ").to(device)\n",
    "\n",
    "config = {\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'weight_decay': best_params['weight_decay'],\n",
    "    'max_epochs': 150,\n",
    "    'early_stop_patience': 15\n",
    "}\n",
    "\n",
    "final_model, metrics = train_model(final_model, train_loader, val_loader, config, verbose=True)\n",
    "\n",
    "print(f\"\\nFinal model metrics:\")\n",
    "print(f\"  Train loss: {metrics['train_loss']:.6f}\")\n",
    "print(f\"  Val loss:   {metrics['val_loss']:.6f}\")\n",
    "print(f\"  Overfit ratio: {metrics['overfit_ratio']:.3f}\")\n",
    "print(f\"  Epochs trained: {metrics['epochs_trained']}\")\n",
    "\n",
    "# === 5. GENERATE LATENT FEATURES ===\n",
    "print(\"\\n[5/7] Generating latent features for all dates...\")\n",
    "\n",
    "# Create dataset for full data\n",
    "full_dataset = SlidingWindowDataset(full_data_normalized, window_size)\n",
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=False, drop_last=False)\n",
    "\n",
    "# Generate latent features\n",
    "final_model.eval()\n",
    "latent_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        batch = batch.to(device)\n",
    "        z = final_model.encode(batch)\n",
    "        latent_features.append(z.cpu().numpy())\n",
    "\n",
    "latent_features = np.vstack(latent_features)\n",
    "print(f\"Raw latent features shape: {latent_features.shape}\")\n",
    "\n",
    "# === 6. FIRST-DIFFERENCE POSTPROCESSING ===\n",
    "print(\"\\n[6/7] Applying first-difference postprocessing...\")\n",
    "\n",
    "# Create DataFrame for raw latent features\n",
    "# First window_size-1 rows are NaN (insufficient lookback)\n",
    "output_df = pd.DataFrame(\n",
    "    index=full_data.index,\n",
    "    columns=[f'real_rate_latent_{i}' for i in range(2)]\n",
    ")\n",
    "output_df.iloc[window_size - 1:] = latent_features\n",
    "\n",
    "# Drop NaN rows (insufficient lookback)\n",
    "output_df_clean = output_df.dropna()\n",
    "\n",
    "# Compute autocorrelation of raw latent (before differencing)\n",
    "raw_autocorr = [output_df_clean[col].autocorr(lag=1) for col in output_df_clean.columns]\n",
    "print(f\"Raw latent autocorr (lag 1): {raw_autocorr}\")\n",
    "\n",
    "# Apply first-difference\n",
    "output_diff = output_df_clean.diff().dropna()  # This removes one more row\n",
    "\n",
    "# Compute autocorrelation of differenced latent\n",
    "diff_autocorr = [output_diff[col].autocorr(lag=1) for col in output_diff.columns]\n",
    "print(f\"Differenced latent autocorr (lag 1): {diff_autocorr}\")\n",
    "\n",
    "print(f\"\\nFinal output shape: {output_diff.shape}\")\n",
    "print(f\"Date range: {output_diff.index.min()} to {output_diff.index.max()}\")\n",
    "print(f\"Columns: {list(output_diff.columns)}\")\n",
    "\n",
    "# === 7. SAVE RESULTS ===\n",
    "print(\"\\n[7/7] Saving results to Kaggle output directory...\")\n",
    "\n",
    "# Save submodel output (first-differenced latent features)\n",
    "output_diff.to_csv(\"submodel_output.csv\")\n",
    "print(f\"Saved: submodel_output.csv ({output_diff.shape[0]} rows, {output_diff.shape[1]} columns)\")\n",
    "\n",
    "# Save model weights\n",
    "torch.save({\n",
    "    'model_state': final_model.state_dict(),\n",
    "    'config': best_params,\n",
    "    'standardization': {\n",
    "        'mean': train_mean.to_dict(),\n",
    "        'std': train_std.to_dict()\n",
    "    }\n",
    "}, \"model.pt\")\n",
    "print(f\"Saved: model.pt\")\n",
    "\n",
    "# Save training result JSON\n",
    "result = {\n",
    "    \"feature\": \"real_rate\",\n",
    "    \"attempt\": 2,\n",
    "    \"architecture\": \"GRU_Autoencoder\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"best_params\": best_params,\n",
    "    \"metrics\": metrics,\n",
    "    \"optuna_trials_completed\": n_completed,\n",
    "    \"optuna_best_value\": best_value,\n",
    "    \"output_shape\": list(output_diff.shape),\n",
    "    \"output_columns\": list(output_diff.columns),\n",
    "    \"autocorrelation\": {\n",
    "        \"raw_latent\": raw_autocorr,\n",
    "        \"differenced_latent\": diff_autocorr\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"train_samples\": len(train_data),\n",
    "        \"val_samples\": len(val_data),\n",
    "        \"test_samples\": len(test_data),\n",
    "        \"full_samples\": len(full_data),\n",
    "        \"window_size\": window_size,\n",
    "        \"latent_dim\": 2\n",
    "    },\n",
    "    \"model_params\": sum(p.numel() for p in final_model.parameters())\n",
    "}\n",
    "\n",
    "with open(\"training_result.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "print(f\"Saved: training_result.json\")\n",
    "\n",
    "# === FINAL SUMMARY ===\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Finished: {datetime.now().isoformat()}\")\n",
    "print(f\"\\nKey Results:\")\n",
    "print(f\"  Overfit ratio: {metrics['overfit_ratio']:.3f} (target: <1.5)\")\n",
    "print(f\"  Autocorr reduction: {raw_autocorr[0]:.3f} → {diff_autocorr[0]:.3f}\")\n",
    "print(f\"  Output shape: {output_diff.shape}\")\n",
    "print(f\"  Model parameters: {result['model_params']:,}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - submodel_output.csv\")\n",
    "print(\"  - model.pt\")\n",
    "print(\"  - training_result.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
