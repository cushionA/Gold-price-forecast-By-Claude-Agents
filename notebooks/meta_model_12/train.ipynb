{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold Meta-Model Training - Attempt 12\n\n**Architecture:** Single XGBoost with reg:squarederror (same as Attempt 7)\n\n**Key Changes from Attempt 7:**\n1. **Feature pruning**: 24 -> 18 features (removed bottom 6 by gain-weighted importance)\n2. **Removed features** (4.91% total importance):\n   - etf_pv_divergence (0.29%, 2 splits)\n   - vix_persistence (0.37%, 1 split)\n   - ie_anchoring_z (0.74%, 4 splits)\n   - ie_gold_sensitivity_z (1.13%, 6 splits)\n   - etf_capital_intensity (1.19%, 5 splits)\n   - xasset_recession_signal (1.19%, 2 splits, high avg gain but unstable)\n\n**Hypothesis:** Low-importance features add noise and compete for limited splits\n(max_depth=2 trees can only use 2-4 features per tree).\nPruning should improve signal-to-noise ratio.\n\n**Inherited from Attempt 7:**\n- Bootstrap variance-based confidence (5 models for HCDA)\n- OLS output scaling (validation-derived, capped at 10x)\n- Strengthened regularization (same HP ranges)\n- Optuna weights: 40/30/10/20\n- 100 Optuna trials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport optuna\nfrom optuna.samplers import TPESampler\nimport json\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\n\nprint(f\"XGBoost version: {xgb.__version__}\")\nprint(f\"Optuna version: {optuna.__version__}\")\nprint(f\"Started: {datetime.now().isoformat()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Definitions (18 features, pruned from 24)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FEATURE_COLUMNS = [\n    # Base features (5) - ALL KEPT\n    'real_rate_change',       # Rank 1, 24.90%\n    'dxy_change',             # Rank 13, 2.36%\n    'vix',                    # Rank 11, 2.60%\n    'yield_spread_change',    # Rank 14, 1.94%\n    'inflation_exp_change',   # Rank 6, 5.13%\n    # VIX submodel (2) - REMOVED: vix_persistence (rank 23, 0.37%, 1 split)\n    'vix_regime_probability', # Rank 7, 4.06%\n    'vix_mean_reversion_z',   # Rank 18, 1.45%\n    # Technical submodel (3) - ALL KEPT\n    'tech_trend_regime_prob', # Rank 2, 16.50%\n    'tech_mean_reversion_z',  # Rank 3, 7.09%\n    'tech_volatility_regime', # Rank 15, 1.80%\n    # Cross-asset submodel (2) - REMOVED: xasset_recession_signal (rank 19, 1.19%, 2 splits)\n    'xasset_regime_prob',     # Rank 9, 3.92%\n    'xasset_divergence',      # Rank 16, 1.79%\n    # Yield curve submodel (2) - ALL KEPT\n    'yc_spread_velocity_z',   # Rank 17, 1.55%\n    'yc_curvature_z',         # Rank 4, 5.29%\n    # ETF flow submodel (1) - REMOVED: etf_capital_intensity (rank 20), etf_pv_divergence (rank 24)\n    'etf_regime_prob',        # Rank 10, 3.11%\n    # Inflation expectation submodel (1) - REMOVED: ie_anchoring_z (rank 22), ie_gold_sensitivity_z (rank 21)\n    'ie_regime_prob',         # Rank 12, 2.43%\n    # Options market submodel (1) - KEPT\n    'options_risk_regime_prob',# Rank 5, 5.22%\n    # Temporal context submodel (1) - KEPT\n    'temporal_context_score', # Rank 8, 3.96%\n]\n\nREMOVED_FEATURES = [\n    'vix_persistence',\n    'xasset_recession_signal',\n    'etf_capital_intensity',\n    'etf_pv_divergence',\n    'ie_anchoring_z',\n    'ie_gold_sensitivity_z',\n]\n\nTARGET = 'gold_return_next'\n\nassert len(FEATURE_COLUMNS) == 18, f\"Expected 18 features, got {len(FEATURE_COLUMNS)}\"\nprint(f\"Features defined: {len(FEATURE_COLUMNS)} features (pruned from 24)\")\nprint(f\"Removed features: {REMOVED_FEATURES}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Fetching (API-Based)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*60)\nprint(\"FETCHING DATA FROM APIs\")\nprint(\"=\"*60)\n\nimport yfinance as yf\n\ntry:\n    from fredapi import Fred\nexcept ImportError:\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"fredapi\"], check=True)\n    from fredapi import Fred\n\nimport os\nFRED_API_KEY = os.environ.get('FRED_API_KEY', '3ffb68facdf6321e180e380c00e909c8')\nfred = Fred(api_key=FRED_API_KEY)\nprint(\"FRED API initialized\")\n\nprint(\"\\nFetching gold price (GC=F)...\")\ngold = yf.download('GC=F', start='2014-01-01', end='2026-02-20', progress=False)\ngold_df = gold[['Close']].copy()\ngold_df.columns = ['gold_price']\ngold_df['gold_return'] = gold_df['gold_price'].pct_change() * 100\ngold_df['gold_return_next'] = gold_df['gold_return'].shift(-1)\ngold_df = gold_df.dropna(subset=['gold_return_next'])\ngold_df.index = pd.to_datetime(gold_df.index).strftime('%Y-%m-%d')\nprint(f\"  Gold: {len(gold_df)} rows\")\n\nprint(\"\\nFetching base features...\")\nprint(\"  Fetching real rate (DFII10)...\")\nreal_rate = fred.get_series('DFII10', observation_start='2014-01-01')\nreal_rate_df = real_rate.to_frame('real_rate_real_rate')\nreal_rate_df.index = pd.to_datetime(real_rate_df.index).strftime('%Y-%m-%d')\n\nprint(\"  Fetching DXY (DX-Y.NYB)...\")\ndxy = yf.download('DX-Y.NYB', start='2014-01-01', end='2026-02-20', progress=False)\ndxy_df = dxy[['Close']].copy()\ndxy_df.columns = ['dxy_dxy']\ndxy_df.index = pd.to_datetime(dxy_df.index).strftime('%Y-%m-%d')\n\nprint(\"  Fetching VIX (VIXCLS)...\")\nvix = fred.get_series('VIXCLS', observation_start='2014-01-01')\nvix_df = vix.to_frame('vix_vix')\nvix_df.index = pd.to_datetime(vix_df.index).strftime('%Y-%m-%d')\n\nprint(\"  Fetching yield curve (DGS10, DGS2)...\")\ndgs10 = fred.get_series('DGS10', observation_start='2014-01-01')\ndgs2 = fred.get_series('DGS2', observation_start='2014-01-01')\nyc_df = pd.DataFrame({'DGS10': dgs10, 'DGS2': dgs2})\nyc_df['yield_curve_yield_spread'] = yc_df['DGS10'] - yc_df['DGS2']\nyc_df = yc_df[['yield_curve_yield_spread']]\nyc_df.index = pd.to_datetime(yc_df.index).strftime('%Y-%m-%d')\n\nprint(\"  Fetching inflation expectation (T10YIE)...\")\ninfl_exp = fred.get_series('T10YIE', observation_start='2014-01-01')\ninfl_exp_df = infl_exp.to_frame('inflation_expectation_inflation_expectation')\ninfl_exp_df.index = pd.to_datetime(infl_exp_df.index).strftime('%Y-%m-%d')\n\nbase_features = gold_df[['gold_return_next']].copy()\nfor df in [real_rate_df, dxy_df, vix_df, yc_df, infl_exp_df]:\n    base_features = base_features.join(df, how='left')\nbase_features = base_features.ffill()\nprint(f\"  Base features: {len(base_features)} rows, {len(base_features.columns)} columns\")\n\nprint(\"\\nLoading submodel outputs from Kaggle Dataset...\")\n\nsubmodel_files = {\n    'vix': {\n        'path': '../input/gold-prediction-submodels/vix.csv',\n        'columns': ['vix_regime_probability', 'vix_mean_reversion_z', 'vix_persistence'],\n        'date_col': 'date', 'tz_aware': False,\n    },\n    'technical': {\n        'path': '../input/gold-prediction-submodels/technical.csv',\n        'columns': ['tech_trend_regime_prob', 'tech_mean_reversion_z', 'tech_volatility_regime'],\n        'date_col': 'date', 'tz_aware': True,\n    },\n    'cross_asset': {\n        'path': '../input/gold-prediction-submodels/cross_asset.csv',\n        'columns': ['xasset_regime_prob', 'xasset_recession_signal', 'xasset_divergence'],\n        'date_col': 'Date', 'tz_aware': False,\n    },\n    'yield_curve': {\n        'path': '../input/gold-prediction-submodels/yield_curve.csv',\n        'columns': ['yc_spread_velocity_z', 'yc_curvature_z'],\n        'date_col': 'index', 'tz_aware': False,\n    },\n    'etf_flow': {\n        'path': '../input/gold-prediction-submodels/etf_flow.csv',\n        'columns': ['etf_regime_prob', 'etf_capital_intensity', 'etf_pv_divergence'],\n        'date_col': 'Date', 'tz_aware': False,\n    },\n    'inflation_expectation': {\n        'path': '../input/gold-prediction-submodels/inflation_expectation.csv',\n        'columns': ['ie_regime_prob', 'ie_anchoring_z', 'ie_gold_sensitivity_z'],\n        'date_col': 'Unnamed: 0', 'tz_aware': False,\n    },\n    'options_market': {\n        'path': '../input/gold-prediction-submodels/options_market.csv',\n        'columns': ['options_risk_regime_prob'],\n        'date_col': 'Date', 'tz_aware': True,\n    },\n    'temporal_context': {\n        'path': '../input/gold-prediction-submodels/temporal_context.csv',\n        'columns': ['temporal_context_score'],\n        'date_col': 'date', 'tz_aware': False,\n    },\n}\n\nsubmodel_dfs = {}\nfor feature, spec in submodel_files.items():\n    df = pd.read_csv(spec['path'])\n    date_col = spec['date_col']\n    if spec['tz_aware']:\n        df['Date'] = pd.to_datetime(df[date_col], utc=True).dt.strftime('%Y-%m-%d')\n    else:\n        if date_col == 'index':\n            df['Date'] = pd.to_datetime(df.iloc[:, 0]).dt.strftime('%Y-%m-%d')\n        elif date_col == 'Unnamed: 0':\n            df['Date'] = pd.to_datetime(df['Unnamed: 0']).dt.strftime('%Y-%m-%d')\n        else:\n            df['Date'] = pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d')\n    df = df[['Date'] + spec['columns']]\n    df = df.set_index('Date')\n    submodel_dfs[feature] = df\n    print(f\"  {feature}: {len(df)} rows\")\n\nprint(f\"\\nData fetching complete\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Transformation and NaN Imputation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\nApplying transformations...\")\n\nfinal_df = base_features.copy()\n\nfinal_df['real_rate_change'] = final_df['real_rate_real_rate'].diff()\nfinal_df['dxy_change'] = final_df['dxy_dxy'].diff()\nfinal_df['vix'] = final_df['vix_vix']\nfinal_df['yield_spread_change'] = final_df['yield_curve_yield_spread'].diff()\nfinal_df['inflation_exp_change'] = final_df['inflation_expectation_inflation_expectation'].diff()\n\nfinal_df = final_df.drop(columns=['real_rate_real_rate', 'dxy_dxy', 'vix_vix',\n                                    'yield_curve_yield_spread', 'inflation_expectation_inflation_expectation'])\n\nprint(\"\\nMerging submodel outputs...\")\nfor feature, df in submodel_dfs.items():\n    final_df = final_df.join(df, how='left')\n\nprint(f\"  Features after merge: {final_df.shape[1]} columns, {len(final_df)} rows\")\n\nprint(\"\\nApplying NaN imputation...\")\nnan_before = final_df.isna().sum().sum()\nprint(f\"  NaN before imputation: {nan_before}\")\n\nregime_cols = ['vix_regime_probability', 'tech_trend_regime_prob',\n               'xasset_regime_prob', 'etf_regime_prob', 'ie_regime_prob',\n               'options_risk_regime_prob', 'temporal_context_score']\nfor col in regime_cols:\n    if col in final_df.columns:\n        final_df[col] = final_df[col].fillna(0.5)\n\nz_cols = ['vix_mean_reversion_z', 'tech_mean_reversion_z',\n          'yc_spread_velocity_z', 'yc_curvature_z',\n          'etf_capital_intensity', 'etf_pv_divergence',\n          'ie_anchoring_z', 'ie_gold_sensitivity_z']\nfor col in z_cols:\n    if col in final_df.columns:\n        final_df[col] = final_df[col].fillna(0.0)\n\ndiv_cols = ['xasset_recession_signal', 'xasset_divergence']\nfor col in div_cols:\n    if col in final_df.columns:\n        final_df[col] = final_df[col].fillna(0.0)\n\ncont_cols = ['tech_volatility_regime', 'vix_persistence']\nfor col in cont_cols:\n    if col in final_df.columns:\n        final_df[col] = final_df[col].fillna(final_df[col].median())\n\nfinal_df = final_df.dropna(subset=['gold_return_next', 'real_rate_change', 'dxy_change',\n                                     'vix', 'yield_spread_change', 'inflation_exp_change'])\n\nnan_after = final_df.isna().sum().sum()\nprint(f\"  NaN after imputation: {nan_after}\")\nprint(f\"  Final dataset: {len(final_df)} rows\")\n\nassert all(col in final_df.columns for col in FEATURE_COLUMNS), \"Missing features after merge!\"\nassert TARGET in final_df.columns, \"Target not found!\"\nfor removed in REMOVED_FEATURES:\n    assert removed not in FEATURE_COLUMNS, f\"Removed feature {removed} still in FEATURE_COLUMNS!\"\n\nprint(f\"\\nAll {len(FEATURE_COLUMNS)} features present\")\nprint(f\"Dataset shape: {final_df.shape}\")\nprint(f\"Date range: {final_df.index.min()} to {final_df.index.max()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Val/Test Split (70/15/15)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_total = len(final_df)\nn_train = int(n_total * 0.70)\nn_val = int(n_total * 0.15)\n\ntrain_df = final_df.iloc[:n_train].copy()\nval_df = final_df.iloc[n_train:n_train+n_val].copy()\ntest_df = final_df.iloc[n_train+n_val:].copy()\n\nprint(f\"\\nData split complete:\")\nprint(f\"  Train: {len(train_df)} rows ({len(train_df)/n_total*100:.1f}%) - {train_df.index.min()} to {train_df.index.max()}\")\nprint(f\"  Val:   {len(val_df)} rows ({len(val_df)/n_total*100:.1f}%) - {val_df.index.min()} to {val_df.index.max()}\")\nprint(f\"  Test:  {len(test_df)} rows ({len(test_df)/n_total*100:.1f}%) - {test_df.index.min()} to {test_df.index.max()}\")\nprint(f\"  Total: {n_total} rows\")\nprint(f\"  Samples per feature: {n_train / len(FEATURE_COLUMNS):.1f}:1 (train)\")\n\nassert train_df.index.max() < val_df.index.min(), \"Train-val overlap!\"\nassert val_df.index.max() < test_df.index.min(), \"Val-test overlap!\"\nprint(f\"\\nNo time-series leakage detected\")\n\nX_train = train_df[FEATURE_COLUMNS].values\ny_train = train_df[TARGET].values\nX_val = val_df[FEATURE_COLUMNS].values\ny_val = val_df[TARGET].values\nX_test = test_df[FEATURE_COLUMNS].values\ny_test = test_df[TARGET].values\n\ndates_train = train_df.index\ndates_val = val_df.index\ndates_test = test_df.index\n\nprint(f\"\\nArray shapes:\")\nprint(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"  X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\nprint(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metric Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_direction_accuracy(y_true, y_pred):\n    mask = (y_true != 0) & (y_pred != 0)\n    if mask.sum() == 0: return 0.0\n    return (np.sign(y_pred[mask]) == np.sign(y_true[mask])).mean()\n\ndef compute_mae(y_true, y_pred):\n    return np.abs(y_pred - y_true).mean()\n\ndef compute_sharpe_trade_cost(y_true, y_pred, cost_bps=5.0):\n    positions = np.sign(y_pred)\n    strategy_returns = positions * y_true / 100.0\n    position_changes = np.abs(np.diff(positions, prepend=0))\n    trade_costs = position_changes * (cost_bps / 10000.0)\n    net_returns = strategy_returns - trade_costs\n    if len(net_returns) < 2 or net_returns.std() == 0: return 0.0\n    return (net_returns.mean() / net_returns.std()) * np.sqrt(252)\n\ndef compute_hcda(y_true, y_pred, threshold_percentile=80):\n    threshold = np.percentile(np.abs(y_pred), threshold_percentile)\n    hc_mask = np.abs(y_pred) > threshold\n    if hc_mask.sum() == 0: return 0.0, 0.0\n    coverage = hc_mask.sum() / len(y_pred)\n    hc_pred, hc_actual = y_pred[hc_mask], y_true[hc_mask]\n    mask = (hc_actual != 0) & (hc_pred != 0)\n    if mask.sum() == 0: return 0.0, coverage\n    return (np.sign(hc_pred[mask]) == np.sign(hc_actual[mask])).mean(), coverage\n\ndef compute_hcda_bootstrap(y_true, y_pred, bootstrap_std, threshold_percentile=80):\n    confidence = 1.0 / (1.0 + bootstrap_std)\n    threshold = np.percentile(confidence, threshold_percentile)\n    hc_mask = confidence > threshold\n    if hc_mask.sum() == 0: return 0.0, 0.0\n    coverage = hc_mask.sum() / len(y_pred)\n    hc_pred, hc_actual = y_pred[hc_mask], y_true[hc_mask]\n    mask = (hc_actual != 0) & (hc_pred != 0)\n    if mask.sum() == 0: return 0.0, coverage\n    return (np.sign(hc_pred[mask]) == np.sign(hc_actual[mask])).mean(), coverage\n\nprint(\"Metric functions defined\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna HPO (100 trials) - Same HP Ranges as Attempt 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def optuna_objective(trial):\n    params = {\n        'objective': 'reg:squarederror',\n        'max_depth': trial.suggest_int('max_depth', 2, 4),\n        'min_child_weight': trial.suggest_int('min_child_weight', 12, 25),\n        'subsample': trial.suggest_float('subsample', 0.4, 0.85),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.7),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 15.0, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 10.0, log=True),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n        'tree_method': 'hist', 'eval_metric': 'rmse', 'verbosity': 0,\n        'seed': 42 + trial.number,\n    }\n    n_estimators = trial.suggest_int('n_estimators', 100, 800)\n\n    model = xgb.XGBRegressor(**params, n_estimators=n_estimators, early_stopping_rounds=100)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n\n    train_pred = model.predict(X_train)\n    val_pred = model.predict(X_val)\n\n    train_da = compute_direction_accuracy(y_train, train_pred)\n    val_da = compute_direction_accuracy(y_val, val_pred)\n    val_mae = compute_mae(y_val, val_pred)\n    val_sharpe = compute_sharpe_trade_cost(y_val, val_pred)\n    val_hc_da, val_hc_coverage = compute_hcda(y_val, val_pred, threshold_percentile=80)\n\n    da_gap = (train_da - val_da) * 100\n    overfit_penalty = max(0.0, (da_gap - 10.0) / 30.0)\n\n    sharpe_norm = np.clip((val_sharpe + 3.0) / 6.0, 0.0, 1.0)\n    da_norm = np.clip((val_da * 100 - 40.0) / 30.0, 0.0, 1.0)\n    mae_norm = np.clip((1.0 - val_mae) / 0.5, 0.0, 1.0)\n    hc_da_norm = np.clip((val_hc_da * 100 - 40.0) / 30.0, 0.0, 1.0)\n\n    objective = (0.40 * sharpe_norm + 0.30 * da_norm + 0.10 * mae_norm + 0.20 * hc_da_norm\n                ) - 0.30 * overfit_penalty\n\n    trial.set_user_attr('val_da', float(val_da))\n    trial.set_user_attr('val_mae', float(val_mae))\n    trial.set_user_attr('val_sharpe', float(val_sharpe))\n    trial.set_user_attr('val_hc_da', float(val_hc_da))\n    trial.set_user_attr('val_hc_coverage', float(val_hc_coverage))\n    trial.set_user_attr('train_da', float(train_da))\n    trial.set_user_attr('da_gap_pp', float(da_gap))\n    trial.set_user_attr('n_estimators_used',\n                         int(model.best_iteration + 1) if hasattr(model, 'best_iteration')\n                         and model.best_iteration is not None else n_estimators)\n    return objective\n\nprint(\"Optuna objective function defined\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\nprint(\"RUNNING OPTUNA HPO (100 trials, 2-hour timeout)\")\nprint(\"=\"*60)\n\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\nstudy.optimize(optuna_objective, n_trials=100, timeout=7200, show_progress_bar=True)\n\nprint(f\"\\nOptuna optimization complete\")\nprint(f\"  Trials completed: {len(study.trials)}\")\nprint(f\"  Best value: {study.best_value:.4f}\")\nprint(f\"\\nBest hyperparameters:\")\nfor k, v in study.best_params.items():\n    print(f\"  {k}: {v}\")\n\nbest_trial = study.best_trial\nprint(f\"\\nBest trial validation metrics:\")\nprint(f\"  DA:     {best_trial.user_attrs['val_da']*100:.2f}%\")\nprint(f\"  HCDA:   {best_trial.user_attrs['val_hc_da']*100:.2f}%\")\nprint(f\"  MAE:    {best_trial.user_attrs['val_mae']:.4f}%\")\nprint(f\"  Sharpe: {best_trial.user_attrs['val_sharpe']:.2f}\")\nprint(f\"  DA gap: {best_trial.user_attrs['da_gap_pp']:.2f}pp\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fallback: Attempt 2 Best Params on 18 Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\nprint(\"FALLBACK: Testing Attempt 2 Best Params on 18 Features\")\nprint(\"=\"*60)\n\nFALLBACK_PARAMS = {\n    'objective': 'reg:squarederror', 'max_depth': 2, 'min_child_weight': 14,\n    'reg_lambda': 4.76, 'reg_alpha': 3.65, 'subsample': 0.478,\n    'colsample_bytree': 0.371, 'learning_rate': 0.025,\n    'tree_method': 'hist', 'eval_metric': 'rmse', 'verbosity': 0, 'seed': 42,\n}\nFALLBACK_N_ESTIMATORS = 300\n\nfallback_model = xgb.XGBRegressor(**FALLBACK_PARAMS, n_estimators=FALLBACK_N_ESTIMATORS, early_stopping_rounds=100)\nfallback_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n\nfallback_val_pred = fallback_model.predict(X_val)\nfallback_train_pred = fallback_model.predict(X_train)\nfallback_train_da = compute_direction_accuracy(y_train, fallback_train_pred)\nfallback_val_da = compute_direction_accuracy(y_val, fallback_val_pred)\nfallback_val_mae = compute_mae(y_val, fallback_val_pred)\nfallback_val_sharpe = compute_sharpe_trade_cost(y_val, fallback_val_pred)\nfallback_val_hc_da, _ = compute_hcda(y_val, fallback_val_pred, threshold_percentile=80)\nfallback_da_gap = (fallback_train_da - fallback_val_da) * 100\n\nsharpe_norm_fb = np.clip((fallback_val_sharpe + 3.0) / 6.0, 0.0, 1.0)\nda_norm_fb = np.clip((fallback_val_da * 100 - 40.0) / 30.0, 0.0, 1.0)\nmae_norm_fb = np.clip((1.0 - fallback_val_mae) / 0.5, 0.0, 1.0)\nhc_da_norm_fb = np.clip((fallback_val_hc_da * 100 - 40.0) / 30.0, 0.0, 1.0)\noverfit_penalty_fb = max(0.0, (fallback_da_gap - 10.0) / 30.0)\nfallback_objective = (0.40 * sharpe_norm_fb + 0.30 * da_norm_fb + 0.10 * mae_norm_fb + 0.20 * hc_da_norm_fb\n                     ) - 0.30 * overfit_penalty_fb\n\nprint(f\"\\nOptuna: {study.best_value:.4f} vs Fallback: {fallback_objective:.4f}\")\n\nif study.best_value >= fallback_objective:\n    print(\"Using Optuna best configuration\")\n    selected_config = 'optuna'\n    selected_params = study.best_params\n    final_model = xgb.XGBRegressor(\n        objective='reg:squarederror',\n        max_depth=selected_params['max_depth'],\n        min_child_weight=selected_params['min_child_weight'],\n        subsample=selected_params['subsample'],\n        colsample_bytree=selected_params['colsample_bytree'],\n        reg_lambda=selected_params['reg_lambda'],\n        reg_alpha=selected_params['reg_alpha'],\n        learning_rate=selected_params['learning_rate'],\n        tree_method='hist', eval_metric='rmse', verbosity=0, seed=42,\n        n_estimators=selected_params['n_estimators'], early_stopping_rounds=100)\nelse:\n    print(\"Using Attempt 2 fallback configuration\")\n    selected_config = 'fallback'\n    selected_params = FALLBACK_PARAMS.copy()\n    selected_params['n_estimators'] = FALLBACK_N_ESTIMATORS\n    final_model = xgb.XGBRegressor(**FALLBACK_PARAMS, n_estimators=FALLBACK_N_ESTIMATORS, early_stopping_rounds=100)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Training + OLS Scaling + Bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\nprint(f\"TRAINING FINAL MODEL ({selected_config.upper()} CONFIG)\")\nprint(\"=\"*60)\n\nfinal_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n\npred_train = final_model.predict(X_train)\npred_val = final_model.predict(X_val)\npred_test = final_model.predict(X_test)\n\npred_full = np.concatenate([pred_train, pred_val, pred_test])\ndates_full = pd.Index(list(dates_train) + list(dates_val) + list(dates_test))\ny_full = np.concatenate([y_train, y_val, y_test])\n\nprint(f\"\\nRaw predictions: Train mean={pred_train.mean():.4f} std={pred_train.std():.4f}\")\nprint(f\"                 Val   mean={pred_val.mean():.4f} std={pred_val.std():.4f}\")\nprint(f\"                 Test  mean={pred_test.mean():.4f} std={pred_test.std():.4f}\")\n\n# === OLS Output Scaling ===\nprint(\"\\n--- OLS Scaling ---\")\nnumerator = np.sum(pred_val * y_val)\ndenominator = np.sum(pred_val ** 2)\nalpha_ols = numerator / denominator if denominator != 0 else 1.0\nalpha_ols = np.clip(alpha_ols, 0.5, 10.0)\nprint(f\"OLS scaling factor: {alpha_ols:.2f}\")\n\nscaled_pred_train = pred_train * alpha_ols\nscaled_pred_val = pred_val * alpha_ols\nscaled_pred_test = pred_test * alpha_ols\nscaled_pred_full = pred_full * alpha_ols\n\nmae_raw = np.mean(np.abs(pred_test - y_test))\nmae_scaled = np.mean(np.abs(scaled_pred_test - y_test))\nprint(f\"MAE raw={mae_raw:.4f}%, scaled={mae_scaled:.4f}%, delta={mae_scaled-mae_raw:+.4f}%\")\n\nuse_scaled = mae_scaled < mae_raw\nprint(f\"Using {'SCALED' if use_scaled else 'RAW'} for MAE\")\n\n# === Bootstrap Ensemble ===\nprint(\"\\n--- Bootstrap Ensemble (5 models) ---\")\nbootstrap_models = []\nbootstrap_seeds = [42, 43, 44, 45, 46]\n\nfor i, seed in enumerate(bootstrap_seeds):\n    bp = selected_params.copy()\n    m = xgb.XGBRegressor(\n        objective='reg:squarederror',\n        max_depth=bp['max_depth'], min_child_weight=bp['min_child_weight'],\n        subsample=bp['subsample'], colsample_bytree=bp['colsample_bytree'],\n        reg_lambda=bp['reg_lambda'], reg_alpha=bp['reg_alpha'],\n        learning_rate=bp['learning_rate'],\n        tree_method='hist', eval_metric='rmse', verbosity=0, seed=seed,\n        n_estimators=bp['n_estimators'], early_stopping_rounds=100)\n    m.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    bootstrap_models.append(m)\n    print(f\"  Model {i+1}/5 trained (seed={seed})\")\n\nensemble_preds_test = np.array([m.predict(X_test) for m in bootstrap_models])\nensemble_preds_train = np.array([m.predict(X_train) for m in bootstrap_models])\nensemble_preds_val = np.array([m.predict(X_val) for m in bootstrap_models])\n\nbootstrap_std_test = np.std(ensemble_preds_test, axis=0)\nbootstrap_std_train = np.std(ensemble_preds_train, axis=0)\nbootstrap_std_val = np.std(ensemble_preds_val, axis=0)\n\nbootstrap_conf_test = 1.0 / (1.0 + bootstrap_std_test)\nbootstrap_conf_train = 1.0 / (1.0 + bootstrap_std_train)\nbootstrap_conf_val = 1.0 / (1.0 + bootstrap_std_val)\n\nprint(f\"\\nBootstrap std (test): [{bootstrap_std_test.min():.4f}, {bootstrap_std_test.max():.4f}], mean={bootstrap_std_test.mean():.4f}\")\n\nhcda_bootstrap_test, hcda_bootstrap_cov = compute_hcda_bootstrap(y_test, pred_test, bootstrap_std_test)\nhcda_pred_test, hcda_pred_cov = compute_hcda(y_test, pred_test)\n\nprint(f\"HCDA bootstrap={hcda_bootstrap_test*100:.2f}%, |pred|={hcda_pred_test*100:.2f}%\")\n\nuse_bootstrap_hcda = hcda_bootstrap_test > hcda_pred_test\nprimary_hcda_method = 'bootstrap' if use_bootstrap_hcda else 'pred'\nprimary_hcda_value = hcda_bootstrap_test if use_bootstrap_hcda else hcda_pred_test\nprint(f\"Using {primary_hcda_method.upper()} for HCDA: {primary_hcda_value*100:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\nprint(\"FINAL EVALUATION\")\nprint(\"=\"*60)\n\nmetrics_all = {}\nfor split_name, y_true, y_pred_raw, y_pred_scaled in [\n    ('train', y_train, pred_train, scaled_pred_train),\n    ('val', y_val, pred_val, scaled_pred_val),\n    ('test', y_test, pred_test, scaled_pred_test),\n]:\n    da = compute_direction_accuracy(y_true, y_pred_raw)\n    mae_raw_s = compute_mae(y_true, y_pred_raw)\n    mae_scaled_s = compute_mae(y_true, y_pred_scaled)\n    metrics_all[split_name] = {\n        'direction_accuracy': float(da),\n        'high_confidence_da': float(compute_hcda(y_true, y_pred_raw)[0]),\n        'high_confidence_coverage': float(compute_hcda(y_true, y_pred_raw)[1]),\n        'mae': float(min(mae_raw_s, mae_scaled_s)),\n        'mae_raw': float(mae_raw_s),\n        'mae_scaled': float(mae_scaled_s),\n        'sharpe_ratio': float(compute_sharpe_trade_cost(y_true, y_pred_raw)),\n    }\n\nfor split_name in ['train', 'val', 'test']:\n    m = metrics_all[split_name]\n    print(f\"\\n{split_name.upper()}: DA={m['direction_accuracy']*100:.2f}%, \"\n          f\"HCDA={m['high_confidence_da']*100:.2f}%, \"\n          f\"MAE={m['mae']:.4f}%, Sharpe={m['sharpe_ratio']:.2f}\")\n\ntrain_test_da_gap = (metrics_all['train']['direction_accuracy'] - metrics_all['test']['direction_accuracy']) * 100\ntest_m = metrics_all['test']\n\ntargets_met = [\n    test_m['direction_accuracy'] > 0.56,\n    primary_hcda_value > 0.60,\n    test_m['mae'] < 0.0075,\n    test_m['sharpe_ratio'] > 0.8,\n]\n\nprint(f\"\\nTARGET STATUS:\")\nprint(f\"  DA > 56%:     {'PASS' if targets_met[0] else 'FAIL'} ({test_m['direction_accuracy']*100:.2f}%)\")\nprint(f\"  HCDA > 60%:   {'PASS' if targets_met[1] else 'FAIL'} ({primary_hcda_value*100:.2f}% via {primary_hcda_method})\")\nprint(f\"  MAE < 0.75%:  {'PASS' if targets_met[2] else 'FAIL'} ({test_m['mae']:.4f}%)\")\nprint(f\"  Sharpe > 0.8: {'PASS' if targets_met[3] else 'FAIL'} ({test_m['sharpe_ratio']:.2f})\")\nprint(f\"\\nTargets passed: {sum(targets_met)}/4\")\nprint(f\"Overfitting: Train-Test DA gap = {train_test_da_gap:.2f}pp (target: <10pp)\")\n\n# Comparison with Attempt 7\natt7 = {'da': 0.6004, 'hcda': 0.6413, 'mae': 0.9429, 'sharpe': 2.4636}\nprint(f\"\\nVS ATTEMPT 7 (24 features):\")\nprint(f\"  DA:     {(test_m['direction_accuracy']-att7['da'])*100:+.2f}pp\")\nprint(f\"  HCDA:   {(primary_hcda_value-att7['hcda'])*100:+.2f}pp\")\nprint(f\"  MAE:    {test_m['mae']-att7['mae']:+.4f}%\")\nprint(f\"  Sharpe: {test_m['sharpe_ratio']-att7['sharpe']:+.2f}\")\n\nnaive_always_up_da = (y_test > 0).sum() / len(y_test)\nprint(f\"\\nNaive always-up DA: {naive_always_up_da*100:.2f}%\")\nprint(f\"Model vs naive: {(test_m['direction_accuracy']-naive_always_up_da)*100:+.2f}pp\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature importance (18 features)\nfeature_importance = final_model.feature_importances_\nfeature_ranking = pd.DataFrame({\n    'feature': FEATURE_COLUMNS, 'importance': feature_importance\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nFEATURE IMPORTANCE (18 features):\")\nfor i, (_, row) in enumerate(feature_ranking.iterrows(), 1):\n    print(f\"  {i:2d}. {row['feature']:30s} {row['importance']:.4f}\")\n\nprint(f\"\\nPrediction distribution (test): mean={pred_test.mean():.4f}, std={pred_test.std():.4f}, \"\n      f\"positive={((pred_test>0).sum()/len(pred_test)*100):.1f}%\")\n\n# Quarterly breakdown\ntest_df_with_pred = test_df.copy()\ntest_df_with_pred['prediction'] = pred_test\ntest_df_with_pred['quarter'] = pd.to_datetime(test_df_with_pred.index).to_period('Q')\nprint(\"\\nQUARTERLY PERFORMANCE:\")\nfor quarter in test_df_with_pred['quarter'].unique():\n    qtr = test_df_with_pred[test_df_with_pred['quarter'] == quarter]\n    qda = compute_direction_accuracy(qtr[TARGET].values, qtr['prediction'].values)\n    qmae = compute_mae(qtr[TARGET].values, qtr['prediction'].values)\n    qsharpe = compute_sharpe_trade_cost(qtr[TARGET].values, qtr['prediction'].values)\n    print(f\"  {quarter}: DA={qda*100:5.1f}%, MAE={qmae:.3f}%, Sharpe={qsharpe:5.2f}, N={len(qtr)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*60)\nprint(\"SAVING RESULTS\")\nprint(\"=\"*60)\n\n# predictions.csv\nsplit_labels = ['train']*len(dates_train) + ['val']*len(dates_val) + ['test']*len(dates_test)\npredictions_df = pd.DataFrame({\n    'date': dates_full, 'split': split_labels, 'actual': y_full,\n    'prediction_raw': pred_full, 'prediction_scaled': scaled_pred_full,\n    'direction_correct': (np.sign(pred_full) == np.sign(y_full)).astype(int),\n    'abs_prediction': np.abs(pred_full),\n})\nthreshold_80 = np.percentile(np.abs(pred_full), 80)\npredictions_df['high_confidence_pred'] = (predictions_df['abs_prediction'] > threshold_80).astype(int)\nbootstrap_conf_full = np.concatenate([bootstrap_conf_train, bootstrap_conf_val, bootstrap_conf_test])\nbootstrap_std_full = np.concatenate([bootstrap_std_train, bootstrap_std_val, bootstrap_std_test])\npredictions_df['bootstrap_confidence'] = bootstrap_conf_full\npredictions_df['bootstrap_std'] = bootstrap_std_full\nthreshold_80_bs = np.percentile(bootstrap_conf_full, 80)\npredictions_df['high_confidence_bootstrap'] = (predictions_df['bootstrap_confidence'] > threshold_80_bs).astype(int)\n\npredictions_df.to_csv('predictions.csv', index=False)\npredictions_df[predictions_df['split']=='test'].to_csv('test_predictions.csv', index=False)\npredictions_df.to_csv('submodel_output.csv', index=False)\nfinal_model.save_model('model.json')\nprint(\"Saved predictions.csv, test_predictions.csv, submodel_output.csv, model.json\")\n\n# training_result.json\ntraining_result = {\n    'feature': 'meta_model', 'attempt': 12,\n    'timestamp': datetime.now().isoformat(),\n    'architecture': 'XGBoost reg:squarederror + Bootstrap confidence + OLS scaling',\n    'phase': '3_meta_model',\n    'pruning_experiment': {\n        'base_attempt': 7, 'features_before': 24, 'features_after': 18,\n        'removed_features': REMOVED_FEATURES, 'removed_total_importance': 0.0491,\n        'hypothesis': 'Removing low-importance features reduces noise',\n    },\n    'model_config': {\n        'n_features': 18, 'train_samples': len(X_train),\n        'val_samples': len(X_val), 'test_samples': len(X_test),\n        'samples_per_feature_ratio': len(X_train) / 18,\n        'selected_configuration': selected_config,\n        'optuna_trials_completed': len(study.trials),\n        'best_params': selected_params,\n    },\n    'optuna_search': {\n        'n_trials': len(study.trials), 'best_value': float(study.best_value),\n        'best_trial_number': study.best_trial.number,\n        'top_5_trials': [{\n            'number': t.number, 'value': float(t.value), 'params': t.params,\n            'val_da': float(t.user_attrs['val_da']),\n            'val_hc_da': float(t.user_attrs['val_hc_da']),\n        } for t in sorted(study.trials, key=lambda x: x.value, reverse=True)[:5]],\n    },\n    'fallback_comparison': {\n        'fallback_objective': float(fallback_objective),\n        'optuna_objective': float(study.best_value),\n        'selected': selected_config,\n        'fallback_metrics': {'da': float(fallback_val_da), 'hcda': float(fallback_val_hc_da),\n                             'mae': float(fallback_val_mae), 'sharpe': float(fallback_val_sharpe)},\n    },\n    'bootstrap_analysis': {\n        'bootstrap_ensemble_size': 5, 'bootstrap_seeds': bootstrap_seeds,\n        'bootstrap_std_range_test': [float(bootstrap_std_test.min()), float(bootstrap_std_test.max())],\n        'bootstrap_std_mean_test': float(bootstrap_std_test.mean()),\n        'bootstrap_conf_range_test': [float(bootstrap_conf_test.min()), float(bootstrap_conf_test.max())],\n        'bootstrap_conf_mean_test': float(bootstrap_conf_test.mean()),\n        'hcda_bootstrap': float(hcda_bootstrap_test), 'hcda_pred': float(hcda_pred_test),\n        'hcda_improvement': float(hcda_bootstrap_test - hcda_pred_test),\n    },\n    'ols_scaling': {\n        'alpha_ols': float(alpha_ols), 'mae_raw': float(mae_raw),\n        'mae_scaled': float(mae_scaled), 'mae_improvement': float(mae_raw - mae_scaled),\n    },\n    'primary_hcda_method': primary_hcda_method,\n    'primary_hcda_value': float(primary_hcda_value),\n    'primary_mae': float(min(mae_raw, mae_scaled)),\n    'metrics': metrics_all,\n    'target_evaluation': {\n        'direction_accuracy': {'target': '> 56.0%', 'actual': f\"{test_m['direction_accuracy']*100:.2f}%\",\n                               'gap': f\"{(test_m['direction_accuracy']-0.56)*100:+.2f}pp\", 'passed': bool(targets_met[0])},\n        'high_confidence_da': {'target': '> 60.0%', 'actual': f\"{primary_hcda_value*100:.2f}%\",\n                               'gap': f\"{(primary_hcda_value-0.60)*100:+.2f}pp\", 'passed': bool(targets_met[1]),\n                               'method_used': primary_hcda_method},\n        'mae': {'target': '< 0.75%', 'actual': f\"{test_m['mae']:.4f}%\",\n                'gap': f\"{(0.0075-test_m['mae']):.4f}%\", 'passed': bool(targets_met[2])},\n        'sharpe_ratio': {'target': '> 0.80', 'actual': f\"{test_m['sharpe_ratio']:.2f}\",\n                         'gap': f\"{(test_m['sharpe_ratio']-0.8):+.2f}\", 'passed': bool(targets_met[3])},\n    },\n    'targets_passed': str(sum(targets_met)), 'targets_total': 4,\n    'overall_passed': all(targets_met),\n    'overfitting_analysis': {\n        'train_test_da_gap_pp': float(train_test_da_gap), 'target_gap_pp': 10.0,\n        'overfitting_check': 'PASS' if train_test_da_gap < 10 else 'FAIL',\n    },\n    'feature_importance': {'top_10_xgb': feature_ranking.head(10).to_dict('records')},\n    'vs_attempt_7': {\n        'da_delta_pp': float((test_m['direction_accuracy']-0.6004)*100),\n        'hcda_delta_pp': float((primary_hcda_value-0.6413)*100),\n        'mae_delta': float(test_m['mae']-0.9429),\n        'sharpe_delta': float(test_m['sharpe_ratio']-2.4636),\n    },\n    'vs_attempt_2': {\n        'da_delta_pp': float((test_m['direction_accuracy']-0.5726)*100),\n        'hcda_delta_pp': float((primary_hcda_value-0.5526)*100),\n        'mae_delta': float(test_m['mae']-0.6877),\n        'sharpe_delta': float(test_m['sharpe_ratio']-1.5835),\n    },\n    'vs_naive': {\n        'naive_always_up_da': f\"{naive_always_up_da*100:.2f}%\",\n        'model_vs_naive_pp': float((test_m['direction_accuracy']-naive_always_up_da)*100),\n    },\n    'prediction_characteristics': {\n        'mean_raw': float(pred_test.mean()), 'std_raw': float(pred_test.std()),\n        'min_raw': float(pred_test.min()), 'max_raw': float(pred_test.max()),\n        'positive_pct': float((pred_test>0).sum()/len(pred_test)*100),\n    },\n}\n\nwith open('training_result.json', 'w') as f:\n    json.dump(training_result, f, indent=2, default=str)\nprint(\"Saved training_result.json\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"TRAINING COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"Features: {len(FEATURE_COLUMNS)} (pruned from 24)\")\nprint(f\"Config: {selected_config.upper()}, HCDA: {primary_hcda_method.upper()}\")\nprint(f\"Targets passed: {sum(targets_met)}/4\")\nprint(f\"\\nVs Attempt 7: DA {(test_m['direction_accuracy']-0.6004)*100:+.2f}pp, \"\n      f\"HCDA {(primary_hcda_value-0.6413)*100:+.2f}pp, \"\n      f\"MAE {test_m['mae']-0.9429:+.4f}%, \"\n      f\"Sharpe {test_m['sharpe_ratio']-2.4636:+.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}