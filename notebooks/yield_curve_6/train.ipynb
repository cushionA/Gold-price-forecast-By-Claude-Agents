{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-0",
   "metadata": {},
   "source": [
    "# Gold SubModel: Yield Curve - Attempt 6\n",
    "**Approach**: Yield Decomposition Velocity (Breakeven + TIPS velocity z-scores)\n",
    "\n",
    "**Key differences from previous attempts**:\n",
    "- No ML model, no PyTorch, no HMM\n",
    "- Decomposes nominal yield dynamics into real rate and inflation premium components\n",
    "- Velocity (daily change) z-scored to avoid autocorrelation\n",
    "\n",
    "**Output features**:\n",
    "1. `yc_be_vel_z`: Z-scored daily change in 10Y breakeven inflation rate (DGS10 - DFII10)\n",
    "2. `yc_tips_vel_z`: Z-scored daily change in 10Y TIPS real yield (DFII10)\n",
    "\n",
    "**Economic logic**:\n",
    "- When yields rise, gold responds differently depending on whether it is real rates or inflation expectations driving the move\n",
    "- `yc_be_vel_z` positive = inflation expectations rising = gold-positive\n",
    "- `yc_tips_vel_z` positive = real rates rising = gold-negative (higher opportunity cost)\n",
    "\n",
    "**Expected Gate 1**: autocorr max 0.046 (far below 0.95 threshold)\n",
    "**Expected Gate 2**: VIF max 1.36 with att2 features. Max corr with att2 = 0.240."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from numpy.linalg import inv as np_inv\n",
    "\n",
    "# Install fredapi if not available\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "except ImportError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'fredapi', '-q'])\n",
    "    from fredapi import Fred\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "FEATURE_NAME = 'yield_curve'\n",
    "ATTEMPT = 6\n",
    "OUTPUT_COLUMNS = ['yc_be_vel_z', 'yc_tips_vel_z']\n",
    "CLIP_RANGE = (-4, 4)\n",
    "\n",
    "print(f'=== Gold SubModel Training: {FEATURE_NAME} attempt {ATTEMPT} ===')\n",
    "print('Approach: Yield Decomposition Velocity (Breakeven + TIPS velocity z-scores)')\n",
    "print(f'Started: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRED API key via Kaggle Secrets (preferred) with environment variable fallback\n",
    "FRED_API_KEY = None\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    FRED_API_KEY = UserSecretsClient().get_secret('FRED_API_KEY')\n",
    "    print('FRED_API_KEY loaded from Kaggle Secrets')\n",
    "except Exception:\n",
    "    FRED_API_KEY = os.environ.get('FRED_API_KEY')\n",
    "    if FRED_API_KEY:\n",
    "        print('FRED_API_KEY loaded from environment')\n",
    "    else:\n",
    "        print('WARNING: FRED_API_KEY not found in Kaggle Secrets or environment')\n",
    "\n",
    "if not FRED_API_KEY:\n",
    "    raise RuntimeError('FRED_API_KEY not found in Kaggle Secrets or environment. '\n",
    "                       'Add FRED_API_KEY to Kaggle Secrets at kaggle.com/settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic path resolution for bigbigzabuton/gold-prediction-submodels\n",
    "# Strategy: check BOTH path AND file existence, then list files for debugging.\n",
    "import glob as _glob\n",
    "\n",
    "PROBE_FILES = ['base_features.csv', 'base_features_raw.csv', 'vix.csv']\n",
    "candidates = [\n",
    "    '/kaggle/input/gold-prediction-submodels',\n",
    "    '/kaggle/input/datasets/bigbigzabuton/gold-prediction-submodels',\n",
    "]\n",
    "\n",
    "DATASET_PATH = None\n",
    "for c in candidates:\n",
    "    if os.path.isdir(c):\n",
    "        files_in_dir = os.listdir(c)\n",
    "        if any(f in files_in_dir for f in PROBE_FILES):\n",
    "            DATASET_PATH = c\n",
    "            print(f'Dataset found: {DATASET_PATH}')\n",
    "            print(f'  Files ({len(files_in_dir)}): {sorted(files_in_dir)[:10]}')\n",
    "            break\n",
    "        else:\n",
    "            print(f'Dir exists but missing probe files: {c} -> {files_in_dir[:5]}')\n",
    "\n",
    "if DATASET_PATH is None:\n",
    "    found = _glob.glob('/kaggle/input/*gold*') + _glob.glob('/kaggle/input/datasets/*/*gold*')\n",
    "    raise RuntimeError(\n",
    "        f'Dataset not found.\\n'\n",
    "        f'Tried: {candidates}\\n'\n",
    "        f'Glob /kaggle/input/*gold*: {found}\\n'\n",
    "        f'All /kaggle/input/: {os.listdir(\"/kaggle/input\") if os.path.exists(\"/kaggle/input\") else \"N/A\"}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch DGS10 and DFII10 from FRED\n",
    "# Only 2 series needed: DGS10 (nominal 10Y) and DFII10 (real 10Y TIPS yield)\n",
    "# Breakeven = DGS10 - DFII10 (equivalent to T10YIE by identity, no extra API call needed)\n",
    "START = '2014-06-01'\n",
    "\n",
    "print('Fetching FRED yield data...')\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "dgs10_raw = fred.get_series('DGS10', observation_start=START)\n",
    "dfii10_raw = fred.get_series('DFII10', observation_start=START)\n",
    "\n",
    "print(f'DGS10: {len(dgs10_raw.dropna())} obs, {dgs10_raw.dropna().index[0].date()} to {dgs10_raw.dropna().index[-1].date()}')\n",
    "print(f'DFII10: {len(dfii10_raw.dropna())} obs, {dfii10_raw.dropna().index[0].date()} to {dfii10_raw.dropna().index[-1].date()}')\n",
    "\n",
    "# Forward-fill up to 3 days for weekends/holidays\n",
    "dgs10_raw = dgs10_raw.ffill(limit=3)\n",
    "dfii10_raw = dfii10_raw.ffill(limit=3)\n",
    "\n",
    "# Inner join: only keep dates where both series have values\n",
    "yields_df = pd.DataFrame({\n",
    "    'dgs10': dgs10_raw,\n",
    "    'dfii10': dfii10_raw,\n",
    "}).dropna()\n",
    "yields_df.index = pd.to_datetime(yields_df.index)\n",
    "print(f'Combined yields (inner join): {len(yields_df)} obs, {yields_df.index[0].date()} to {yields_df.index[-1].date()}')\n",
    "\n",
    "# Sanity check: breakeven should be mostly positive and in 0.5-3.5% range\n",
    "breakeven_check = yields_df['dgs10'] - yields_df['dfii10']\n",
    "print(f'Breakeven (DGS10 - DFII10): min={breakeven_check.min():.2f}%, max={breakeven_check.max():.2f}%, mean={breakeven_check.mean():.2f}%')\n",
    "pct_positive = (breakeven_check > 0).mean() * 100\n",
    "print(f'Breakeven positive: {pct_positive:.1f}% of observations')\n",
    "print(f'DGS10 range: {yields_df[\"dgs10\"].min():.2f}% to {yields_df[\"dgs10\"].max():.2f}%')\n",
    "print(f'DFII10 range: {yields_df[\"dfii10\"].min():.2f}% to {yields_df[\"dfii10\"].max():.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch gold price from Yahoo Finance for target computation\n",
    "gold = yf.download('GC=F', start=START, auto_adjust=True, progress=False)\n",
    "\n",
    "if gold.empty or len(gold) < 100:\n",
    "    raise ValueError(f'GC=F download returned insufficient data: {len(gold)} rows')\n",
    "\n",
    "if isinstance(gold.columns, pd.MultiIndex):\n",
    "    gold_close = gold['Close'].iloc[:, 0]\n",
    "else:\n",
    "    gold_close = gold['Close'].squeeze()\n",
    "\n",
    "gold_ret = gold_close.pct_change() * 100\n",
    "gold_ret_next = gold_ret.shift(-1)  # next-day return (target)\n",
    "gold_ret_next.name = 'gold_return_next'\n",
    "gold_ret_next.index = pd.to_datetime(gold_ret_next.index).tz_localize(None)\n",
    "print(f'GC=F: {len(gold_ret_next.dropna())} obs, {gold_ret_next.dropna().index[0].date()} to {gold_ret_next.dropna().index[-1].date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base_features for date alignment (try both filename variants)\n",
    "base_path = None\n",
    "for fname in ['base_features.csv', 'base_features_raw.csv']:\n",
    "    candidate = os.path.join(DATASET_PATH, fname)\n",
    "    if os.path.exists(candidate):\n",
    "        base_path = candidate\n",
    "        print(f'Using base features file: {fname}')\n",
    "        break\n",
    "\n",
    "if base_path is None:\n",
    "    files_in_dir = os.listdir(DATASET_PATH)\n",
    "    raise FileNotFoundError(\n",
    "        f'base_features[_raw].csv not found in {DATASET_PATH}.\\n'\n",
    "        f'Available files: {sorted(files_in_dir)}'\n",
    "    )\n",
    "\n",
    "base_features = pd.read_csv(base_path, index_col=0, parse_dates=True)\n",
    "base_features.index = pd.to_datetime(base_features.index)\n",
    "TARGET_DATES = base_features.index\n",
    "print(f'Base features: {len(base_features)} rows, {TARGET_DATES[0].date()} to {TARGET_DATES[-1].date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split: train/val/test = 70/15/15 on base_features dates\n",
    "common_dates = yields_df.index.intersection(TARGET_DATES)\n",
    "common_dates = common_dates.intersection(gold_ret_next.dropna().index)\n",
    "common_dates = common_dates.sort_values()\n",
    "\n",
    "n = len(common_dates)\n",
    "train_end_idx = int(n * 0.70)\n",
    "val_end_idx = int(n * 0.85)\n",
    "\n",
    "train_dates = common_dates[:train_end_idx]\n",
    "val_dates = common_dates[train_end_idx:val_end_idx]\n",
    "test_dates = common_dates[val_end_idx:]\n",
    "\n",
    "print(f'Total aligned: {n}')\n",
    "print(f'Train: {len(train_dates)} ({train_dates[0].date()} to {train_dates[-1].date()})')\n",
    "print(f'Val:   {len(val_dates)} ({val_dates[0].date()} to {val_dates[-1].date()})')\n",
    "print(f'Test:  {len(test_dates)} ({test_dates[0].date()} to {test_dates[-1].date()})')\n",
    "\n",
    "# Create masks on yields_df index\n",
    "val_mask = yields_df.index.isin(val_dates)\n",
    "test_mask = yields_df.index.isin(test_dates)\n",
    "\n",
    "# Aligned gold target\n",
    "gold_aligned = gold_ret_next.reindex(yields_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation functions\n",
    "\n",
    "def rolling_zscore(x, window):\n",
    "    \"\"\"Rolling z-score with NaN-safe handling.\"\"\"\n",
    "    min_p = max(window // 2, 10)\n",
    "    m = x.rolling(window, min_periods=min_p).mean()\n",
    "    s = x.rolling(window, min_periods=min_p).std()\n",
    "    z = (x - m) / s.replace(0, np.nan)\n",
    "    return z.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "def generate_all_features(yields_df, zscore_window):\n",
    "    \"\"\"\n",
    "    Generate yield decomposition velocity z-scores.\n",
    "\n",
    "    Approach:\n",
    "    - breakeven = DGS10 - DFII10 (10Y inflation premium)\n",
    "    - yc_be_vel_z: z-scored daily change in breakeven (inflation expectation speed)\n",
    "    - yc_tips_vel_z: z-scored daily change in DFII10 (real rate speed)\n",
    "\n",
    "    Both features use the same zscore_window for temporal consistency.\n",
    "    Velocity (diff) not level avoids autocorrelation > 0.95.\n",
    "    \"\"\"\n",
    "    # Breakeven inflation rate = DGS10 - DFII10 (identity: equals T10YIE)\n",
    "    breakeven = yields_df['dgs10'] - yields_df['dfii10']\n",
    "\n",
    "    # Feature 1: Breakeven velocity z-score\n",
    "    be_vel = breakeven.diff()  # daily change in breakeven\n",
    "\n",
    "    # Feature 2: TIPS velocity z-score\n",
    "    tips_vel = yields_df['dfii10'].diff()  # daily change in real rate\n",
    "\n",
    "    features = pd.DataFrame(index=yields_df.index)\n",
    "    features['yc_be_vel_z'] = rolling_zscore(be_vel, zscore_window).clip(*CLIP_RANGE)\n",
    "    features['yc_tips_vel_z'] = rolling_zscore(tips_vel, zscore_window).clip(*CLIP_RANGE)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def compute_mi(feature, target, n_bins=20):\n",
    "    \"\"\"MI between feature and target using quantile binning.\"\"\"\n",
    "    valid = feature.dropna().index.intersection(target.dropna().index)\n",
    "    if len(valid) < 50:\n",
    "        return 0.0\n",
    "    f = feature[valid]\n",
    "    t = target[valid]\n",
    "    try:\n",
    "        f_binned = pd.qcut(f, q=n_bins, labels=False, duplicates='drop')\n",
    "        t_binned = pd.qcut(t, q=n_bins, labels=False, duplicates='drop')\n",
    "        valid2 = f_binned.notna() & t_binned.notna()\n",
    "        if valid2.sum() < 50:\n",
    "            return 0.0\n",
    "        return float(mutual_info_score(f_binned[valid2], t_binned[valid2]))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "print('Feature generation functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna HPO: maximize sum of MI between 2 features and gold_return_next on validation set\n",
    "# Search space: zscore_window in {30, 45, 60, 90, 120}\n",
    "# Only 5 unique values; 20 trials gives complete coverage with repetition for robustness.\n",
    "# Both features share the same zscore_window for temporal consistency.\n",
    "\n",
    "def objective(trial):\n",
    "    zscore_window = trial.suggest_categorical('zscore_window', [30, 45, 60, 90, 120])\n",
    "\n",
    "    features = generate_all_features(yields_df, zscore_window)\n",
    "\n",
    "    target_val = gold_aligned[val_mask]\n",
    "    mi_sum = 0.0\n",
    "    for col in OUTPUT_COLUMNS:\n",
    "        feat_val = features[col][val_mask]\n",
    "        mi_sum += compute_mi(feat_val, target_val)\n",
    "\n",
    "    return mi_sum\n",
    "\n",
    "\n",
    "print('Running Optuna HPO (20 trials, timeout=180s, TPE sampler)...')\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(objective, n_trials=20, timeout=180)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "n_completed = len(study.trials)\n",
    "print(f'Optuna complete: {n_completed} trials')\n",
    "print(f'Best params: {best_params}')\n",
    "print(f'Best MI sum (val): {best_value:.6f}')\n",
    "\n",
    "# Print all trial results for transparency\n",
    "print('\\n=== All Optuna Trials ===')\n",
    "for t in sorted(study.trials, key=lambda t: t.value if t.value is not None else 0, reverse=True):\n",
    "    print(f'  zscore_window={t.params[\"zscore_window\"]:>4d}, MI_sum={t.value:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final features with best Optuna parameters\n",
    "print(f'Generating final features with zscore_window={best_params[\"zscore_window\"]}...')\n",
    "final_features = generate_all_features(\n",
    "    yields_df,\n",
    "    zscore_window=best_params['zscore_window']\n",
    ")\n",
    "\n",
    "print(f'Final features shape: {final_features.shape}')\n",
    "print(f'NaN counts before alignment:')\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    print(f'  {col}: {final_features[col].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks: autocorrelation, internal correlation, VIF\n",
    "print('=== Quality Checks ===')\n",
    "\n",
    "# Autocorrelation check (Gate 1 threshold: 0.95)\n",
    "autocorr_results = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    series_clean = final_features[col].dropna()\n",
    "    if len(series_clean) > 1:\n",
    "        ac = float(series_clean.autocorr(lag=1))\n",
    "    else:\n",
    "        ac = 0.0\n",
    "    autocorr_results[col] = round(ac, 6)\n",
    "    status = 'FAIL' if abs(ac) > 0.95 else 'PASS'\n",
    "    print(f'Autocorr {col}: {ac:.6f} [{status}]')\n",
    "\n",
    "# NaN check\n",
    "print('\\nNaN per column:')\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    nan_count = final_features[col].isna().sum()\n",
    "    nan_pct = nan_count / len(final_features) * 100\n",
    "    print(f'  {col}: {nan_count} ({nan_pct:.1f}%)')\n",
    "\n",
    "# Internal correlation check\n",
    "clean_features = final_features[OUTPUT_COLUMNS].dropna()\n",
    "corr_matrix = clean_features.corr()\n",
    "print(f'\\nInternal correlation matrix:')\n",
    "print(corr_matrix.round(4).to_string())\n",
    "\n",
    "# Upper triangle values for max internal corr\n",
    "upper_tri_vals = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
    "max_internal_corr = float(np.abs(upper_tri_vals).max())\n",
    "print(f'\\nMax internal |correlation|: {max_internal_corr:.4f}')\n",
    "\n",
    "# VIF check\n",
    "print('\\nVIF values:')\n",
    "X = clean_features.values\n",
    "cm = np.corrcoef(X.T)\n",
    "max_vif = None\n",
    "try:\n",
    "    inv_cm = np_inv(cm)\n",
    "    vif_values = np.diag(inv_cm)\n",
    "    for col, v in zip(OUTPUT_COLUMNS, vif_values):\n",
    "        print(f'  VIF {col}: {v:.4f}')\n",
    "    max_vif = float(np.max(vif_values))\n",
    "    print(f'  Max VIF: {max_vif:.4f}')\n",
    "except Exception as e:\n",
    "    print(f'  VIF calculation failed: {e}')\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f'\\nDescriptive statistics:')\n",
    "print(final_features[OUTPUT_COLUMNS].describe().round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI on validation and test sets\n",
    "val_mi = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    mi = compute_mi(final_features[col][val_mask], gold_aligned[val_mask])\n",
    "    val_mi[col] = round(mi, 6)\n",
    "\n",
    "test_mi = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    mi = compute_mi(final_features[col][test_mask], gold_aligned[test_mask])\n",
    "    test_mi[col] = round(mi, 6)\n",
    "\n",
    "print('MI on validation set:')\n",
    "for col, mi in val_mi.items():\n",
    "    print(f'  {col}: {mi:.6f}')\n",
    "print(f'  Val MI sum: {sum(val_mi.values()):.6f}')\n",
    "\n",
    "print('\\nMI on test set:')\n",
    "for col, mi in test_mi.items():\n",
    "    print(f'  {col}: {mi:.6f}')\n",
    "print(f'  Test MI sum: {sum(test_mi.values()):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align output to base_features date range and save\n",
    "output = final_features[OUTPUT_COLUMNS].reindex(base_features.index)\n",
    "output.index.name = 'Date'\n",
    "\n",
    "# Forward-fill up to 3 days for minor gaps (FRED data has occasional missing trading days)\n",
    "output = output.ffill(limit=3)\n",
    "\n",
    "# Drop rows that are entirely NaN (warmup period before rolling windows are populated)\n",
    "output = output.dropna(how='all')\n",
    "\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Date range: {output.index[0].date()} to {output.index[-1].date()}')\n",
    "print(f'NaN per column after alignment and ffill:')\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    nan_count = output[col].isna().sum()\n",
    "    print(f'  {col}: {nan_count}')\n",
    "\n",
    "# Save submodel output\n",
    "output.to_csv('/kaggle/working/submodel_output.csv')\n",
    "print('\\nSaved: /kaggle/working/submodel_output.csv')\n",
    "print(output.tail(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training result JSON\n",
    "# overfit_ratio set to 1.0: deterministic feature engineering, no train/val model split\n",
    "# evaluator uses this for Gate 1 (ratio <1.5 passes)\n",
    "\n",
    "result = {\n",
    "    'feature': FEATURE_NAME,\n",
    "    'attempt': ATTEMPT,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'approach': 'Yield Decomposition Velocity',\n",
    "    'description': (\n",
    "        'Z-scored daily changes in 10Y breakeven inflation rate and TIPS real yield. '\n",
    "        '2 features: yc_be_vel_z (breakeven velocity = DGS10-DFII10 daily change z-score), '\n",
    "        'yc_tips_vel_z (real rate velocity = DFII10 daily change z-score). '\n",
    "        'Decomposes nominal yield dynamics into inflation premium and real rate components. '\n",
    "        'Both use shared zscore_window for temporal consistency. '\n",
    "        'Velocity not level avoids autocorrelation (level >0.99, velocity ~0.046).'\n",
    "    ),\n",
    "    'best_params': best_params,\n",
    "    'metrics': {\n",
    "        'overfit_ratio': 1.0,\n",
    "        'train_loss': 0.0,\n",
    "        'val_loss': 0.0,\n",
    "        'mi_sum_val': round(best_value, 6),\n",
    "        'mi_individual_val': val_mi,\n",
    "        'mi_individual_test': test_mi,\n",
    "        'mi_sum_test': round(sum(test_mi.values()), 6),\n",
    "        'autocorrelations': autocorr_results,\n",
    "        'max_internal_corr': round(max_internal_corr, 6),\n",
    "        'max_vif': round(max_vif, 4) if max_vif is not None else None,\n",
    "        'optuna_trials_completed': n_completed,\n",
    "        'optuna_best_value': round(best_value, 6),\n",
    "        'output_nan_counts': {col: int(output[col].isna().sum()) for col in OUTPUT_COLUMNS},\n",
    "    },\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': OUTPUT_COLUMNS,\n",
    "    'data_info': {\n",
    "        'total_aligned': n,\n",
    "        'train_samples': len(train_dates),\n",
    "        'val_samples': len(val_dates),\n",
    "        'test_samples': len(test_dates),\n",
    "        'date_range_start': str(output.index.min().date()),\n",
    "        'date_range_end': str(output.index.max().date()),\n",
    "        'fred_tickers': ['DGS10', 'DFII10'],\n",
    "        'gold_ticker': 'GC=F',\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/training_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "\n",
    "print('Saved: /kaggle/working/training_result.json')\n",
    "print(f'\\n=== Training complete! ===')\n",
    "print(f'Finished: {datetime.now().isoformat()}')\n",
    "print(f'Output columns: {OUTPUT_COLUMNS}')\n",
    "print(f'Best params: zscore_window={best_params[\"zscore_window\"]}')\n",
    "print(f'Best MI sum (val): {best_value:.6f}')\n",
    "print(f'Autocorrelations: {autocorr_results}')\n",
    "print(json.dumps(result, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
