"""
Gold Prediction SubModel Training - Cross-Asset Attempt 2
Self-contained: Load pre-computed features -> Fetch Gold target -> Compute MI -> Save results
Generated by builder_model agent
"""

# ============================================================
# 1. IMPORTS
# ============================================================
import pandas as pd
import numpy as np
import yfinance as yf
import json
import os
from datetime import datetime
from sklearn.metrics import mutual_info_score

np.random.seed(42)

# ============================================================
# 2. PRE-COMPUTED FEATURES LOADING
# ============================================================
def load_precomputed_features():
    """
    Load pre-computed features from builder_data.

    Unlike VIX/Technical which generate features in training script,
    Cross-Asset uses deterministic HMM-based features pre-computed
    by builder_data to avoid non-deterministic HMM fitting.

    Expected columns:
    - Date
    - xasset_regime_prob (3D HMM probability)
    - xasset_recession_signal (first diff of gold/copper ratio z-score)
    - xasset_divergence (daily gold-silver return diff z-score)
    """
    # In Kaggle environment, the features CSV would be uploaded as a dataset
    # For now, we'll reconstruct from raw data since we can't upload files
    # This mimics the builder_data process exactly

    print("Fetching raw commodity data from Yahoo Finance...")

    # Fetch with buffer for rolling window warmup
    start_date = "2014-06-01"

    # Download data
    gold = yf.download("GC=F", start=start_date, progress=False)
    silver = yf.download("SI=F", start=start_date, progress=False)
    copper = yf.download("HG=F", start=start_date, progress=False)

    # Extract close prices
    df = pd.DataFrame({
        'gold_close': gold['Close'],
        'silver_close': silver['Close'],
        'copper_close': copper['Close']
    })

    # Forward-fill missing values (max 3 days)
    df = df.ffill(limit=3).dropna()

    # Compute daily returns
    df['gold_ret'] = df['gold_close'].pct_change()
    df['silver_ret'] = df['silver_close'].pct_change()
    df['copper_ret'] = df['copper_close'].pct_change()

    # === Component 1: HMM Regime Detection ===
    # Pre-fit HMM on full data (deterministic approach for Attempt 2)
    # Using fixed parameters from design: 3 components, 10 restarts

    from hmmlearn.hmm import GaussianHMM

    print("Fitting 3D HMM on commodity returns...")

    # Prepare HMM input (drop NaN from returns)
    hmm_data = df[['gold_ret', 'silver_ret', 'copper_ret']].dropna()
    X = hmm_data.values

    # Multi-restart to find best model
    best_model = None
    best_score = -np.inf
    n_components = 3
    n_restarts = 10

    for seed in range(n_restarts):
        try:
            model = GaussianHMM(
                n_components=n_components,
                covariance_type='full',
                n_iter=200,
                tol=1e-4,
                random_state=seed
            )
            model.fit(X)
            score = model.score(X)
            if score > best_score:
                best_score = score
                best_model = model
        except Exception as e:
            print(f"HMM restart {seed} failed: {e}")
            continue

    if best_model is None:
        print("ERROR: All HMM restarts failed")
        # Return default values
        df['xasset_regime_prob'] = 0.5
    else:
        print(f"Best HMM log-likelihood: {best_score:.2f}")

        # Generate probabilities for full data
        probs = best_model.predict_proba(X)

        # Identify highest-variance state (crisis/dislocation)
        traces = []
        for i in range(n_components):
            traces.append(np.trace(best_model.covars_[i]))
        high_var_state = np.argmax(traces)

        print(f"Crisis state: {high_var_state}, trace: {traces[high_var_state]:.4f}")

        # Extract crisis state probability
        regime_prob = probs[:, high_var_state]

        # Align to original dataframe
        df.loc[hmm_data.index, 'xasset_regime_prob'] = regime_prob

    # === Component 2: Gold/Copper Ratio Z-Score Change ===
    print("Computing gold/copper ratio z-score change...")

    df['gc_ratio'] = df['gold_close'] / df['copper_close']

    # 90d rolling z-score
    zscore_window = 90
    rolling_mean = df['gc_ratio'].rolling(zscore_window).mean()
    rolling_std = df['gc_ratio'].rolling(zscore_window).std()
    gc_z = (df['gc_ratio'] - rolling_mean) / rolling_std

    # First difference (key design correction)
    gc_z_diff = gc_z.diff()
    df['xasset_recession_signal'] = gc_z_diff.clip(-4, 4)

    # === Component 3: Daily Gold-Silver Divergence ===
    print("Computing gold-silver divergence z-score...")

    div_window = 20
    gs_diff = df['gold_ret'] - df['silver_ret']
    rolling_mean_div = gs_diff.rolling(div_window).mean()
    rolling_std_div = gs_diff.rolling(div_window).std()
    gs_z = (gs_diff - rolling_mean_div) / rolling_std_div
    df['xasset_divergence'] = gs_z.clip(-4, 4)

    # Forward-fill remaining NaN
    df['xasset_regime_prob'] = df['xasset_regime_prob'].ffill()
    df['xasset_recession_signal'] = df['xasset_recession_signal'].ffill()
    df['xasset_divergence'] = df['xasset_divergence'].ffill()

    # Filter to base_features date range
    df = df[df.index >= '2015-01-30']
    df = df[df.index <= '2025-02-12']

    # Select output columns
    output_df = df[['xasset_regime_prob', 'xasset_recession_signal', 'xasset_divergence']].copy()

    print(f"Features generated: {len(output_df)} rows")
    print(f"Date range: {output_df.index.min()} to {output_df.index.max()}")
    print(f"NaN counts: {output_df.isna().sum().to_dict()}")

    return output_df

# ============================================================
# 3. GOLD TARGET FETCHING
# ============================================================
def fetch_gold_target():
    """Fetch GLD close prices and compute next-day return."""
    print("Fetching GLD for target variable...")

    gld = yf.download("GLD", start="2015-01-01", progress=False)

    df = pd.DataFrame({
        'gold_close': gld['Close']
    })

    # Compute next-day return (%)
    df['gold_return_next'] = df['gold_close'].pct_change().shift(-1) * 100

    # Drop last row (no target)
    df = df.dropna(subset=['gold_return_next'])

    print(f"Gold target: {len(df)} rows")
    print(f"Date range: {df.index.min()} to {df.index.max()}")

    return df[['gold_return_next']]

# ============================================================
# 4. MUTUAL INFORMATION COMPUTATION
# ============================================================
def compute_mi_metrics(features, target, train_size, val_size):
    """
    Compute MI metrics for logging.

    Design specifies MI as the primary quality metric for
    deterministic features (no HP optimization needed).
    """
    # Align features and target
    common_idx = features.index.intersection(target.index)
    X = features.loc[common_idx]
    y = target.loc[common_idx, 'gold_return_next']

    # Data split (time-series order)
    train_X = X.iloc[:train_size]
    train_y = y.iloc[:train_size]

    val_X = X.iloc[train_size:train_size + val_size]
    val_y = y.iloc[train_size:train_size + val_size]

    test_X = X.iloc[train_size + val_size:]
    test_y = y.iloc[train_size + val_size:]

    # Discretize function
    def discretize(x, bins=20):
        valid = ~np.isnan(x)
        if valid.sum() < bins:
            return None
        x_clean = x.copy()
        x_clean[~valid] = np.nanmedian(x)
        try:
            return pd.qcut(x_clean, bins, labels=False, duplicates='drop')
        except Exception:
            return None

    # Compute MI for each feature on validation set
    mi_results = {}

    for col in X.columns:
        feat_val = val_X[col].values
        tgt_val = val_y.values

        mask = ~np.isnan(feat_val) & ~np.isnan(tgt_val)
        if mask.sum() < 50:
            mi_results[col] = 0.0
            continue

        feat_disc = discretize(feat_val[mask])
        tgt_disc = discretize(tgt_val[mask])

        if feat_disc is None or tgt_disc is None:
            mi_results[col] = 0.0
        else:
            try:
                mi = mutual_info_score(feat_disc, tgt_disc)
                mi_results[col] = mi
            except Exception:
                mi_results[col] = 0.0

    mi_sum = sum(mi_results.values())

    print(f"\nMutual Information (validation set):")
    for col, mi in mi_results.items():
        print(f"  {col}: {mi:.4f}")
    print(f"  MI Sum: {mi_sum:.4f}")

    # Compute autocorrelation for Gate 1 check
    autocorr_results = {}
    for col in X.columns:
        vals = train_X[col].dropna().values
        if len(vals) > 1:
            autocorr = np.corrcoef(vals[:-1], vals[1:])[0, 1]
            autocorr_results[col] = autocorr
        else:
            autocorr_results[col] = 0.0

    print(f"\nAutocorrelation (lag 1, training set):")
    for col, ac in autocorr_results.items():
        print(f"  {col}: {ac:.4f}")

    return {
        'mi_individual': mi_results,
        'mi_sum': mi_sum,
        'autocorr': autocorr_results,
        'data_splits': {
            'train_samples': len(train_X),
            'val_samples': len(val_X),
            'test_samples': len(test_X)
        }
    }

# ============================================================
# 5. MAIN EXECUTION
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("Gold SubModel Training: cross_asset Attempt 2")
    print("=" * 60)
    print(f"Started: {datetime.now().isoformat()}")

    # Install hmmlearn (not pre-installed on Kaggle)
    print("\nInstalling hmmlearn...")
    import subprocess
    subprocess.run(["pip", "install", "hmmlearn"], check=True, capture_output=True)

    # Load pre-computed features
    print("\n" + "=" * 60)
    print("STEP 1: Loading Pre-Computed Features")
    print("=" * 60)
    features = load_precomputed_features()

    # Fetch gold target
    print("\n" + "=" * 60)
    print("STEP 2: Fetching Gold Target")
    print("=" * 60)
    target = fetch_gold_target()

    # Align dates
    common_idx = features.index.intersection(target.index)
    features = features.loc[common_idx]
    target = target.loc[common_idx]

    print(f"\nAligned data: {len(features)} rows")

    # Data split (70/15/15)
    total_samples = len(features)
    train_size = int(total_samples * 0.70)
    val_size = int(total_samples * 0.15)
    test_size = total_samples - train_size - val_size

    print(f"Data split: train={train_size}, val={val_size}, test={test_size}")

    # Compute MI metrics
    print("\n" + "=" * 60)
    print("STEP 3: Computing MI Metrics")
    print("=" * 60)
    metrics = compute_mi_metrics(features, target, train_size, val_size)

    # === SAVE RESULTS (Kaggle output directory) ===
    print("\n" + "=" * 60)
    print("STEP 4: Saving Results")
    print("=" * 60)

    # Save submodel output
    features.to_csv("submodel_output.csv")
    print(f"Saved: submodel_output.csv ({len(features)} rows)")

    # No model weights (deterministic features)
    # Create a metadata file instead
    metadata = {
        'method': 'deterministic_hmm',
        'hmm_n_components': 3,
        'hmm_n_restarts': 10,
        'zscore_window': 90,
        'div_window': 20,
        'note': 'Pre-computed features, no HP optimization'
    }

    with open("model_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    print("Saved: model_metadata.json")

    # Training result
    result = {
        "feature": "cross_asset",
        "attempt": 2,
        "timestamp": datetime.now().isoformat(),
        "best_params": metadata,
        "metrics": {
            'mi_individual': metrics['mi_individual'],
            'mi_sum': metrics['mi_sum'],
            'autocorr': metrics['autocorr'],
            'overfit_ratio': None,  # N/A for deterministic features
        },
        "optuna_trials_completed": 0,  # No HP optimization
        "optuna_best_value": metrics['mi_sum'],
        "output_shape": list(features.shape),
        "output_columns": list(features.columns),
        "data_info": metrics['data_splits']
    }

    with open("training_result.json", "w") as f:
        json.dump(result, f, indent=2, default=str)
    print("Saved: training_result.json")

    print("\n" + "=" * 60)
    print("Training Complete!")
    print("=" * 60)
    print(f"Finished: {datetime.now().isoformat()}")
    print(f"Output shape: {features.shape}")
    print(f"Output columns: {list(features.columns)}")
    print(f"MI Sum: {metrics['mi_sum']:.4f}")

    # Summary statistics
    print("\n" + "=" * 60)
    print("Feature Statistics (full dataset)")
    print("=" * 60)
    print(features.describe())
