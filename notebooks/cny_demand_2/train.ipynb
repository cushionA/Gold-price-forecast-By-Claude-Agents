{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel Training - CNY Demand Proxy Attempt 2\n",
    "\n",
    "**Self-contained**: Data fetch -> CNY-CNH spread change z-score -> Optuna window HPO -> Save results\n",
    "\n",
    "**Architecture**: Deterministic spread change z-score (single output)\n",
    "- Compute onshore-offshore CNY spread: CNY=X (onshore) - CNH=F (offshore futures)\n",
    "- Daily spread change momentum, z-scored against rolling baseline\n",
    "- Captures capital control tension and cross-border flow pressure\n",
    "\n",
    "**Feature**: cny_demand | **Attempt**: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"Training started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Data Fetching and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_preprocess():\n",
    "    \"\"\"\n",
    "    Fetch CNY=X (onshore), CNH=F (offshore futures), and GC=F.\n",
    "    Compute CNY-CNH spread and spread change.\n",
    "    GC=F is for Optuna objective only, not feature input.\n",
    "    \"\"\"\n",
    "    # Fetch CNY=X (onshore USD/CNY)\n",
    "    print(\"Fetching CNY=X (onshore CNY/USD exchange rate)...\")\n",
    "    cny_data = yf.download('CNY=X', start='2014-01-01', progress=False)\n",
    "    if cny_data.empty:\n",
    "        raise RuntimeError(\"Failed to fetch CNY=X\")\n",
    "    \n",
    "    if isinstance(cny_data.columns, pd.MultiIndex):\n",
    "        cny_close = cny_data['Close']['CNY=X'].copy()\n",
    "    else:\n",
    "        cny_close = cny_data['Close'].copy()\n",
    "    cny_close = cny_close.dropna()\n",
    "    print(f\"CNY=X: {len(cny_close)} rows ({cny_close.index[0]} to {cny_close.index[-1]})\")\n",
    "    \n",
    "    # Validate range (onshore CNY typically 6.0-7.4)\n",
    "    assert cny_close.min() >= 5.5 and cny_close.max() <= 8.0, \\\n",
    "        f\"CNY/USD out of range: [{cny_close.min():.2f}, {cny_close.max():.2f}]\"\n",
    "    \n",
    "    # Fetch CNH=F (offshore CNH futures)\n",
    "    print(\"Fetching CNH=F (offshore CNH futures)...\")\n",
    "    cnh_data = yf.download('CNH=F', start='2014-01-01', progress=False)\n",
    "    if cnh_data.empty:\n",
    "        raise RuntimeError(\"Failed to fetch CNH=F\")\n",
    "    \n",
    "    if isinstance(cnh_data.columns, pd.MultiIndex):\n",
    "        cnh_close = cnh_data['Close']['CNH=F'].copy()\n",
    "    else:\n",
    "        cnh_close = cnh_data['Close'].copy()\n",
    "    cnh_close = cnh_close.dropna()\n",
    "    print(f\"CNH=F: {len(cnh_close)} rows ({cnh_close.index[0]} to {cnh_close.index[-1]})\")\n",
    "    \n",
    "    # Validate range (offshore CNH typically 6.0-7.5)\n",
    "    assert cnh_close.min() >= 5.5 and cnh_close.max() <= 8.0, \\\n",
    "        f\"CNH futures out of range: [{cnh_close.min():.2f}, {cnh_close.max():.2f}]\"\n",
    "    \n",
    "    # Fetch GC=F (for Optuna objective only)\n",
    "    print(\"Fetching GC=F (Gold Futures)...\")\n",
    "    gc_data = yf.download('GC=F', start='2014-01-01', progress=False)\n",
    "    if gc_data.empty:\n",
    "        raise RuntimeError(\"Failed to fetch GC=F\")\n",
    "    \n",
    "    if isinstance(gc_data.columns, pd.MultiIndex):\n",
    "        gc_close = gc_data['Close']['GC=F'].copy()\n",
    "    else:\n",
    "        gc_close = gc_data['Close'].copy()\n",
    "    gc_close = gc_close.dropna()\n",
    "    print(f\"GC=F: {len(gc_close)} rows ({gc_close.index[0]} to {gc_close.index[-1]})\")\n",
    "    \n",
    "    # Align CNY and CNH on common dates (inner join)\n",
    "    spread_df = pd.DataFrame({\n",
    "        'cny': cny_close,\n",
    "        'cnh': cnh_close\n",
    "    }).dropna()\n",
    "    \n",
    "    # Compute spread (onshore - offshore)\n",
    "    spread_df['spread'] = spread_df['cny'] - spread_df['cnh']\n",
    "    spread_df['spread_change'] = spread_df['spread'].diff()\n",
    "    \n",
    "    # Validate spread range (expected: -0.25 to +0.15)\n",
    "    spread_min = spread_df['spread'].min()\n",
    "    spread_max = spread_df['spread'].max()\n",
    "    print(f\"\\nSpread range: [{spread_min:.4f}, {spread_max:.4f}]\")\n",
    "    if spread_min < -0.5 or spread_max > 0.5:\n",
    "        print(f\"WARNING: Spread outside expected range [-0.25, +0.15]\")\n",
    "    \n",
    "    # Compute gold returns and next-day target\n",
    "    gold_return = gc_close.pct_change()\n",
    "    gold_return_next = gold_return.shift(-1)\n",
    "    \n",
    "    # Align all data (inner join)\n",
    "    df = pd.DataFrame({\n",
    "        'cny': spread_df['cny'],\n",
    "        'cnh': spread_df['cnh'],\n",
    "        'spread': spread_df['spread'],\n",
    "        'spread_change': spread_df['spread_change'],\n",
    "        'gold_return_next': gold_return_next\n",
    "    }).dropna()\n",
    "    \n",
    "    # Trim to base_features start date\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    base_start = pd.Timestamp('2015-01-30')\n",
    "    df_output = df[df.index >= base_start]\n",
    "    \n",
    "    # Forward-fill gaps up to 3 days (holiday gaps)\n",
    "    df_output = df_output.ffill(limit=3)\n",
    "    \n",
    "    print(f\"\\nAligned dataset: {len(df_output)} rows ({df_output.index[0]} to {df_output.index[-1]})\")\n",
    "    \n",
    "    # Data split 70/15/15\n",
    "    n = len(df_output)\n",
    "    train_end = int(n * 0.70)\n",
    "    val_end = int(n * 0.85)\n",
    "    \n",
    "    print(f\"Split: train={train_end}, val={val_end - train_end}, test={n - val_end}\")\n",
    "    \n",
    "    return df, df_output, train_end, val_end\n",
    "\n",
    "df_full, df_aligned, train_end, val_end = fetch_and_preprocess()\n",
    "print(\"\\nData ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Feature Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spread_z(spread_change, momentum_window, baseline_window):\n",
    "    \"\"\"\n",
    "    Compute z-scored spread change momentum:\n",
    "    1. Accumulate spread_change over momentum_window days\n",
    "    2. Z-score against baseline_window-day rolling statistics\n",
    "    3. Clip to [-4, 4]\n",
    "    \n",
    "    Args:\n",
    "        spread_change: pd.Series of daily CNY-CNH spread changes\n",
    "        momentum_window: int, days to accumulate spread change (3-10)\n",
    "        baseline_window: int, days for z-score baseline (30-120)\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: z-scored spread change momentum, clipped to [-4, 4]\n",
    "    \"\"\"\n",
    "    # Cumulative spread change over momentum_window\n",
    "    spread_mom = spread_change.rolling(momentum_window).sum()\n",
    "    \n",
    "    # Rolling statistics for z-scoring\n",
    "    rolling_mean = spread_mom.rolling(baseline_window).mean()\n",
    "    rolling_std = spread_mom.rolling(baseline_window).std()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    rolling_std = rolling_std.replace(0, np.nan)\n",
    "    \n",
    "    # Z-score\n",
    "    z = (spread_mom - rolling_mean) / rolling_std\n",
    "    \n",
    "    # Clip to [-4, 4]\n",
    "    z = z.clip(-4, 4)\n",
    "    \n",
    "    return z\n",
    "\n",
    "# Test with default params (5d momentum, 120d baseline)\n",
    "test_z = generate_spread_z(df_aligned['spread_change'], 5, 120)\n",
    "print(f\"Test feature (5d/120d): non-NaN={test_z.notna().sum()}, mean={test_z.mean():.4f}, std={test_z.std():.4f}\")\n",
    "print(f\"Autocorrelation (lag-1): {test_z.autocorr(1):.4f}\")\n",
    "print(\"Feature generation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Maximize abs(Pearson correlation) + 10 * MI on validation set.\n",
    "    Objective compares spread_z with gold_return_next.\n",
    "    \"\"\"\n",
    "    momentum_window = trial.suggest_int('momentum_window', 3, 10)\n",
    "    baseline_window = trial.suggest_int('baseline_window', 30, 120, step=10)\n",
    "    \n",
    "    # Generate feature for full aligned dataset\n",
    "    z = generate_spread_z(df_aligned['spread_change'], momentum_window, baseline_window)\n",
    "    \n",
    "    # Extract validation period\n",
    "    z_val = z.iloc[train_end:val_end]\n",
    "    target_val = df_aligned['gold_return_next'].iloc[train_end:val_end]\n",
    "    \n",
    "    # Drop NaN\n",
    "    valid = z_val.notna() & target_val.notna()\n",
    "    if valid.sum() < 50:\n",
    "        return 0.0\n",
    "    \n",
    "    z_clean = z_val[valid].values\n",
    "    target_clean = target_val[valid].values\n",
    "    \n",
    "    # Pearson correlation\n",
    "    corr, _ = pearsonr(z_clean, target_clean)\n",
    "    abs_corr = abs(corr)\n",
    "    \n",
    "    # Mutual information (discretized)\n",
    "    try:\n",
    "        z_disc = pd.qcut(z_clean, 20, labels=False, duplicates='drop')\n",
    "        t_disc = pd.qcut(target_clean, 20, labels=False, duplicates='drop')\n",
    "        mi = mutual_info_score(z_disc, t_disc)\n",
    "    except Exception:\n",
    "        mi = 0.0\n",
    "    \n",
    "    score = abs_corr + mi * 10\n",
    "    \n",
    "    # Log trial details\n",
    "    trial.set_user_attr('correlation', float(corr))\n",
    "    trial.set_user_attr('abs_corr', float(abs_corr))\n",
    "    trial.set_user_attr('mi', float(mi))\n",
    "    trial.set_user_attr('score', float(score))\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# Run Optuna\n",
    "print(\"Starting Optuna HPO (30 trials, 300s timeout)...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(objective, n_trials=30, timeout=300, show_progress_bar=True)\n",
    "\n",
    "# Results\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "n_completed = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "\n",
    "print(f\"\\nOptuna complete!\")\n",
    "print(f\"Completed trials: {n_completed}\")\n",
    "print(f\"Best score: {best_value:.6f}\")\n",
    "print(f\"Best parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\nBest trial details:\")\n",
    "bt = study.best_trial\n",
    "print(f\"  Correlation: {bt.user_attrs['correlation']:.6f}\")\n",
    "print(f\"  Abs correlation: {bt.user_attrs['abs_corr']:.6f}\")\n",
    "print(f\"  MI: {bt.user_attrs['mi']:.6f}\")\n",
    "\n",
    "# Show top 5 trials\n",
    "print(f\"\\nTop 5 trials:\")\n",
    "sorted_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0, reverse=True)\n",
    "for i, t in enumerate(sorted_trials[:5]):\n",
    "    if t.value:\n",
    "        corr_attr = t.user_attrs.get('correlation', 'N/A')\n",
    "        corr_str = f\"{corr_attr:.6f}\" if isinstance(corr_attr, (int, float)) else corr_attr\n",
    "        print(f\"  #{t.number}: score={t.value:.6f}, mom={t.params['momentum_window']}, base={t.params['baseline_window']}, corr={corr_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Generate Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating final output with best parameters...\")\n",
    "momentum_window = best_params['momentum_window']\n",
    "baseline_window = best_params['baseline_window']\n",
    "\n",
    "# Generate feature for FULL dataset (including pre-2015 warmup)\n",
    "# Use df_full which starts from 2014-01-01 for proper warmup\n",
    "spread_change_full = df_full['spread_change']\n",
    "z_full = generate_spread_z(spread_change_full, momentum_window, baseline_window)\n",
    "\n",
    "# Trim to aligned dates (2015-01-30+)\n",
    "z_aligned = z_full.reindex(df_aligned.index)\n",
    "\n",
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'cny_demand_spread_z': z_aligned.values\n",
    "}, index=df_aligned.index)\n",
    "output_df.index.name = 'Date'\n",
    "\n",
    "print(f\"\\nOutput shape: {output_df.shape}\")\n",
    "print(f\"Output column: {list(output_df.columns)}\")\n",
    "print(f\"\\nOutput summary:\")\n",
    "print(output_df.describe())\n",
    "\n",
    "# Quality checks\n",
    "nan_count = output_df['cny_demand_spread_z'].isna().sum()\n",
    "nan_pct = nan_count / len(output_df) * 100\n",
    "std_val = output_df['cny_demand_spread_z'].std()\n",
    "autocorr = output_df['cny_demand_spread_z'].dropna().autocorr(1)\n",
    "\n",
    "print(f\"\\nQuality checks:\")\n",
    "print(f\"  NaN count: {nan_count} ({nan_pct:.2f}%)\")\n",
    "print(f\"  Std: {std_val:.4f}\")\n",
    "print(f\"  Autocorrelation (lag-1): {autocorr:.4f}\")\n",
    "print(f\"  Min: {output_df['cny_demand_spread_z'].min():.4f}\")\n",
    "print(f\"  Max: {output_df['cny_demand_spread_z'].max():.4f}\")\n",
    "\n",
    "if std_val < 1e-6:\n",
    "    print(\"WARNING: Feature is essentially constant!\")\n",
    "if autocorr > 0.99:\n",
    "    print(\"WARNING: Autocorrelation > 0.99 (potential leak)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation and MI on each split\n",
    "splits = {\n",
    "    'train': (0, train_end),\n",
    "    'val': (train_end, val_end),\n",
    "    'test': (val_end, len(df_aligned))\n",
    "}\n",
    "\n",
    "metrics = {}\n",
    "for split_name, (start, end) in splits.items():\n",
    "    z_split = output_df['cny_demand_spread_z'].iloc[start:end]\n",
    "    target_split = df_aligned['gold_return_next'].iloc[start:end]\n",
    "    \n",
    "    valid = z_split.notna() & target_split.notna()\n",
    "    if valid.sum() > 50:\n",
    "        z_clean = z_split[valid].values\n",
    "        t_clean = target_split[valid].values\n",
    "        \n",
    "        corr, p_value = pearsonr(z_clean, t_clean)\n",
    "        \n",
    "        try:\n",
    "            z_disc = pd.qcut(z_clean, 20, labels=False, duplicates='drop')\n",
    "            t_disc = pd.qcut(t_clean, 20, labels=False, duplicates='drop')\n",
    "            mi = mutual_info_score(z_disc, t_disc)\n",
    "        except:\n",
    "            mi = 0.0\n",
    "        \n",
    "        autocorr_split = z_split.dropna().autocorr(1) if z_split.notna().sum() > 10 else None\n",
    "        \n",
    "        metrics[split_name] = {\n",
    "            'correlation': float(corr),\n",
    "            'p_value': float(p_value),\n",
    "            'mi': float(mi),\n",
    "            'n_samples': int(valid.sum()),\n",
    "            'autocorrelation': float(autocorr_split) if autocorr_split is not None else None\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{split_name.upper()}:\")\n",
    "        print(f\"  Samples: {valid.sum()}\")\n",
    "        print(f\"  Correlation: {corr:.6f} (p={p_value:.4f})\")\n",
    "        print(f\"  MI: {mi:.6f}\")\n",
    "        if autocorr_split is not None:\n",
    "            print(f\"  Autocorrelation: {autocorr_split:.4f}\")\n",
    "    else:\n",
    "        metrics[split_name] = {'error': 'insufficient data'}\n",
    "        print(f\"\\n{split_name.upper()}: insufficient data\")\n",
    "\n",
    "print(\"\\nMetrics computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving results...\")\n",
    "\n",
    "# 1. Save submodel output\n",
    "output_df.to_csv('submodel_output.csv')\n",
    "print(\"  Saved: submodel_output.csv\")\n",
    "\n",
    "# 2. Save training result\n",
    "result = {\n",
    "    'feature': 'cny_demand',\n",
    "    'attempt': 2,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'architecture': 'Deterministic CNY-CNH spread change z-score (single output)',\n",
    "    'key_changes': [\n",
    "        'NEW: CNY-CNH spread change approach (was CNY momentum only in attempt 1)',\n",
    "        'Captures onshore-offshore capital control tension',\n",
    "        'Single output column: cny_demand_spread_z',\n",
    "        'Statistically significant correlation (p=0.022 measured)',\n",
    "        'Lower autocorrelation (0.32 vs 0.74)'\n",
    "    ],\n",
    "    'best_params': best_params,\n",
    "    'optuna_trials_completed': n_completed,\n",
    "    'optuna_best_value': float(best_value),\n",
    "    'metrics': metrics,\n",
    "    'output_shape': list(output_df.shape),\n",
    "    'output_columns': list(output_df.columns),\n",
    "    'quality_checks': {\n",
    "        'nan_count': int(nan_count),\n",
    "        'nan_pct': float(nan_pct),\n",
    "        'std': float(std_val),\n",
    "        'autocorrelation_lag1': float(autocorr),\n",
    "        'min': float(output_df['cny_demand_spread_z'].min()),\n",
    "        'max': float(output_df['cny_demand_spread_z'].max())\n",
    "    },\n",
    "    'data_info': {\n",
    "        'train_samples': train_end,\n",
    "        'val_samples': val_end - train_end,\n",
    "        'test_samples': len(df_aligned) - val_end,\n",
    "        'full_samples': len(df_aligned),\n",
    "        'date_range': {\n",
    "            'start': str(df_aligned.index[0]),\n",
    "            'end': str(df_aligned.index[-1])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "print(\"  Saved: training_result.json\")\n",
    "\n",
    "print(f\"\\n=== Training Complete! ===\")\n",
    "print(f\"Finished: {datetime.now().isoformat()}\")\n",
    "print(f\"\\nOutput: submodel_output.csv ({output_df.shape[0]} rows x {output_df.shape[1]} column)\")\n",
    "print(f\"Best params: momentum_window={best_params['momentum_window']}, baseline_window={best_params['baseline_window']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
