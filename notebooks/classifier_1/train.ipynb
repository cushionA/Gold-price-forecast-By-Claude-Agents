{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold DOWN Classifier - Attempt 1\n",
    "\n",
    "**Architecture**: XGBoost binary:logistic + optional focal loss  \n",
    "**Features**: 18 NEW features (different from regression model's 24)  \n",
    "**Purpose**: Detect DOWN days to ensemble with regression meta-model (attempt 7)  \n",
    "\n",
    "The regression model has 96% bullish bias - it catches 20/21 UP days but only 1/10 DOWN days.  \n",
    "This classifier uses volatility regime shifts, cross-asset stress, and reversal signals to improve DOWN detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install required packages\n",
    "import subprocess\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"xgboost\"], check=True)\n",
    "    import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"optuna\"], check=True)\n",
    "    import optuna\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, recall_score, precision_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "N_TRIALS = 100\n",
    "TIMEOUT = 3600  # 1 hour\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Gold DOWN Classifier - Attempt 1\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {datetime.now().isoformat()}\")\n",
    "print(f\"Configuration: {N_TRIALS} trials, seed={RANDOM_SEED}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Fetching and Preprocessing\n",
    "# This embeds the complete data fetching function from src/fetch_classifier.py\n",
    "\n",
    "def fetch_and_preprocess():\n",
    "    \"\"\"\n",
    "    Fetch all data sources and compute 18 classifier features.\n",
    "    Returns: train_df, val_df, test_df, full_df (each with 18 features + target)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import libraries\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "        from fredapi import Fred\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"yfinance\", \"fredapi\"], check=True)\n",
    "        import yfinance as yf\n",
    "        from fredapi import Fred\n",
    "    \n",
    "    # Get FRED API key (Kaggle Secrets with hardcoded fallback)\n",
    "    api_key = os.environ.get('FRED_API_KEY')\n",
    "    if api_key is None:\n",
    "        # Hardcoded fallback for Kaggle\n",
    "        api_key = \"3ffb68facdf6321e180e380c00e909c8\"\n",
    "        print(\"WARNING: Using hardcoded FRED_API_KEY\")\n",
    "    \n",
    "    fred = Fred(api_key=api_key)\n",
    "    \n",
    "    # Fetch raw data\n",
    "    START_DATE = \"2014-01-01\"  # 1 year extra for warmup\n",
    "    \n",
    "    print(\"Fetching yfinance data...\")\n",
    "    \n",
    "    # GC=F: Gold futures (OHLCV)\n",
    "    gc = yf.download('GC=F', start=START_DATE, progress=False, auto_adjust=True)\n",
    "    if isinstance(gc.columns, pd.MultiIndex):\n",
    "        gc.columns = [col[0] for col in gc.columns]\n",
    "    gc = gc[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    gc.columns = [f'GC_{col}' for col in gc.columns]\n",
    "    \n",
    "    # GLD: Gold ETF (for volume features)\n",
    "    gld = yf.download('GLD', start=START_DATE, progress=False, auto_adjust=True)\n",
    "    if isinstance(gld.columns, pd.MultiIndex):\n",
    "        gld.columns = [col[0] for col in gld.columns]\n",
    "    gld = gld[['Volume']].copy()\n",
    "    gld.columns = ['GLD_Volume']\n",
    "    \n",
    "    # SI=F: Silver futures\n",
    "    si = yf.download('SI=F', start=START_DATE, progress=False, auto_adjust=True)\n",
    "    if isinstance(si.columns, pd.MultiIndex):\n",
    "        si.columns = [col[0] for col in si.columns]\n",
    "    si = si[['Close']].copy()\n",
    "    si.columns = ['SI_Close']\n",
    "    \n",
    "    # HG=F: Copper futures\n",
    "    hg = yf.download('HG=F', start=START_DATE, progress=False, auto_adjust=True)\n",
    "    if isinstance(hg.columns, pd.MultiIndex):\n",
    "        hg.columns = [col[0] for col in hg.columns]\n",
    "    hg = hg[['Close']].copy()\n",
    "    hg.columns = ['HG_Close']\n",
    "    \n",
    "    # DX-Y.NYB: Dollar Index\n",
    "    dxy = yf.download('DX-Y.NYB', start=START_DATE, progress=False, auto_adjust=True)\n",
    "    if isinstance(dxy.columns, pd.MultiIndex):\n",
    "        dxy.columns = [col[0] for col in dxy.columns]\n",
    "    dxy = dxy[['Close']].copy()\n",
    "    dxy.columns = ['DXY_Close']\n",
    "    \n",
    "    # ^GSPC: S&P 500\n",
    "    spx = yf.download('^GSPC', start=START_DATE, progress=False, auto_adjust=True)\n",
    "    if isinstance(spx.columns, pd.MultiIndex):\n",
    "        spx.columns = [col[0] for col in spx.columns]\n",
    "    spx = spx[['Close']].copy()\n",
    "    spx.columns = ['SPX_Close']\n",
    "    \n",
    "    print(\"Fetching FRED data...\")\n",
    "    \n",
    "    # FRED series\n",
    "    gvz = pd.DataFrame({'GVZ': fred.get_series('GVZCLS', observation_start=START_DATE)})\n",
    "    vix = pd.DataFrame({'VIX': fred.get_series('VIXCLS', observation_start=START_DATE)})\n",
    "    dfii10 = pd.DataFrame({'DFII10': fred.get_series('DFII10', observation_start=START_DATE)})\n",
    "    dgs10 = pd.DataFrame({'DGS10': fred.get_series('DGS10', observation_start=START_DATE)})\n",
    "    \n",
    "    # Convert index to datetime\n",
    "    for df in [gvz, vix, dfii10, dgs10]:\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Merge all data\n",
    "    print(\"Merging data sources...\")\n",
    "    \n",
    "    df = gc.copy()\n",
    "    for data in [gld, si, hg, dxy, spx, gvz, vix, dfii10, dgs10]:\n",
    "        df = df.join(data, how='left')\n",
    "    \n",
    "    # Forward-fill missing values\n",
    "    fred_cols = ['GVZ', 'VIX', 'DFII10', 'DGS10']\n",
    "    for col in fred_cols:\n",
    "        df[col] = df[col].ffill(limit=5)\n",
    "    \n",
    "    yf_cols = [col for col in df.columns if col not in fred_cols]\n",
    "    for col in yf_cols:\n",
    "        df[col] = df[col].ffill(limit=3)\n",
    "    \n",
    "    # Drop any remaining NaN rows\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"Merged data: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Helper functions\n",
    "    def rolling_zscore(series, window=60):\n",
    "        mean = series.rolling(window).mean()\n",
    "        std = series.rolling(window).std()\n",
    "        return (series - mean) / std.clip(lower=1e-8)\n",
    "    \n",
    "    def rolling_beta(y, x, window=20):\n",
    "        cov = y.rolling(window).cov(x)\n",
    "        var = x.rolling(window).var()\n",
    "        return cov / var.clip(lower=1e-8)\n",
    "    \n",
    "    # Compute features\n",
    "    print(\"Computing features...\")\n",
    "    \n",
    "    # Gold return (for target and some features)\n",
    "    df['gold_return'] = df['GC_Close'].pct_change() * 100\n",
    "    \n",
    "    # Category A: Volatility Regime Features (5)\n",
    "    rv_10 = df['gold_return'].rolling(10).std()\n",
    "    rv_30 = df['gold_return'].rolling(30).std()\n",
    "    df['rv_ratio_10_30'] = rv_10 / rv_30.clip(lower=1e-8)\n",
    "    df['rv_ratio_10_30_z'] = rolling_zscore(df['rv_ratio_10_30'], 60)\n",
    "    df['gvz_level_z'] = rolling_zscore(df['GVZ'], 60)\n",
    "    df['gvz_vix_ratio'] = df['GVZ'] / df['VIX'].clip(lower=1e-8)\n",
    "    \n",
    "    daily_range = (df['GC_High'] - df['GC_Low']) / df['GC_Close'].clip(lower=1e-8)\n",
    "    avg_range = daily_range.rolling(20).mean()\n",
    "    df['intraday_range_ratio'] = daily_range / avg_range.clip(lower=1e-8)\n",
    "    \n",
    "    # Category B: Cross-Asset Stress Features (4)\n",
    "    vix_change = df['VIX'].pct_change() * 100\n",
    "    dxy_change = df['DXY_Close'].pct_change() * 100\n",
    "    spx_return = df['SPX_Close'].pct_change() * 100\n",
    "    yield_change = df['DGS10'].diff()\n",
    "    \n",
    "    vix_z = rolling_zscore(vix_change, 20)\n",
    "    dxy_z = rolling_zscore(dxy_change, 20)\n",
    "    spx_z = rolling_zscore(spx_return, 20)\n",
    "    yield_z = rolling_zscore(yield_change, 20)\n",
    "    \n",
    "    df['risk_off_score'] = vix_z + dxy_z - spx_z - yield_z\n",
    "    \n",
    "    gold_5d_ret = df['GC_Close'].pct_change(5) * 100\n",
    "    silver_5d_ret = df['SI_Close'].pct_change(5) * 100\n",
    "    divergence = gold_5d_ret - silver_5d_ret\n",
    "    df['gold_silver_ratio_change'] = rolling_zscore(divergence, 60)\n",
    "    \n",
    "    df['equity_gold_beta_20d'] = rolling_beta(df['gold_return'], spx_return, 20)\n",
    "    \n",
    "    copper_5d_ret = df['HG_Close'].pct_change(5) * 100\n",
    "    divergence_copper = gold_5d_ret - copper_5d_ret\n",
    "    df['gold_copper_ratio_change'] = rolling_zscore(divergence_copper, 60)\n",
    "    \n",
    "    # Category C: Rate and Currency Shock Features (3)\n",
    "    rate_change = df['DFII10'].diff()\n",
    "    rate_std_20 = rate_change.rolling(20).std()\n",
    "    df['rate_surprise'] = np.abs(rate_change) / rate_std_20.clip(lower=1e-8)\n",
    "    df['rate_surprise_signed'] = np.sign(rate_change) * df['rate_surprise']\n",
    "    \n",
    "    dxy_accel = dxy_change - dxy_change.shift(1)\n",
    "    df['dxy_acceleration'] = rolling_zscore(dxy_accel, 20)\n",
    "    \n",
    "    # Category D: Volume and Flow Features (2)\n",
    "    df['gld_volume_z'] = rolling_zscore(df['GLD_Volume'], 20)\n",
    "    df['volume_return_sign'] = np.sign(df['gold_return']) * df['gld_volume_z']\n",
    "    \n",
    "    # Category E: Momentum Context Features (2)\n",
    "    ret_5d = df['GC_Close'].pct_change(5) * 100\n",
    "    ret_20d = df['GC_Close'].pct_change(20) * 100\n",
    "    mom_div = ret_5d - ret_20d\n",
    "    df['momentum_divergence'] = rolling_zscore(mom_div, 60)\n",
    "    \n",
    "    high_20d = df['GC_Close'].rolling(20).max()\n",
    "    low_20d = df['GC_Close'].rolling(20).min()\n",
    "    range_20d = (high_20d - low_20d).clip(lower=1e-8)\n",
    "    df['distance_from_20d_high'] = (df['GC_Close'] - high_20d) / range_20d\n",
    "    \n",
    "    # Category F: Calendar Features (2)\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['month_of_year'] = df.index.month\n",
    "    \n",
    "    # Create target variable\n",
    "    df['target'] = (df['gold_return'].shift(-1) > 0).astype(int)\n",
    "    \n",
    "    # Select features and drop warmup rows\n",
    "    feature_cols = [\n",
    "        'rv_ratio_10_30', 'rv_ratio_10_30_z', 'gvz_level_z', 'gvz_vix_ratio',\n",
    "        'intraday_range_ratio', 'risk_off_score', 'gold_silver_ratio_change',\n",
    "        'equity_gold_beta_20d', 'gold_copper_ratio_change', 'rate_surprise',\n",
    "        'rate_surprise_signed', 'dxy_acceleration', 'gld_volume_z',\n",
    "        'volume_return_sign', 'momentum_divergence', 'distance_from_20d_high',\n",
    "        'day_of_week', 'month_of_year'\n",
    "    ]\n",
    "    \n",
    "    df_final = df[feature_cols + ['target', 'gold_return']].copy()\n",
    "    df_final = df_final.dropna()\n",
    "    \n",
    "    print(f\"Final dataset: {len(df_final)} rows, {len(feature_cols)} features\")\n",
    "    \n",
    "    # Verify target balance\n",
    "    up_pct = 100 * (df_final['target'] == 1).sum() / len(df_final)\n",
    "    down_pct = 100 * (df_final['target'] == 0).sum() / len(df_final)\n",
    "    print(f\"Target balance: UP={up_pct:.2f}%, DOWN={down_pct:.2f}%\")\n",
    "    \n",
    "    # Train/val/test split (70/15/15)\n",
    "    n = len(df_final)\n",
    "    train_end = int(n * 0.70)\n",
    "    val_end = int(n * 0.85)\n",
    "    \n",
    "    train_df = df_final.iloc[:train_end].copy()\n",
    "    val_df = df_final.iloc[train_end:val_end].copy()\n",
    "    test_df = df_final.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"Train: {len(train_df)} rows ({train_df.index.min()} to {train_df.index.max()})\")\n",
    "    print(f\"Val:   {len(val_df)} rows ({val_df.index.min()} to {val_df.index.max()})\")\n",
    "    print(f\"Test:  {len(test_df)} rows ({test_df.index.min()} to {test_df.index.max()})\")\n",
    "    \n",
    "    return train_df, val_df, test_df, df_final, feature_cols\n",
    "\n",
    "# Execute data fetching\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA FETCHING\")\n",
    "print(\"=\"*80)\n",
    "train_df, val_df, test_df, full_df, feature_cols = fetch_and_preprocess()\n",
    "print(\"\\nData fetching complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for NaN\n",
    "print(\"\\nChecking for NaN values...\")\n",
    "nan_counts = full_df[feature_cols].isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"WARNING: NaN values found:\")\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "else:\n",
    "    print(\"✓ No NaN values\")\n",
    "\n",
    "# Feature statistics\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(full_df[feature_cols].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Check feature correlations\n",
    "print(\"\\nChecking feature correlations...\")\n",
    "corr_matrix = full_df[feature_cols].corr()\n",
    "high_corr = []\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.85:\n",
    "            high_corr.append((feature_cols[i], feature_cols[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"WARNING: High correlations found:\")\n",
    "    for f1, f2, corr in high_corr:\n",
    "        print(f\"  {f1} <-> {f2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"✓ No feature pairs with correlation > 0.85\")\n",
    "\n",
    "# Class balance per split\n",
    "print(\"\\nClass balance per split:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    up_pct = 100 * (df['target'] == 1).sum() / len(df)\n",
    "    down_pct = 100 * (df['target'] == 0).sum() / len(df)\n",
    "    print(f\"  {name}: UP={up_pct:.2f}%, DOWN={down_pct:.2f}%\")\n",
    "\n",
    "print(\"\\nData validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Focal Loss Implementation\n",
    "def focal_loss_obj(gamma=2.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Custom focal loss objective for XGBoost.\n",
    "    gamma: focusing parameter (default 2.0)\n",
    "    alpha: balancing parameter (default 0.5 for balanced)\n",
    "    \"\"\"\n",
    "    def obj(y_pred, dtrain):\n",
    "        y_true = dtrain.get_label()\n",
    "        # Sigmoid to get probability\n",
    "        p = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        # Probability of true class\n",
    "        p_t = y_true * p + (1 - y_true) * (1 - p)\n",
    "        # Alpha balancing\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        # Focal loss gradient\n",
    "        grad = alpha_t * (1 - p_t)**gamma * (p - y_true)\n",
    "        # Approximate Hessian\n",
    "        hess = alpha_t * (1 - p_t)**gamma * p * (1 - p)\n",
    "        hess = np.maximum(hess, 1e-7)  # numerical stability\n",
    "        return grad, hess\n",
    "    return obj\n",
    "\n",
    "print(\"Focal loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Optuna HPO\nprint(\"\\n\" + \"=\"*80)\nprint(\"HYPERPARAMETER OPTIMIZATION\")\nprint(\"=\"*80)\n\n# Prepare data\nX_train = train_df[feature_cols].values\ny_train = train_df['target'].values\nX_val = val_df[feature_cols].values\ny_val = val_df['target'].values\n\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_cols)\ndval = xgb.DMatrix(X_val, label=y_val, feature_names=feature_cols)\n\ndef objective(trial):\n    \"\"\"\n    Optuna objective function.\n    Returns: composite metric (40% F1_DOWN + 30% ROC-AUC + 30% Balanced Accuracy)\n    \"\"\"\n    # Suggest hyperparameters\n    max_depth = trial.suggest_int('max_depth', 2, 5)\n    min_child_weight = trial.suggest_int('min_child_weight', 8, 30)\n    subsample = trial.suggest_float('subsample', 0.4, 0.9)\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.2, 0.8)\n    reg_lambda = trial.suggest_float('reg_lambda', 0.5, 20.0, log=True)\n    reg_alpha = trial.suggest_float('reg_alpha', 0.1, 10.0, log=True)\n    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.05, log=True)\n    n_estimators = trial.suggest_int('n_estimators', 100, 500, step=50)\n    scale_pos_weight = trial.suggest_float('scale_pos_weight', 0.8, 1.5)\n    use_focal = trial.suggest_categorical('use_focal', [True, False])\n    \n    params = {\n        'max_depth': max_depth,\n        'min_child_weight': min_child_weight,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'reg_lambda': reg_lambda,\n        'reg_alpha': reg_alpha,\n        'learning_rate': learning_rate,\n        'scale_pos_weight': scale_pos_weight,\n        'tree_method': 'hist',\n        'verbosity': 0,\n        'seed': RANDOM_SEED + trial.number,\n        'eval_metric': 'logloss',\n    }\n    \n    # Focal loss: pass as obj argument, not in params\n    custom_obj = None\n    if use_focal:\n        focal_gamma = trial.suggest_float('focal_gamma', 0.5, 3.0)\n        custom_obj = focal_loss_obj(gamma=focal_gamma)\n    else:\n        params['objective'] = 'binary:logistic'\n    \n    # Train model\n    evals = [(dtrain, 'train'), (dval, 'val')]\n    model = xgb.train(\n        params,\n        dtrain,\n        num_boost_round=n_estimators,\n        obj=custom_obj,\n        evals=evals,\n        early_stopping_rounds=80,\n        verbose_eval=False\n    )\n    \n    # Predictions (raw margin if focal loss, need sigmoid)\n    y_val_raw = model.predict(dval)\n    if use_focal:\n        y_val_pred_proba = 1.0 / (1.0 + np.exp(-y_val_raw))\n    else:\n        y_val_pred_proba = y_val_raw\n    \n    y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n    \n    # Metrics\n    balanced_acc = balanced_accuracy_score(y_val, y_val_pred)\n    f1_down = f1_score(y_val, y_val_pred, pos_label=0, zero_division=0)\n    try:\n        roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n    except ValueError:\n        roc_auc = 0.5\n    \n    # Composite objective (maximize)\n    composite = 0.40 * f1_down + 0.30 * roc_auc + 0.30 * balanced_acc\n    \n    trial.set_user_attr('balanced_acc', float(balanced_acc))\n    trial.set_user_attr('f1_down', float(f1_down))\n    trial.set_user_attr('roc_auc', float(roc_auc))\n    trial.set_user_attr('down_recall', float(recall_score(y_val, y_val_pred, pos_label=0)))\n    trial.set_user_attr('up_recall', float(recall_score(y_val, y_val_pred, pos_label=1)))\n    \n    return composite\n\n# Run Optuna\nprint(f\"\\nRunning Optuna with {N_TRIALS} trials...\")\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(\n    direction='maximize',\n    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n)\n\nstudy.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)\n\nbt = study.best_trial\nprint(f\"\\nOptuna complete! Best trial #{bt.number}:\")\nprint(f\"  Value: {bt.value:.4f}\")\nprint(f\"  Balanced Acc: {bt.user_attrs['balanced_acc']:.4f}\")\nprint(f\"  F1 DOWN: {bt.user_attrs['f1_down']:.4f}\")\nprint(f\"  ROC-AUC: {bt.user_attrs['roc_auc']:.4f}\")\nprint(f\"  DOWN recall: {bt.user_attrs['down_recall']:.4f}\")\nprint(f\"  UP recall: {bt.user_attrs['up_recall']:.4f}\")\nprint(f\"\\n  Params: {study.best_params}\")\n\nbest_params = study.best_params"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Train Final Model with Best Params\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL MODEL TRAINING\")\nprint(\"=\"*80)\n\n# Prepare final params\nfinal_params = {\n    'max_depth': best_params['max_depth'],\n    'min_child_weight': best_params['min_child_weight'],\n    'subsample': best_params['subsample'],\n    'colsample_bytree': best_params['colsample_bytree'],\n    'reg_lambda': best_params['reg_lambda'],\n    'reg_alpha': best_params['reg_alpha'],\n    'learning_rate': best_params['learning_rate'],\n    'scale_pos_weight': best_params['scale_pos_weight'],\n    'tree_method': 'hist',\n    'verbosity': 0,\n    'seed': RANDOM_SEED,\n    'eval_metric': 'logloss',\n}\n\ncustom_obj_final = None\nuse_focal_final = best_params['use_focal']\nif use_focal_final:\n    custom_obj_final = focal_loss_obj(gamma=best_params['focal_gamma'])\n    print(f\"Using focal loss (gamma={best_params['focal_gamma']:.2f})\")\nelse:\n    final_params['objective'] = 'binary:logistic'\n    print(\"Using binary:logistic\")\n\n# Train final model\nevals = [(dtrain, 'train'), (dval, 'val')]\nfinal_model = xgb.train(\n    final_params,\n    dtrain,\n    num_boost_round=best_params['n_estimators'],\n    obj=custom_obj_final,\n    evals=evals,\n    early_stopping_rounds=80,\n    verbose_eval=False\n)\n\n# Prepare test set\nX_test = test_df[feature_cols].values\ny_test = test_df['target'].values\ndtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_cols)\n\n# Predictions on all splits (handle focal loss raw margin → sigmoid)\ndef predict_proba(model, dmat, is_focal):\n    raw = model.predict(dmat)\n    if is_focal:\n        return 1.0 / (1.0 + np.exp(-raw))\n    return raw\n\ntrain_pred_proba = predict_proba(final_model, dtrain, use_focal_final)\ntrain_pred = (train_pred_proba > 0.5).astype(int)\n\nval_pred_proba = predict_proba(final_model, dval, use_focal_final)\nval_pred = (val_pred_proba > 0.5).astype(int)\n\ntest_pred_proba = predict_proba(final_model, dtest, use_focal_final)\ntest_pred = (test_pred_proba > 0.5).astype(int)\n\n# Compute metrics for all splits\ndef compute_metrics(y_true, y_pred, y_pred_proba, split_name):\n    metrics = {\n        'balanced_acc': balanced_accuracy_score(y_true, y_pred),\n        'down_recall': recall_score(y_true, y_pred, pos_label=0),\n        'down_precision': precision_score(y_true, y_pred, pos_label=0, zero_division=0),\n        'f1_down': f1_score(y_true, y_pred, pos_label=0, zero_division=0),\n        'up_recall': recall_score(y_true, y_pred, pos_label=1),\n        'up_precision': precision_score(y_true, y_pred, pos_label=1, zero_division=0),\n    }\n    try:\n        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n    except ValueError:\n        metrics['roc_auc'] = 0.5\n    \n    print(f\"\\n{split_name} Metrics:\")\n    print(f\"  Balanced Accuracy: {metrics['balanced_acc']:.4f}\")\n    print(f\"  DOWN Recall: {metrics['down_recall']:.4f}\")\n    print(f\"  DOWN Precision: {metrics['down_precision']:.4f}\")\n    print(f\"  DOWN F1: {metrics['f1_down']:.4f}\")\n    print(f\"  UP Recall: {metrics['up_recall']:.4f}\")\n    print(f\"  UP Precision: {metrics['up_precision']:.4f}\")\n    print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n    \n    return metrics\n\ntrain_metrics = compute_metrics(y_train, train_pred, train_pred_proba, \"TRAIN\")\nval_metrics = compute_metrics(y_val, val_pred, val_pred_proba, \"VALIDATION\")\ntest_metrics = compute_metrics(y_test, test_pred, test_pred_proba, \"TEST\")\n\n# Confusion matrix (test set)\nprint(\"\\nTest Set Confusion Matrix:\")\ncm = confusion_matrix(y_test, test_pred)\nprint(\"             Predicted\")\nprint(\"           DOWN    UP\")\nprint(f\"Actual DOWN  {cm[0,0]:3d}  {cm[0,1]:3d}\")\nprint(f\"       UP    {cm[1,0]:3d}  {cm[1,1]:3d}\")\n\nprint(\"\\nFinal model training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature Importance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance (gain-based)\n",
    "importance = final_model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame([\n",
    "    {'feature': k, 'gain': v} for k, v in importance.items()\n",
    "]).sort_values('gain', ascending=False)\n",
    "\n",
    "# Normalize to percentages\n",
    "importance_df['gain_pct'] = 100 * importance_df['gain'] / importance_df['gain'].sum()\n",
    "\n",
    "print(\"\\nFeature Importance (Gain):\")\n",
    "for idx, row in importance_df.iterrows():\n",
    "    print(f\"  {row['feature']:30s} {row['gain_pct']:6.2f}%\")\n",
    "\n",
    "print(f\"\\nTop 5 features: {', '.join(importance_df.head(5)['feature'].tolist())}\")\n",
    "print(f\"Bottom 3 features: {', '.join(importance_df.tail(3)['feature'].tolist())}\")\n",
    "\n",
    "# Check concentration\n",
    "top_feature_pct = importance_df.iloc[0]['gain_pct']\n",
    "if top_feature_pct > 30:\n",
    "    print(f\"\\nWARNING: Top feature dominance at {top_feature_pct:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n✓ Feature importance well-distributed (top feature: {top_feature_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Ensemble with Regression Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: This section would load regression predictions from the dataset.\n",
    "# For now, we'll simulate ensemble metrics based on classifier performance.\n",
    "\n",
    "print(\"\\nNote: Full ensemble evaluation requires regression model predictions.\")\n",
    "print(\"Ensemble integration will be completed post-training.\")\n",
    "\n",
    "# Placeholder for threshold optimization\n",
    "# This would iterate over thresholds and compute ensemble DA + Sharpe\n",
    "optimal_threshold = 0.55  # Default\n",
    "\n",
    "print(f\"\\nOptimal threshold (default): {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Generate Classifier Output\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING CLASSIFIER OUTPUT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare full dataset predictions\n",
    "X_full = full_df[feature_cols].values\n",
    "dfull = xgb.DMatrix(X_full, feature_names=feature_cols)\n",
    "full_pred_proba = final_model.predict(dfull)\n",
    "\n",
    "# Create output DataFrame\n",
    "classifier_output = pd.DataFrame({\n",
    "    'Date': full_df.index.strftime('%Y-%m-%d'),\n",
    "    'p_up': full_pred_proba,\n",
    "    'p_down': 1 - full_pred_proba,\n",
    "    'predicted_direction': (full_pred_proba > 0.5).astype(int)\n",
    "})\n",
    "\n",
    "# Save classifier output\n",
    "classifier_output.to_csv('classifier.csv', index=False)\n",
    "print(f\"\\nClassifier output saved: {len(classifier_output)} rows\")\n",
    "print(f\"  Date range: {classifier_output['Date'].min()} to {classifier_output['Date'].max()}\")\n",
    "print(f\"  UP predictions: {(classifier_output['predicted_direction'] == 1).sum()} ({100*(classifier_output['predicted_direction'] == 1).sum()/len(classifier_output):.2f}%)\")\n",
    "print(f\"  DOWN predictions: {(classifier_output['predicted_direction'] == 0).sum()} ({100*(classifier_output['predicted_direction'] == 0).sum()/len(classifier_output):.2f}%)\")\n",
    "\n",
    "# P(DOWN) distribution\n",
    "print(f\"\\nP(DOWN) distribution:\")\n",
    "print(f\"  Mean: {classifier_output['p_down'].mean():.4f}\")\n",
    "print(f\"  Std: {classifier_output['p_down'].std():.4f}\")\n",
    "print(f\"  Min: {classifier_output['p_down'].min():.4f}\")\n",
    "print(f\"  Max: {classifier_output['p_down'].max():.4f}\")\n",
    "\n",
    "if classifier_output['p_down'].std() < 0.05:\n",
    "    print(\"\\nWARNING: P(DOWN) distribution has low variance (< 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: 2026 Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2026 PREDICTIONS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter 2026 dates\n",
    "classifier_2026 = classifier_output[classifier_output['Date'] >= '2026-01-01'].copy()\n",
    "\n",
    "if len(classifier_2026) > 0:\n",
    "    print(f\"\\n2026 predictions: {len(classifier_2026)} days\")\n",
    "    print(f\"  UP predictions: {(classifier_2026['predicted_direction'] == 1).sum()}\")\n",
    "    print(f\"  DOWN predictions: {(classifier_2026['predicted_direction'] == 0).sum()}\")\n",
    "    print(f\"  Mean P(DOWN): {classifier_2026['p_down'].mean():.4f}\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\nSample 2026 predictions:\")\n",
    "    print(classifier_2026.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo 2026 data available in current dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save Training Results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare training result JSON\n",
    "training_result = {\n",
    "    \"feature\": \"classifier\",\n",
    "    \"attempt\": 1,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"architecture\": \"XGBoost binary:logistic\" + (\" + focal loss\" if best_params['use_focal'] else \"\"),\n",
    "    \"n_features\": len(feature_cols),\n",
    "    \"features\": feature_cols,\n",
    "    \"best_params\": best_params,\n",
    "    \"optuna_trials_completed\": len(study.trials),\n",
    "    \"optuna_best_value\": float(study.best_value),\n",
    "    \"standalone_metrics\": {\n",
    "        \"train\": {k: float(v) for k, v in train_metrics.items()},\n",
    "        \"val\": {k: float(v) for k, v in val_metrics.items()},\n",
    "        \"test\": {k: float(v) for k, v in test_metrics.items()},\n",
    "    },\n",
    "    \"ensemble_metrics\": {\n",
    "        \"optimal_threshold\": float(optimal_threshold),\n",
    "        \"note\": \"Full ensemble evaluation pending regression model integration\"\n",
    "    },\n",
    "    \"feature_importance\": {\n",
    "        \"ranked\": importance_df[['feature', 'gain_pct']].to_dict('records'),\n",
    "        \"top5\": importance_df.head(5)['feature'].tolist(),\n",
    "        \"bottom3\": importance_df.tail(3)['feature'].tolist(),\n",
    "    },\n",
    "    \"class_balance\": {\n",
    "        \"train_up_pct\": float(100 * (y_train == 1).sum() / len(y_train)),\n",
    "        \"train_down_pct\": float(100 * (y_train == 0).sum() / len(y_train)),\n",
    "        \"val_up_pct\": float(100 * (y_val == 1).sum() / len(y_val)),\n",
    "        \"val_down_pct\": float(100 * (y_val == 0).sum() / len(y_val)),\n",
    "        \"test_up_pct\": float(100 * (y_test == 1).sum() / len(y_test)),\n",
    "        \"test_down_pct\": float(100 * (y_test == 0).sum() / len(y_test)),\n",
    "    },\n",
    "    \"p_down_distribution\": {\n",
    "        \"mean\": float(classifier_output['p_down'].mean()),\n",
    "        \"std\": float(classifier_output['p_down'].std()),\n",
    "        \"min\": float(classifier_output['p_down'].min()),\n",
    "        \"max\": float(classifier_output['p_down'].max()),\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"train_samples\": len(train_df),\n",
    "        \"val_samples\": len(val_df),\n",
    "        \"test_samples\": len(test_df),\n",
    "        \"full_samples\": len(full_df),\n",
    "        \"train_date_range\": f\"{train_df.index.min()} to {train_df.index.max()}\",\n",
    "        \"val_date_range\": f\"{val_df.index.min()} to {val_df.index.max()}\",\n",
    "        \"test_date_range\": f\"{test_df.index.min()} to {test_df.index.max()}\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save training result\n",
    "with open('training_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_result, f, indent=2, default=str)\n",
    "print(\"\\nTraining result saved: training_result.json\")\n",
    "\n",
    "# Save model\n",
    "final_model.save_model('model.json')\n",
    "print(\"Model saved: model.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {datetime.now().isoformat()}\")\n",
    "print(\"\\nOutputs:\")\n",
    "print(\"  - classifier.csv: Classifier predictions for all dates\")\n",
    "print(\"  - training_result.json: Complete training metrics and metadata\")\n",
    "print(\"  - model.json: Trained XGBoost model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Diagnostic Plots\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC PLOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. P(DOWN) distribution\n",
    "axes[0, 0].hist(classifier_output['p_down'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('P(DOWN) Distribution')\n",
    "axes[0, 0].set_xlabel('P(DOWN)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Feature Importance\n",
    "top_10 = importance_df.head(10)\n",
    "axes[0, 1].barh(top_10['feature'], top_10['gain_pct'])\n",
    "axes[0, 1].set_title('Top 10 Feature Importance (Gain %)')\n",
    "axes[0, 1].set_xlabel('Gain %')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. Confusion Matrix (Test)\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "im = axes[1, 0].imshow(cm, cmap='Blues')\n",
    "axes[1, 0].set_title('Test Set Confusion Matrix')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "axes[1, 0].set_xticks([0, 1])\n",
    "axes[1, 0].set_yticks([0, 1])\n",
    "axes[1, 0].set_xticklabels(['DOWN', 'UP'])\n",
    "axes[1, 0].set_yticklabels(['DOWN', 'UP'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = axes[1, 0].text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "# 4. Metrics Comparison\n",
    "metrics_names = ['Balanced Acc', 'DOWN F1', 'ROC-AUC']\n",
    "train_vals = [train_metrics['balanced_acc'], train_metrics['f1_down'], train_metrics['roc_auc']]\n",
    "val_vals = [val_metrics['balanced_acc'], val_metrics['f1_down'], val_metrics['roc_auc']]\n",
    "test_vals = [test_metrics['balanced_acc'], test_metrics['f1_down'], test_metrics['roc_auc']]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.25\n",
    "axes[1, 1].bar(x - width, train_vals, width, label='Train')\n",
    "axes[1, 1].bar(x, val_vals, width, label='Val')\n",
    "axes[1, 1].bar(x + width, test_vals, width, label='Test')\n",
    "axes[1, 1].set_title('Metrics Comparison Across Splits')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics_names)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('diagnostics.png', dpi=100, bbox_inches='tight')\n",
    "print(\"\\nDiagnostic plots saved: diagnostics.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}