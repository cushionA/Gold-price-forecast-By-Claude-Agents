{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel Training - real_rate Attempt 9\n",
    "\n",
    "**Method**: Real Yield Curve Shape + Slope-Curvature Interaction (5-Feature Design)\n",
    "\n",
    "**Change from Attempt 7**: All 4 attempt-7 features retained + 5th interaction feature added.\n",
    "- Attempt 7 (4 feat incl rr_slope_level_z): Gate 3 PASS via Sharpe (+0.329), DA fail (-0.00149pp)\n",
    "- Attempt 8 (3 feat excl rr_slope_level_z): Gate 3 PASS via MAE (-0.0203%), Sharpe fail (-0.260)\n",
    "- Attempt 9 goal: retain all 4 attempt-7 features (preserve Sharpe) + add slope-curvature interaction\n",
    "  (low-autocorr, orthogonal) to push DA into positive territory.\n",
    "\n",
    "**New Feature**: rr_slope_curvature_interaction_z = rolling_zscore(slope.diff() * curvature.diff(), 60)\n",
    "- Captures simultaneous slope AND curvature stress events (butterfly distortions)\n",
    "- Expected autocorr near 0 (product of two near-iid series)\n",
    "- Does NOT add another high-autocorr feature (avoids attempt 8's Sharpe degradation pattern)\n",
    "\n",
    "**Self-contained**: Data fetch (FRED public CSV) + Feature computation + Gate evaluation + Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import urllib.request\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(f'Started: {datetime.now().isoformat()}')\n",
    "print('Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: FRED Public CSV Data Fetcher (No API key required)\n",
    "def fetch_fred_series(series_id, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Fetch FRED series using public CSV endpoint (no API key required).\n",
    "    Returns a pandas Series indexed by date.\n",
    "    \"\"\"\n",
    "    url = f'https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}'\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        with urllib.request.urlopen(req, timeout=60) as response:\n",
    "            csv_data = response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Failed to fetch FRED series {series_id}: {e}')\n",
    "\n",
    "    df = pd.read_csv(StringIO(csv_data))\n",
    "    date_col = df.columns[0]\n",
    "    val_col = df.columns[1]\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df = df.set_index(date_col)\n",
    "    series = pd.to_numeric(df[val_col], errors='coerce')\n",
    "    series.name = series_id\n",
    "    if start_date:\n",
    "        series = series[series.index >= pd.Timestamp(start_date)]\n",
    "    if end_date:\n",
    "        series = series[series.index <= pd.Timestamp(end_date)]\n",
    "    return series\n",
    "\n",
    "print('FRED public CSV fetcher ready (no API key required)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Fetching - DFII5, DFII10, DFII30\n",
    "print('=' * 60)\n",
    "print('DATA FETCHING')\n",
    "print('=' * 60)\n",
    "\n",
    "FETCH_START = '2014-01-01'\n",
    "FETCH_END = '2025-02-28'\n",
    "SCHEMA_START = '2015-01-02'\n",
    "SCHEMA_END = '2025-02-12'\n",
    "\n",
    "# All 3 DFII series needed for slope and curvature computation\n",
    "series_ids = ['DFII5', 'DFII10', 'DFII30']\n",
    "raw_series = {}\n",
    "for sid in series_ids:\n",
    "    s = fetch_fred_series(sid, start_date=FETCH_START, end_date=FETCH_END)\n",
    "    raw_series[sid] = s\n",
    "    n_clean = s.dropna()\n",
    "    print(f'  {sid}: {len(n_clean)} obs, {n_clean.index[0].date()} to {n_clean.index[-1].date()}')\n",
    "\n",
    "df_raw = pd.DataFrame(raw_series)\n",
    "df_raw.index = pd.to_datetime(df_raw.index)\n",
    "df_raw = df_raw.sort_index()\n",
    "\n",
    "df = df_raw.dropna()\n",
    "print(f'\\nCombined (all 3 non-null): {len(df)} obs, {df.index[0].date()} to {df.index[-1].date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Intermediate Series Computation\n",
    "print('=' * 60)\n",
    "print('INTERMEDIATE SERIES')\n",
    "print('=' * 60)\n",
    "\n",
    "df['slope'] = df['DFII30'] - df['DFII5']\n",
    "df['curvature'] = 2 * df['DFII10'] - df['DFII5'] - df['DFII30']\n",
    "\n",
    "print(f'Slope range: {df[\"slope\"].min():.3f} to {df[\"slope\"].max():.3f} (mean={df[\"slope\"].mean():.3f})')\n",
    "print(f'Curvature range: {df[\"curvature\"].min():.3f} to {df[\"curvature\"].max():.3f} (mean={df[\"curvature\"].mean():.3f})')\n",
    "\n",
    "df['dfii10_chg'] = df['DFII10'].diff()\n",
    "df['slope_chg'] = df['slope'].diff()\n",
    "df['curvature_chg'] = df['curvature'].diff()\n",
    "\n",
    "# Interaction: product of slope_chg and curvature_chg\n",
    "# Captures simultaneous butterfly distortion events\n",
    "df['slope_curvature_interaction'] = df['slope_chg'] * df['curvature_chg']\n",
    "\n",
    "print(f'DFII10 daily chg: [{df[\"dfii10_chg\"].min():.4f}, {df[\"dfii10_chg\"].max():.4f}]')\n",
    "print(f'Slope daily chg: [{df[\"slope_chg\"].min():.4f}, {df[\"slope_chg\"].max():.4f}]')\n",
    "print(f'Curvature daily chg: [{df[\"curvature_chg\"].min():.4f}, {df[\"curvature_chg\"].max():.4f}]')\n",
    "print(f'Slope*Curvature interaction: [{df[\"slope_curvature_interaction\"].min():.6f}, {df[\"slope_curvature_interaction\"].max():.6f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Submodel Dataset Path Resolution\n",
    "print('=' * 60)\n",
    "print('DATASET PATH RESOLUTION')\n",
    "print('=' * 60)\n",
    "\n",
    "candidate_paths = [\n",
    "    '/kaggle/input/datasets/bigbigzabuton/gold-prediction-submodels',\n",
    "    '../input/gold-prediction-submodels',\n",
    "    '/kaggle/input/gold-prediction-submodels',\n",
    "]\n",
    "SUBMODEL_PATH = None\n",
    "for cp in candidate_paths:\n",
    "    if os.path.exists(cp):\n",
    "        SUBMODEL_PATH = cp\n",
    "        print(f'Found submodel dataset at: {cp}')\n",
    "        break\n",
    "\n",
    "if SUBMODEL_PATH is None:\n",
    "    print('ERROR: gold-prediction-submodels not found!')\n",
    "    for root in ['/kaggle/input', '../input']:\n",
    "        if os.path.exists(root):\n",
    "            print(f'Available at {root}: {os.listdir(root)}')\n",
    "    raise RuntimeError('gold-prediction-submodels dataset not found. Check kernel-metadata.json dataset_sources.')\n",
    "\n",
    "submodel_files = os.listdir(SUBMODEL_PATH)\n",
    "print(f'Submodel files ({len(submodel_files)}): {submodel_files[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Feature Computation (Deterministic - 5 features)\n",
    "print('=' * 60)\n",
    "print('FEATURE COMPUTATION (ATTEMPT 9: 4 ATTEMPT-7 FEATURES + 1 INTERACTION)')\n",
    "print('=' * 60)\n",
    "print('Design:')\n",
    "print('  Features 1-4: same as attempt 7 (rr_level_change_z, rr_slope_chg_z,')\n",
    "print('                rr_curvature_chg_z, rr_slope_level_z)')\n",
    "print('  Feature 5 (NEW): rr_slope_curvature_interaction_z')\n",
    "print('    = rolling_zscore(slope.diff() * curvature.diff(), 60)')\n",
    "print('    Captures butterfly distortion events, low autocorr, orthogonal to existing')\n",
    "print()\n",
    "\n",
    "def rolling_zscore(series, window, min_periods_ratio=0.5):\n",
    "    \"\"\"Rolling z-score normalization. Returns nan during warmup.\"\"\"\n",
    "    min_p = max(10, int(window * min_periods_ratio))\n",
    "    mu = series.rolling(window, min_periods=min_p).mean()\n",
    "    sigma = series.rolling(window, min_periods=min_p).std()\n",
    "    sigma = sigma.where(sigma > 1e-10, np.nan)\n",
    "    z = (series - mu) / sigma\n",
    "    return z.clip(-4, 4)\n",
    "\n",
    "# Feature 1: rr_level_change_z - standardized daily change in 10Y real yield\n",
    "# Window=30: captures ~1.5 month regime of rate velocity\n",
    "rr_level_change_z = rolling_zscore(df['dfii10_chg'], 30)\n",
    "\n",
    "# Feature 2: rr_slope_chg_z - standardized daily change in slope (30Y-5Y)\n",
    "# Window=60: captures medium-term steepening/flattening dynamics\n",
    "rr_slope_chg_z = rolling_zscore(df['slope_chg'], 60)\n",
    "\n",
    "# Feature 3: rr_curvature_chg_z - standardized daily change in curvature\n",
    "# Window=60: captures belly distortion dynamics\n",
    "rr_curvature_chg_z = rolling_zscore(df['curvature_chg'], 60)\n",
    "\n",
    "# Feature 4: rr_slope_level_z - current slope level (regime indicator)\n",
    "# Re-included from attempt 7. Critical for Sharpe improvement (+0.329 in attempt 7).\n",
    "# Removal in attempt 8 reversed Sharpe to -0.260.\n",
    "rr_slope_level_z = rolling_zscore(df['slope'], 60)\n",
    "\n",
    "# Feature 5 (NEW): rr_slope_curvature_interaction_z\n",
    "# Product of slope_chg and curvature_chg: captures butterfly distortion events\n",
    "# when BOTH slope AND curvature are simultaneously stressed.\n",
    "# Low autocorr (product of two near-iid series) - will not degrade MAE like rr_slope_level_z\n",
    "# Orthogonal to individual features by construction of cross-products\n",
    "rr_slope_curvature_interaction_z = rolling_zscore(df['slope_curvature_interaction'], 60)\n",
    "\n",
    "features = pd.DataFrame({\n",
    "    'rr_level_change_z': rr_level_change_z,\n",
    "    'rr_slope_chg_z': rr_slope_chg_z,\n",
    "    'rr_curvature_chg_z': rr_curvature_chg_z,\n",
    "    'rr_slope_level_z': rr_slope_level_z,\n",
    "    'rr_slope_curvature_interaction_z': rr_slope_curvature_interaction_z,\n",
    "}, index=df.index)\n",
    "\n",
    "print('Features computed (5 total):')\n",
    "for col in features.columns:\n",
    "    n_valid = features[col].notna().sum()\n",
    "    n_nan = features[col].isna().sum()\n",
    "    nan_pct = n_nan / len(features) * 100\n",
    "    ac = features[col].autocorr(1)\n",
    "    std = features[col].std()\n",
    "    print(f'  {col}: valid={n_valid}, nan={n_nan} ({nan_pct:.1f}%), autocorr={ac:.4f}, std={std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Align to Schema Date Range\n",
    "print('=' * 60)\n",
    "print('ALIGNMENT TO SCHEMA DATES')\n",
    "print('=' * 60)\n",
    "\n",
    "import yfinance as yf\n",
    "gold = yf.download('GC=F', start=FETCH_START, end=FETCH_END, auto_adjust=True, progress=False)\n",
    "gold_dates = gold.index\n",
    "print(f'Gold trading calendar: {len(gold_dates)} dates, {gold_dates[0].date()} to {gold_dates[-1].date()}')\n",
    "\n",
    "schema_dates = gold_dates[(gold_dates >= SCHEMA_START) & (gold_dates <= SCHEMA_END)]\n",
    "print(f'Schema dates: {len(schema_dates)} dates, {schema_dates[0].date()} to {schema_dates[-1].date()}')\n",
    "\n",
    "features_aligned = features.reindex(schema_dates, method='ffill', limit=1)\n",
    "\n",
    "gold_close = gold['Close'].squeeze()\n",
    "gold_return = gold_close.pct_change() * 100\n",
    "gold_return_next = gold_return.shift(-1)\n",
    "gold_return_next_schema = gold_return_next.reindex(schema_dates)\n",
    "\n",
    "print(f'Features aligned shape: {features_aligned.shape}')\n",
    "print(f'NaN counts after alignment:')\n",
    "for col in features_aligned.columns:\n",
    "    n_nan = features_aligned[col].isna().sum()\n",
    "    print(f'  {col}: {n_nan} NaN ({n_nan/len(features_aligned)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Data Split (70/15/15)\n",
    "print('=' * 60)\n",
    "print('DATA SPLIT')\n",
    "print('=' * 60)\n",
    "\n",
    "common_mask = features_aligned.notna().all(axis=1) & gold_return_next_schema.notna()\n",
    "common_dates = schema_dates[common_mask]\n",
    "print(f'Common dates (all 5 features + target): {len(common_dates)}')\n",
    "\n",
    "n = len(common_dates)\n",
    "n_train = int(n * 0.70)\n",
    "n_val = int(n * 0.15)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_dates = common_dates[:n_train]\n",
    "val_dates = common_dates[n_train:n_train + n_val]\n",
    "test_dates = common_dates[n_train + n_val:]\n",
    "\n",
    "print(f'Train: {len(train_dates)} ({train_dates[0].date()} to {train_dates[-1].date()})')\n",
    "print(f'Val:   {len(val_dates)} ({val_dates[0].date()} to {val_dates[-1].date()})')\n",
    "print(f'Test:  {len(test_dates)} ({test_dates[0].date()} to {test_dates[-1].date()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Gate 1 - Standalone Quality Check\n",
    "print('=' * 60)\n",
    "print('GATE 1: STANDALONE QUALITY')\n",
    "print('=' * 60)\n",
    "\n",
    "gate1_results = {}\n",
    "\n",
    "for col in features_aligned.columns:\n",
    "    feat = features_aligned[col].dropna()\n",
    "    nan_pct = features_aligned[col].isna().mean() * 100\n",
    "    std_val = feat.std()\n",
    "    autocorr = feat.autocorr(1)\n",
    "\n",
    "    is_constant = std_val < 0.01\n",
    "    is_all_nan = len(feat) == 0\n",
    "    high_autocorr = autocorr > 0.99\n",
    "\n",
    "    pass_gate1 = not is_constant and not is_all_nan and not high_autocorr\n",
    "\n",
    "    gate1_results[col] = {\n",
    "        'nan_pct': float(nan_pct),\n",
    "        'std': float(std_val),\n",
    "        'autocorr': float(autocorr),\n",
    "        'is_constant': bool(is_constant),\n",
    "        'is_all_nan': bool(is_all_nan),\n",
    "        'high_autocorr': bool(high_autocorr),\n",
    "        'pass': bool(pass_gate1)\n",
    "    }\n",
    "\n",
    "    status = 'PASS' if pass_gate1 else 'FAIL'\n",
    "    print(f'  {col}: {status} | nan={nan_pct:.1f}%, std={std_val:.4f}, autocorr={autocorr:.4f}')\n",
    "\n",
    "gate1_pass = all(r['pass'] for r in gate1_results.values())\n",
    "\n",
    "overfit_ratio = 1.0  # deterministic features, no training\n",
    "print(f'\\nOverfit ratio: {overfit_ratio:.2f} (deterministic, no training)')\n",
    "print(f'Gate 1 Overall: {\"PASS\" if gate1_pass else \"FAIL\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Gate 2 - Information Gain (MI test)\n",
    "print('=' * 60)\n",
    "print('GATE 2: INFORMATION GAIN')\n",
    "print('=' * 60)\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def compute_mi(feature, target, n_bins=20):\n",
    "    \"\"\"Compute mutual information using quantile binning.\"\"\"\n",
    "    mask = feature.notna() & target.notna()\n",
    "    f = feature[mask]\n",
    "    t = target[mask]\n",
    "    if len(f) < 100:\n",
    "        return 0.0\n",
    "    try:\n",
    "        f_binned = pd.qcut(f, q=n_bins, labels=False, duplicates='drop')\n",
    "        t_binned = pd.qcut(t, q=n_bins, labels=False, duplicates='drop')\n",
    "        return float(mutual_info_score(f_binned, t_binned))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "base_mi_total = None\n",
    "try:\n",
    "    base_path = os.path.join(SUBMODEL_PATH, 'base_features.csv')\n",
    "    if not os.path.exists(base_path):\n",
    "        base_path = os.path.join(SUBMODEL_PATH, 'meta_model_input.csv')\n",
    "    base_df = pd.read_csv(base_path, index_col=0, parse_dates=True)\n",
    "    base_df.index = pd.to_datetime(base_df.index)\n",
    "    print(f'Loaded base features: {base_df.shape}')\n",
    "\n",
    "    target_test = gold_return_next_schema.reindex(test_dates)\n",
    "    base_test = base_df.reindex(test_dates)\n",
    "\n",
    "    base_mi_total = 0.0\n",
    "    for col in base_df.select_dtypes(include=[np.number]).columns[:10]:\n",
    "        mi = compute_mi(base_test[col], target_test)\n",
    "        base_mi_total += mi\n",
    "    print(f'Baseline MI (top 10 base features, test set): {base_mi_total:.6f}')\n",
    "except Exception as e:\n",
    "    print(f'WARNING: Could not load base features: {e}')\n",
    "    base_mi_total = 0.1\n",
    "\n",
    "target_test = gold_return_next_schema.reindex(test_dates)\n",
    "feat_test = features_aligned.reindex(test_dates)\n",
    "\n",
    "new_mi_total = 0.0\n",
    "per_feature_mi = {}\n",
    "for col in features_aligned.columns:\n",
    "    mi = compute_mi(feat_test[col], target_test)\n",
    "    new_mi_total += mi\n",
    "    per_feature_mi[col] = float(mi)\n",
    "    print(f'  MI({col}) = {mi:.6f}')\n",
    "\n",
    "print(f'\\nNew features MI total: {new_mi_total:.6f}')\n",
    "print(f'Baseline MI total: {base_mi_total:.6f}')\n",
    "\n",
    "if base_mi_total > 0:\n",
    "    mi_increase_pct = (new_mi_total / base_mi_total) * 100\n",
    "else:\n",
    "    mi_increase_pct = 999.0\n",
    "print(f'MI increase: {mi_increase_pct:.2f}% (threshold: > 5%)')\n",
    "gate2_mi_pass = mi_increase_pct > 5.0\n",
    "print(f'Gate 2 MI: {\"PASS\" if gate2_mi_pass else \"FAIL\"}')\n",
    "\n",
    "from numpy.linalg import inv\n",
    "feat_clean = feat_test.dropna()\n",
    "if len(feat_clean) > 10:\n",
    "    X = feat_clean.values\n",
    "    corr = np.corrcoef(X.T)\n",
    "    try:\n",
    "        vif_values = np.diag(inv(corr))\n",
    "        max_vif = float(np.max(vif_values))\n",
    "    except Exception:\n",
    "        max_vif = 999.0\n",
    "else:\n",
    "    max_vif = 0.0\n",
    "print(f'Max VIF: {max_vif:.3f} (threshold: < 10)')\n",
    "gate2_vif_pass = max_vif < 10.0\n",
    "\n",
    "target_full = gold_return_next_schema\n",
    "rolling_corr_stds = []\n",
    "for col in features_aligned.columns:\n",
    "    merged = pd.concat([features_aligned[col], target_full], axis=1).dropna()\n",
    "    if len(merged) > 120:\n",
    "        rc = merged.iloc[:,0].rolling(60).corr(merged.iloc[:,1])\n",
    "        rolling_corr_stds.append(float(rc.std()))\n",
    "max_rolling_corr_std = max(rolling_corr_stds) if rolling_corr_stds else 0.0\n",
    "print(f'Max rolling corr std: {max_rolling_corr_std:.4f} (threshold: < 0.15)')\n",
    "gate2_stability_pass = max_rolling_corr_std < 0.15\n",
    "\n",
    "gate2_pass = gate2_mi_pass and gate2_vif_pass\n",
    "print(f'Gate 2 Overall: {\"PASS\" if gate2_pass else \"FAIL\"} (MI={gate2_mi_pass}, VIF={gate2_vif_pass}, stability={gate2_stability_pass})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Gate 3 - Ablation Test (4-fold cross-validation)\n",
    "print('=' * 60)\n",
    "print('GATE 3: ABLATION TEST (4-FOLD CV)')\n",
    "print('=' * 60)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "meta_input_path = os.path.join(SUBMODEL_PATH, 'meta_model_input.csv')\n",
    "if os.path.exists(meta_input_path):\n",
    "    meta_df = pd.read_csv(meta_input_path, index_col=0, parse_dates=True)\n",
    "    print(f'Meta model input loaded: {meta_df.shape}')\n",
    "else:\n",
    "    print('meta_model_input.csv not found - constructing from base_features_raw.csv + submodel CSVs')\n",
    "    base_path = os.path.join(SUBMODEL_PATH, 'base_features_raw.csv')\n",
    "    meta_df = pd.read_csv(base_path, index_col=0, parse_dates=True)\n",
    "    print(f'  base_features_raw.csv: {meta_df.shape}')\n",
    "    sm_files = sorted([f for f in os.listdir(SUBMODEL_PATH)\n",
    "                       if f.endswith('.csv') and f != 'base_features_raw.csv'\n",
    "                       and 'real_rate' not in f])\n",
    "    for fname in sm_files:\n",
    "        try:\n",
    "            sm_df = pd.read_csv(os.path.join(SUBMODEL_PATH, fname), index_col=0, parse_dates=True)\n",
    "            sm_df.index = pd.to_datetime(sm_df.index)\n",
    "            meta_df = meta_df.join(sm_df, how='left')\n",
    "            print(f'  {fname}: {sm_df.shape}')\n",
    "        except Exception as e:\n",
    "            print(f'  WARNING: Could not load {fname}: {e}')\n",
    "    print(f'Meta model input (constructed): {meta_df.shape}')\n",
    "\n",
    "meta_df.index = pd.to_datetime(meta_df.index)\n",
    "print(f'Columns: {list(meta_df.columns[:10])}...')\n",
    "\n",
    "target_col = 'gold_return_next' if 'gold_return_next' in meta_df.columns else meta_df.columns[0]\n",
    "print(f'Target column: {target_col}')\n",
    "\n",
    "base_feature_cols = [c for c in meta_df.columns if c != target_col]\n",
    "print(f'Base feature columns: {len(base_feature_cols)}')\n",
    "\n",
    "def compute_direction_accuracy(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return 0.5\n",
    "    return float((np.sign(y_pred[mask]) == np.sign(y_true[mask])).mean())\n",
    "\n",
    "def compute_sharpe(y_true, y_pred, cost_bps=5):\n",
    "    position = np.sign(y_pred)\n",
    "    strategy_ret = position * y_true\n",
    "    position_changes = np.abs(np.diff(np.concatenate([[0], position])))\n",
    "    costs = position_changes * (cost_bps / 10000.0)\n",
    "    net_ret = strategy_ret - costs\n",
    "    if net_ret.std() < 1e-10:\n",
    "        return 0.0\n",
    "    return float(net_ret.mean() / net_ret.std() * np.sqrt(252))\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "folds = []\n",
    "n_common = len(common_dates)\n",
    "fold_size = n_common // 4\n",
    "for i in range(4):\n",
    "    test_start = i * fold_size\n",
    "    test_end = (i + 1) * fold_size if i < 3 else n_common\n",
    "    train_idx = list(range(0, test_start))\n",
    "    test_idx = list(range(test_start, test_end))\n",
    "    if len(train_idx) < 50:\n",
    "        print(f'  Fold {i+1}: skipped (insufficient training data)')\n",
    "        continue\n",
    "    folds.append((train_idx, test_idx))\n",
    "\n",
    "print(f'Running {len(folds)} folds...')\n",
    "\n",
    "base_da_folds = []\n",
    "base_sharpe_folds = []\n",
    "base_mae_folds = []\n",
    "ext_da_folds = []\n",
    "ext_sharpe_folds = []\n",
    "ext_mae_folds = []\n",
    "\n",
    "for fold_i, (train_idx, test_idx) in enumerate(folds):\n",
    "    fold_train_dates = common_dates[train_idx]\n",
    "    fold_test_dates = common_dates[test_idx]\n",
    "\n",
    "    base_train = meta_df[base_feature_cols].reindex(fold_train_dates)\n",
    "    base_test_fold = meta_df[base_feature_cols].reindex(fold_test_dates)\n",
    "    y_train = meta_df[target_col].reindex(fold_train_dates)\n",
    "    y_test = meta_df[target_col].reindex(fold_test_dates)\n",
    "\n",
    "    new_train = features_aligned.reindex(fold_train_dates)\n",
    "    new_test = features_aligned.reindex(fold_test_dates)\n",
    "\n",
    "    ext_train = pd.concat([base_train, new_train], axis=1)\n",
    "    ext_test = pd.concat([base_test_fold, new_test], axis=1)\n",
    "\n",
    "    base_train_clean = base_train.join(y_train).dropna()\n",
    "    ext_train_clean = ext_train.join(y_train).dropna()\n",
    "    base_test_clean = base_test_fold.join(y_test).dropna()\n",
    "    ext_test_clean = ext_test.join(y_test).dropna()\n",
    "\n",
    "    if len(base_train_clean) < 50 or len(base_test_clean) < 10:\n",
    "        print(f'  Fold {fold_i+1}: skipped (insufficient data)')\n",
    "        continue\n",
    "\n",
    "    base_model = xgb.XGBRegressor(**xgb_params)\n",
    "    base_model.fit(base_train_clean[base_feature_cols], base_train_clean[target_col])\n",
    "    base_pred = base_model.predict(base_test_clean[base_feature_cols])\n",
    "    y_test_base = base_test_clean[target_col].values\n",
    "\n",
    "    base_da = compute_direction_accuracy(y_test_base, base_pred)\n",
    "    base_sh = compute_sharpe(y_test_base, base_pred)\n",
    "    base_mae_val = mean_absolute_error(y_test_base, base_pred)\n",
    "    base_da_folds.append(base_da)\n",
    "    base_sharpe_folds.append(base_sh)\n",
    "    base_mae_folds.append(base_mae_val)\n",
    "\n",
    "    ext_cols = [c for c in ext_train.columns if c != target_col]\n",
    "    ext_model = xgb.XGBRegressor(**xgb_params)\n",
    "    ext_model.fit(ext_train_clean[ext_cols], ext_train_clean[target_col])\n",
    "    ext_pred = ext_model.predict(ext_test_clean[ext_cols])\n",
    "    y_test_ext = ext_test_clean[target_col].values\n",
    "\n",
    "    ext_da = compute_direction_accuracy(y_test_ext, ext_pred)\n",
    "    ext_sh = compute_sharpe(y_test_ext, ext_pred)\n",
    "    ext_mae_val = mean_absolute_error(y_test_ext, ext_pred)\n",
    "    ext_da_folds.append(ext_da)\n",
    "    ext_sharpe_folds.append(ext_sh)\n",
    "    ext_mae_folds.append(ext_mae_val)\n",
    "\n",
    "    print(f'  Fold {fold_i+1}: Base DA={base_da:.4f}, Ext DA={ext_da:.4f} (delta={ext_da-base_da:+.4f})')\n",
    "    print(f'           Base Sharpe={base_sh:.4f}, Ext Sharpe={ext_sh:.4f} (delta={ext_sh-base_sh:+.4f})')\n",
    "    print(f'           Base MAE={base_mae_val:.4f}, Ext MAE={ext_mae_val:.4f} (delta={ext_mae_val-base_mae_val:+.4f})')\n",
    "\n",
    "base_da_avg = float(np.mean(base_da_folds))\n",
    "ext_da_avg = float(np.mean(ext_da_folds))\n",
    "base_sharpe_avg = float(np.mean(base_sharpe_folds))\n",
    "ext_sharpe_avg = float(np.mean(ext_sharpe_folds))\n",
    "base_mae_avg = float(np.mean(base_mae_folds))\n",
    "ext_mae_avg = float(np.mean(ext_mae_folds))\n",
    "\n",
    "da_delta = ext_da_avg - base_da_avg\n",
    "sharpe_delta = ext_sharpe_avg - base_sharpe_avg\n",
    "mae_delta = ext_mae_avg - base_mae_avg\n",
    "\n",
    "print(f'\\n--- Gate 3 Summary ---')\n",
    "print(f'Direction Accuracy: {base_da_avg:.4f} -> {ext_da_avg:.4f} (delta={da_delta:+.4f})')\n",
    "print(f'Sharpe Ratio:       {base_sharpe_avg:.4f} -> {ext_sharpe_avg:.4f} (delta={sharpe_delta:+.4f})')\n",
    "print(f'MAE:                {base_mae_avg:.4f} -> {ext_mae_avg:.4f} (delta={mae_delta:+.4f})')\n",
    "\n",
    "gate3_da_pass = da_delta >= 0.005\n",
    "gate3_sharpe_pass = sharpe_delta >= 0.05\n",
    "gate3_mae_pass = mae_delta <= -0.01\n",
    "gate3_pass = gate3_da_pass or gate3_sharpe_pass or gate3_mae_pass\n",
    "print(f'Gate 3: DA={gate3_da_pass}, Sharpe={gate3_sharpe_pass}, MAE={gate3_mae_pass}')\n",
    "print(f'Gate 3 Overall: {\"PASS\" if gate3_pass else \"FAIL\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save Outputs\n",
    "print('=' * 60)\n",
    "print('SAVING OUTPUTS')\n",
    "print('=' * 60)\n",
    "\n",
    "# 1. Submodel output CSV (5 columns)\n",
    "output = features_aligned.copy()\n",
    "output.index.name = 'date'\n",
    "output.to_csv('submodel_output.csv')\n",
    "print(f'Saved submodel_output.csv: {output.shape}')\n",
    "print(f'Columns: {list(output.columns)}')\n",
    "\n",
    "# 2. Training result JSON\n",
    "result = {\n",
    "    'feature': 'real_rate',\n",
    "    'attempt': 9,\n",
    "    'method': 'deterministic_yield_curve_5feature_with_interaction',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'description': 'Real yield curve: 4 attempt-7 features retained + slope-curvature interaction z-score as 5th feature. Attempt 7 achieved Sharpe +0.329 via rr_slope_level_z. Attempt 8 removed it but Sharpe reversed to -0.260 (MAE improved). Attempt 9 retains rr_slope_level_z + adds low-autocorr interaction feature to push DA positive.',\n",
    "    'design_choices': {\n",
    "        'retained_from_attempt_7': ['rr_level_change_z', 'rr_slope_chg_z', 'rr_curvature_chg_z', 'rr_slope_level_z'],\n",
    "        'new_in_attempt_9': 'rr_slope_curvature_interaction_z = rolling_zscore(slope.diff() * curvature.diff(), 60)',\n",
    "        'rationale': 'Interaction feature captures butterfly distortion events. Low autocorr (product of two near-iid series). Orthogonal to existing features.'\n",
    "    },\n",
    "    'data_sources': ['FRED:DFII5', 'FRED:DFII10', 'FRED:DFII30'],\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': list(output.columns),\n",
    "    'gate1': {\n",
    "        'pass': gate1_pass,\n",
    "        'overfit_ratio': overfit_ratio,\n",
    "        'checks': gate1_results\n",
    "    },\n",
    "    'gate2': {\n",
    "        'pass': gate2_pass,\n",
    "        'checks': {\n",
    "            'mi': {\n",
    "                'new_total': float(new_mi_total),\n",
    "                'baseline_total': float(base_mi_total),\n",
    "                'increase': float(mi_increase_pct),\n",
    "                'pass': bool(gate2_mi_pass)\n",
    "            },\n",
    "            'vif': {\n",
    "                'max': float(max_vif),\n",
    "                'pass': bool(gate2_vif_pass)\n",
    "            },\n",
    "            'stability': {\n",
    "                'max_rolling_corr_std': float(max_rolling_corr_std),\n",
    "                'pass': bool(gate2_stability_pass)\n",
    "            }\n",
    "        },\n",
    "        'per_feature_mi': per_feature_mi\n",
    "    },\n",
    "    'gate3': {\n",
    "        'pass': gate3_pass,\n",
    "        'baseline': {\n",
    "            'direction_accuracy': base_da_avg,\n",
    "            'sharpe_ratio': base_sharpe_avg,\n",
    "            'mae': base_mae_avg\n",
    "        },\n",
    "        'extended': {\n",
    "            'direction_accuracy': ext_da_avg,\n",
    "            'sharpe_ratio': ext_sharpe_avg,\n",
    "            'mae': ext_mae_avg\n",
    "        },\n",
    "        'checks': {\n",
    "            'direction': {\n",
    "                'delta': float(da_delta),\n",
    "                'threshold': 0.005,\n",
    "                'pass': bool(gate3_da_pass)\n",
    "            },\n",
    "            'sharpe': {\n",
    "                'delta': float(sharpe_delta),\n",
    "                'threshold': 0.05,\n",
    "                'pass': bool(gate3_sharpe_pass)\n",
    "            },\n",
    "            'mae': {\n",
    "                'delta': float(mae_delta),\n",
    "                'threshold': -0.01,\n",
    "                'pass': bool(gate3_mae_pass)\n",
    "            }\n",
    "        },\n",
    "        'fold_results': {\n",
    "            'base_da': base_da_folds,\n",
    "            'ext_da': ext_da_folds,\n",
    "            'base_sharpe': base_sharpe_folds,\n",
    "            'ext_sharpe': ext_sharpe_folds,\n",
    "            'base_mae': base_mae_folds,\n",
    "            'ext_mae': ext_mae_folds\n",
    "        }\n",
    "    },\n",
    "    'feature_stats': {\n",
    "        col: {\n",
    "            'mean': float(output[col].mean()),\n",
    "            'std': float(output[col].std()),\n",
    "            'min': float(output[col].min()),\n",
    "            'max': float(output[col].max()),\n",
    "            'autocorr': float(output[col].autocorr(1)),\n",
    "            'nan_pct': float(output[col].isna().mean() * 100)\n",
    "        }\n",
    "        for col in output.columns\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Saved training_result.json')\n",
    "\n",
    "print(f'\\nFinished: {datetime.now().isoformat()}')\n",
    "print('Training complete!')\n",
    "print(f'Gate 1: {\"PASS\" if gate1_pass else \"FAIL\"}')\n",
    "print(f'Gate 2: {\"PASS\" if gate2_pass else \"FAIL\"} (MI +{mi_increase_pct:.1f}%, VIF {max_vif:.2f})')\n",
    "print(f'Gate 3: {\"PASS\" if gate3_pass else \"FAIL\"} (DA {da_delta:+.4f}, Sharpe {sharpe_delta:+.4f}, MAE {mae_delta:+.4f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
