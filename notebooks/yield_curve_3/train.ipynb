{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gold SubModel: Yield Curve - Attempt 3\n",
    "**Approach**: Mixed z-score features - velocity + level/regime signals\n",
    "**Changes from Attempt 2**:\n",
    "- Kept: yc_curvature_z, yc_10y3m_velocity_z (ranked 6th in feature importance)\n",
    "- Removed: yc_spread_velocity_z, yc_dgs3mo_velocity_z (low contribution)\n",
    "- Added: yc_spread_level_z (10Y-2Y spread LEVEL z-score - curve regime)\n",
    "- Added: yc_vol_regime_z (rolling 10Y rate vol z-score - stress indicator)\n",
    "- New HPO params: level_zscore_window (126-504), vol_window (10-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pandas-datareader', '-q'])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "import yfinance as yf\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f'=== Gold SubModel Training: yield_curve attempt 3 ===')\n",
    "print(f'Approach: Mixed z-score features - velocity + level/regime signals')\n",
    "print(f'Started: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No FRED API key needed - using pandas_datareader for direct FRED public access\n",
    "# pandas_datareader fetches FRED data over public HTTP (no authentication required)\n",
    "print('Using pandas_datareader for FRED data (no API key required)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data():\n",
    "    \"\"\"\n",
    "    Fetch yield curve data from FRED via pandas_datareader (no API key needed)\n",
    "    Series: DGS10, DGS2, DGS5, DGS3MO\n",
    "    \"\"\"\n",
    "    start = '2014-01-01'\n",
    "    print('Fetching yield curve data from FRED via pandas_datareader...')\n",
    "\n",
    "    # pandas_datareader fetches FRED data publicly (no API key required)\n",
    "    dgs10  = pdr.DataReader('DGS10',  'fred', start=start)['DGS10']  / 100\n",
    "    dgs2   = pdr.DataReader('DGS2',   'fred', start=start)['DGS2']   / 100\n",
    "    dgs5   = pdr.DataReader('DGS5',   'fred', start=start)['DGS5']   / 100\n",
    "    dgs3mo = pdr.DataReader('DGS3MO', 'fred', start=start)['DGS3MO'] / 100\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'dgs10':  dgs10,\n",
    "        'dgs2':   dgs2,\n",
    "        'dgs5':   dgs5,\n",
    "        'dgs3mo': dgs3mo,\n",
    "    })\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.index.name = 'date'\n",
    "\n",
    "    # Forward-fill weekends and holidays (max 5 business days)\n",
    "    df = df.ffill(limit=5)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Derived series\n",
    "    df['spread_10y2y'] = df['dgs10'] - df['dgs2']\n",
    "    df['spread_10y3m'] = df['dgs10'] - df['dgs3mo']\n",
    "    df['curvature']    = df['dgs5'] - 0.5 * (df['dgs2'] + df['dgs10'])\n",
    "\n",
    "    print(f'Yield data: {len(df)} rows from {df.index.min().date()} to {df.index.max().date()}')\n",
    "\n",
    "    # Gold target from Yahoo Finance\n",
    "    print('Fetching GLD for gold target...')\n",
    "    gld = yf.download('GLD', start='2015-01-01', progress=False)\n",
    "    if gld.empty:\n",
    "        raise ValueError('GLD download returned empty DataFrame')\n",
    "    if len(gld) < 100:\n",
    "        raise ValueError(f'GLD download too short: {len(gld)} rows')\n",
    "\n",
    "    if isinstance(gld.columns, pd.MultiIndex):\n",
    "        gold_close = gld['Close'].iloc[:, 0]\n",
    "    else:\n",
    "        gold_close = gld['Close']\n",
    "\n",
    "    target = pd.DataFrame({'gold_close': gold_close.values}, index=gld.index)\n",
    "    target.index = pd.to_datetime(target.index)\n",
    "    target.index.name = 'date'\n",
    "    target['gold_return_next'] = target['gold_close'].pct_change().shift(-1) * 100\n",
    "    target = target.dropna(subset=['gold_return_next'])\n",
    "\n",
    "    # Align on common dates, restrict to 2015-01-01 onwards\n",
    "    common_idx = df.index.intersection(target.index)\n",
    "    common_idx = common_idx[common_idx >= pd.Timestamp('2015-01-01')]\n",
    "    df = df.loc[common_idx]\n",
    "    target = target.loc[common_idx]\n",
    "\n",
    "    print(f'Aligned: {len(df)} rows from {df.index.min().date()} to {df.index.max().date()}')\n",
    "    return df, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, change_window_short, velocity_zscore_window, level_zscore_window, vol_window):\n",
    "    \"\"\"\n",
    "    Generate 4 yield curve features - 2 kept from attempt 2 + 2 new level/regime signals.\n",
    "\n",
    "    Kept from attempt 2 (top-performing):\n",
    "      yc_curvature_z       : z-score of daily curvature change\n",
    "      yc_10y3m_velocity_z  : z-score of short-term 10Y-3M spread change\n",
    "\n",
    "    New level/regime signals:\n",
    "      yc_spread_level_z    : z-score of 10Y-2Y spread LEVEL (curve regime context)\n",
    "                             Uses level_zscore_window (6mo-2yr) for normalization\n",
    "                             Captures inversion / steep / flat regime\n",
    "      yc_vol_regime_z      : z-score of rolling vol of 10Y rate changes (stress indicator)\n",
    "                             High vol = stress environment beneficial to gold\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    def zscore_of_change(series, n_change, window):\n",
    "        \"\"\"Z-score of n-day change, normalized over rolling window.\"\"\"\n",
    "        chg = series.diff(n_change)\n",
    "        mu = chg.rolling(window, min_periods=window // 2).mean()\n",
    "        sigma = chg.rolling(window, min_periods=window // 2).std()\n",
    "        z = (chg - mu) / sigma.clip(lower=1e-8)\n",
    "        return z.clip(-4, 4).ffill()\n",
    "\n",
    "    def zscore_of_level(series, window):\n",
    "        \"\"\"Z-score of level, normalized over rolling window.\"\"\"\n",
    "        mu = series.rolling(window, min_periods=window // 2).mean()\n",
    "        sigma = series.rolling(window, min_periods=window // 2).std()\n",
    "        z = (series - mu) / sigma.clip(lower=1e-8)\n",
    "        return z.clip(-4, 4).ffill()\n",
    "\n",
    "    # 1. Curvature z-score (kept from attempt 2 - 1-day change, curvature_zscore_window)\n",
    "    #    Use velocity_zscore_window for curvature normalization (same role as attempt 2)\n",
    "    result['yc_curvature_z'] = zscore_of_change(df['curvature'], 1, velocity_zscore_window)\n",
    "\n",
    "    # 2. 10Y-3M spread velocity z-score (kept from attempt 2 - ranked 6th)\n",
    "    result['yc_10y3m_velocity_z'] = zscore_of_change(df['spread_10y3m'], change_window_short, velocity_zscore_window)\n",
    "\n",
    "    # 3. NEW: 10Y-2Y spread LEVEL z-score (regime signal)\n",
    "    #    Long window (6mo-2yr) to capture whether curve is currently inverted/steep vs history\n",
    "    #    Positive = steeper than recent average; negative = flatter/inverted\n",
    "    result['yc_spread_level_z'] = zscore_of_level(df['spread_10y2y'], level_zscore_window)\n",
    "\n",
    "    # 4. NEW: Rate volatility regime z-score\n",
    "    #    Rolling std of 10Y daily changes; z-scored over velocity_zscore_window\n",
    "    #    High values = elevated volatility regime (stress environment)\n",
    "    dgs10_chg = df['dgs10'].diff(1)\n",
    "    rate_vol = dgs10_chg.abs().rolling(vol_window, min_periods=vol_window // 2).mean()\n",
    "    result['yc_vol_regime_z'] = zscore_of_level(rate_vol, velocity_zscore_window)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(df, target_df, train_end, val_end):\n",
    "    val_y = target_df['gold_return_next'].iloc[train_end:val_end].values\n",
    "\n",
    "    def discretize(x, bins=20):\n",
    "        valid = ~np.isnan(x)\n",
    "        if valid.sum() < bins:\n",
    "            return None\n",
    "        x_clean = x.copy()\n",
    "        x_clean[~valid] = np.nanmedian(x)\n",
    "        try:\n",
    "            return pd.qcut(x_clean, bins, labels=False, duplicates='drop')\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def objective(trial):\n",
    "        # change_window_short: for velocity features (short-term dynamics)\n",
    "        change_window_short = trial.suggest_int('change_window_short', 1, 5)\n",
    "        # velocity_zscore_window: normalization window for velocity and vol features\n",
    "        velocity_zscore_window = trial.suggest_int('velocity_zscore_window', 20, 60)\n",
    "        # level_zscore_window: normalization window for spread level (6mo to 2yr)\n",
    "        level_zscore_window = trial.suggest_int('level_zscore_window', 126, 504)\n",
    "        # vol_window: window for rolling rate volatility calculation\n",
    "        vol_window = trial.suggest_int('vol_window', 10, 30)\n",
    "\n",
    "        try:\n",
    "            features = generate_features(\n",
    "                df,\n",
    "                change_window_short=change_window_short,\n",
    "                velocity_zscore_window=velocity_zscore_window,\n",
    "                level_zscore_window=level_zscore_window,\n",
    "                vol_window=vol_window\n",
    "            )\n",
    "            val_X = features.iloc[train_end:val_end]\n",
    "\n",
    "            mi_sum = 0.0\n",
    "            for col in features.columns:\n",
    "                feat_val = val_X[col].values\n",
    "                mask = ~np.isnan(feat_val) & ~np.isnan(val_y)\n",
    "                if mask.sum() < 50:\n",
    "                    continue\n",
    "                feat_disc = discretize(feat_val[mask])\n",
    "                tgt_disc = discretize(val_y[mask])\n",
    "                if feat_disc is not None and tgt_disc is not None:\n",
    "                    mi_sum += mutual_info_score(feat_disc, tgt_disc)\n",
    "            return mi_sum\n",
    "        except Exception as e:\n",
    "            print(f'Trial failed: {e}')\n",
    "            return 0.0\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fetch Data ===\n",
    "data_df, target_df = fetch_data()\n",
    "\n",
    "n = len(data_df)\n",
    "train_end = int(n * 0.70)\n",
    "val_end   = int(n * 0.85)\n",
    "\n",
    "print(f'\\nData split:')\n",
    "print(f'  Train: {train_end} rows ({data_df.index[0].date()} - {data_df.index[train_end-1].date()})')\n",
    "print(f'  Val:   {val_end - train_end} rows ({data_df.index[train_end].date()} - {data_df.index[val_end-1].date()})')\n",
    "print(f'  Test:  {n - val_end} rows ({data_df.index[val_end].date()} - {data_df.index[-1].date()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optuna HPO: 50 trials ===\n",
    "print('Running Optuna HPO (50 trials)...')\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    create_objective(data_df, target_df, train_end, val_end),\n",
    "    n_trials=50,\n",
    "    timeout=600\n",
    ")\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'\\nOptuna complete:')\n",
    "print(f'  Best MI sum: {study.best_value:.4f}')\n",
    "print(f'  Best params: {best_params}')\n",
    "print(f'  Completed trials: {len(study.trials)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Final Output with Best Params ===\n",
    "print('\\nGenerating final submodel output...')\n",
    "output = generate_features(data_df, **best_params)\n",
    "\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Output columns: {list(output.columns)}')\n",
    "print(f'Date range: {output.index.min().date()} to {output.index.max().date()}')\n",
    "print('\\nOutput summary:')\n",
    "print(output.describe())\n",
    "\n",
    "# Gate 1 checks: std and autocorrelation\n",
    "print('\\nGate 1 checks (training set):')\n",
    "train_output = output.iloc[:train_end]\n",
    "autocorr_results = {}\n",
    "for col in output.columns:\n",
    "    vals = train_output[col].dropna().values\n",
    "    std_val = np.std(vals)\n",
    "    if len(vals) > 1:\n",
    "        ac = np.corrcoef(vals[:-1], vals[1:])[0, 1]\n",
    "    else:\n",
    "        ac = 0.0\n",
    "    autocorr_results[col] = float(ac)\n",
    "    status = 'FAIL' if std_val < 1e-6 else ('WARN' if abs(ac) > 0.95 else 'OK')\n",
    "    print(f'  {col}: std={std_val:.4f}, autocorr={ac:.4f} [{status}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Gate 2 prep: MI on validation set ===\n",
    "def discretize_final(x, bins=20):\n",
    "    valid = ~np.isnan(x)\n",
    "    if valid.sum() < bins:\n",
    "        return None\n",
    "    x_clean = x.copy()\n",
    "    x_clean[~valid] = np.nanmedian(x)\n",
    "    try:\n",
    "        return pd.qcut(x_clean, bins, labels=False, duplicates='drop')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "val_X = output.iloc[train_end:val_end]\n",
    "val_y = target_df['gold_return_next'].iloc[train_end:val_end]\n",
    "\n",
    "mi_results = {}\n",
    "for col in output.columns:\n",
    "    feat_val = val_X[col].values\n",
    "    mask = ~np.isnan(feat_val) & ~np.isnan(val_y.values)\n",
    "    feat_disc = discretize_final(feat_val[mask])\n",
    "    tgt_disc  = discretize_final(val_y.values[mask])\n",
    "    if feat_disc is not None and tgt_disc is not None:\n",
    "        mi_results[col] = float(mutual_info_score(feat_disc, tgt_disc))\n",
    "    else:\n",
    "        mi_results[col] = 0.0\n",
    "\n",
    "mi_sum = sum(mi_results.values())\n",
    "print(f'MI results (validation set):')\n",
    "for col, mi in mi_results.items():\n",
    "    print(f'  {col}: {mi:.4f}')\n",
    "print(f'  MI Sum: {mi_sum:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Results ===\n",
    "print('\\nSaving results...')\n",
    "\n",
    "output_with_date = output.reset_index()\n",
    "output_with_date.columns = ['Date'] + list(output.columns)\n",
    "output_with_date.to_csv('submodel_output.csv', index=False)\n",
    "\n",
    "result = {\n",
    "    'feature': 'yield_curve',\n",
    "    'attempt': 3,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_params': best_params,\n",
    "    'metrics': {\n",
    "        'mi_individual': mi_results,\n",
    "        'mi_sum': mi_sum,\n",
    "        'autocorr': autocorr_results,\n",
    "        'optuna_best_value': float(study.best_value),\n",
    "        'optuna_trials_completed': len(study.trials)\n",
    "    },\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': list(output.columns),\n",
    "    'data_info': {\n",
    "        'total_samples': len(data_df),\n",
    "        'train_samples': train_end,\n",
    "        'val_samples': val_end - train_end,\n",
    "        'test_samples': n - val_end,\n",
    "        'date_range_start': str(data_df.index.min().date()),\n",
    "        'date_range_end': str(data_df.index.max().date())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "\n",
    "print(f'=== Training complete! ===')\n",
    "print(f'Finished: {datetime.now().isoformat()}')\n",
    "print(f'Files: submodel_output.csv, training_result.json')\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Columns: {list(output.columns)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
