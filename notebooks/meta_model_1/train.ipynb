{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Meta-Model Training - Attempt 1\n",
    "\n",
    "**Self-contained training notebook**\n",
    "\n",
    "This notebook:\n",
    "1. Fetches base features from yfinance and FRED\n",
    "2. Generates submodel outputs using saved HMM parameters\n",
    "3. Merges all 39 features\n",
    "4. Trains XGBoost meta-model with custom directional-weighted MAE objective\n",
    "5. Optimizes hyperparameters via Optuna (50 trials)\n",
    "6. Evaluates on test set against all 4 targets\n",
    "\n",
    "**Architecture**: XGBoost with directional-weighted MAE loss\n",
    "\n",
    "**Features**: 19 base + 20 submodel outputs = 39 total\n",
    "\n",
    "**Targets**: DA > 56%, HC-DA > 60%, MAE < 0.75%, Sharpe > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install packages if needed\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"xgboost\"], check=True)\n",
    "    import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"optuna\"], check=True)\n",
    "    import optuna\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"yfinance\"], check=True)\n",
    "    import yfinance as yf\n",
    "\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"fredapi\"], check=True)\n",
    "    from fredapi import Fred\n",
    "\n",
    "try:\n",
    "    from hmmlearn import hmm as hmmlearn_hmm\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"hmmlearn\"], check=True)\n",
    "    from hmmlearn import hmm as hmmlearn_hmm\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "\n",
    "# Random seeds\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get FRED API key from Kaggle Secrets\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    FRED_API_KEY = secrets.get_secret(\"FRED_API_KEY\")\n",
    "    print(\"FRED API key loaded from Kaggle Secrets\")\n",
    "except:\n",
    "    # Fallback for local testing (will fail in Kaggle if secret not set)\n",
    "    FRED_API_KEY = os.environ.get('FRED_API_KEY')\n",
    "    if FRED_API_KEY:\n",
    "        print(\"FRED API key loaded from environment\")\n",
    "    else:\n",
    "        raise KeyError(\"FRED_API_KEY not found in Kaggle Secrets or environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching and Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_base_features():\n",
    "    \"\"\"\n",
    "    Fetch base features from yfinance and FRED.\n",
    "    Returns DataFrame with 19 base features + gold_return_next target.\n",
    "    \"\"\"\n",
    "    print(\"Fetching base features...\")\n",
    "    \n",
    "    fred = Fred(api_key=FRED_API_KEY)\n",
    "    \n",
    "    # Fetch FRED data\n",
    "    start_date = '2014-06-01'\n",
    "    \n",
    "    dfii10 = fred.get_series('DFII10', observation_start=start_date)  # Real rate\n",
    "    dgs10 = fred.get_series('DGS10', observation_start=start_date)    # 10Y yield\n",
    "    dgs2 = fred.get_series('DGS2', observation_start=start_date)      # 2Y yield\n",
    "    t10yie = fred.get_series('T10YIE', observation_start=start_date)  # Inflation expectation\n",
    "    vix = fred.get_series('VIXCLS', observation_start=start_date)     # VIX\n",
    "    \n",
    "    # Create base DataFrame from FRED data\n",
    "    df = pd.DataFrame({\n",
    "        'real_rate_real_rate': dfii10,\n",
    "        'yield_curve_dgs10': dgs10,\n",
    "        'yield_curve_dgs2': dgs2,\n",
    "        'inflation_expectation_inflation_expectation': t10yie,\n",
    "        'vix_vix': vix\n",
    "    })\n",
    "    \n",
    "    # Fetch Yahoo Finance data\n",
    "    print(\"  Fetching Yahoo Finance data...\")\n",
    "    \n",
    "    tickers = {\n",
    "        'DX-Y.NYB': 'dxy',\n",
    "        'GLD': 'technical_gld',\n",
    "        'GC=F': 'gc',\n",
    "        'SI=F': 'cross_asset_silver',\n",
    "        'HG=F': 'cross_asset_copper',\n",
    "        '^GSPC': 'cross_asset_sp500',\n",
    "        'CNY=X': 'cny_demand_cny'\n",
    "    }\n",
    "    \n",
    "    yf_data = {}\n",
    "    for ticker, name in tickers.items():\n",
    "        data = yf.download(ticker, start=start_date, progress=False)\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            data.columns = data.columns.get_level_values(0)\n",
    "        yf_data[name] = data\n",
    "    \n",
    "    # Add DXY\n",
    "    df['dxy_dxy'] = yf_data['dxy']['Close']\n",
    "    \n",
    "    # Add GLD OHLCV\n",
    "    df['technical_gld_open'] = yf_data['technical_gld']['Open']\n",
    "    df['technical_gld_high'] = yf_data['technical_gld']['High']\n",
    "    df['technical_gld_low'] = yf_data['technical_gld']['Low']\n",
    "    df['technical_gld_close'] = yf_data['technical_gld']['Close']\n",
    "    df['technical_gld_volume'] = yf_data['technical_gld']['Volume']\n",
    "    \n",
    "    # Add cross-asset\n",
    "    df['cross_asset_silver_close'] = yf_data['cross_asset_silver']['Close']\n",
    "    df['cross_asset_copper_close'] = yf_data['cross_asset_copper']['Close']\n",
    "    df['cross_asset_sp500_close'] = yf_data['cross_asset_sp500']['Close']\n",
    "    \n",
    "    # Add CNY\n",
    "    df['cny_demand_cny_usd'] = yf_data['cny_demand_cny']['Close']\n",
    "    \n",
    "    # Add ETF flow features (using GLD)\n",
    "    df['etf_flow_gld_volume'] = yf_data['technical_gld']['Volume']\n",
    "    df['etf_flow_gld_close'] = yf_data['technical_gld']['Close']\n",
    "    df['etf_flow_volume_ma20'] = df['etf_flow_gld_volume'].rolling(20).mean()\n",
    "    \n",
    "    # Add yield spread\n",
    "    df['yield_curve_yield_spread'] = df['yield_curve_dgs10'] - df['yield_curve_dgs2']\n",
    "    \n",
    "    # Add target: next-day gold return\n",
    "    gc_close = yf_data['gc']['Close']\n",
    "    gold_return = gc_close.pct_change() * 100  # Percentage\n",
    "    df['gold_return_next'] = gold_return.shift(-1)  # Next-day return\n",
    "    \n",
    "    # Forward-fill missing values (up to 3 days)\n",
    "    df = df.ffill(limit=3)\n",
    "    \n",
    "    # Drop NaN\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"  Base features loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\"  Date range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "base_df = fetch_base_features()\n",
    "print(f\"\\nBase features shape: {base_df.shape}\")\n",
    "print(f\"Columns: {list(base_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submodel Output Generation\n",
    "\n",
    "Generate outputs from 7 HMM-based submodels using parameters from Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VIX Submodel ===\n",
    "def generate_vix_features(base_df):\n",
    "    \"\"\"\n",
    "    Generate VIX submodel features.\n",
    "    Based on vix_1 parameters (3-component HMM on [vix_change, vix_vol_5d]).\n",
    "    \"\"\"\n",
    "    print(\"Generating VIX submodel features...\")\n",
    "    \n",
    "    df = pd.DataFrame(index=base_df.index)\n",
    "    df['vix'] = base_df['vix_vix']\n",
    "    \n",
    "    # Compute derived features\n",
    "    df['vix_change'] = df['vix'].diff()\n",
    "    df['vix_vol_5d'] = df['vix_change'].rolling(5).std()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train 3-component HMM\n",
    "    X = df[['vix_change', 'vix_vol_5d']].values\n",
    "    valid_mask = np.isfinite(X).all(axis=1)\n",
    "    X_clean = X[valid_mask]\n",
    "    \n",
    "    model = hmmlearn_hmm.GaussianHMM(n_components=3, covariance_type='full', n_iter=100, random_state=42)\n",
    "    model.fit(X_clean)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = np.full((len(df), 3), np.nan)\n",
    "    probs[valid_mask] = model.predict_proba(X[valid_mask])\n",
    "    probs_df = pd.DataFrame(probs, index=df.index).fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Identify high-variance state\n",
    "    state_vars = [model.covars_[i][0, 0] for i in range(3)]\n",
    "    high_var_state = np.argmax(state_vars)\n",
    "    \n",
    "    # Compute features\n",
    "    output = pd.DataFrame(index=base_df.index)\n",
    "    output['vix_regime_probability'] = probs_df.iloc[:, high_var_state].reindex(base_df.index).fillna(0)\n",
    "    \n",
    "    # Mean reversion z-score\n",
    "    vix_reindexed = df['vix'].reindex(base_df.index).fillna(method='ffill')\n",
    "    vix_ma = vix_reindexed.rolling(20).mean()\n",
    "    vix_std = vix_reindexed.rolling(60).std()\n",
    "    output['vix_mean_reversion_z'] = ((vix_reindexed - vix_ma) / vix_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    # Persistence\n",
    "    vix_change_reindexed = df['vix_change'].reindex(base_df.index).fillna(0)\n",
    "    output['vix_persistence'] = vix_change_reindexed.rolling(5).apply(lambda x: (x > 0).sum() / 5).fillna(0)\n",
    "    \n",
    "    print(f\"  VIX features shape: {output.shape}\")\n",
    "    return output\n",
    "\n",
    "vix_features = generate_vix_features(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Technical Submodel ===\n",
    "def generate_technical_features(base_df):\n",
    "    \"\"\"\n",
    "    Generate technical submodel features.\n",
    "    Based on technical_1 parameters (2-component HMM on [log_return, atr]).\n",
    "    \"\"\"\n",
    "    print(\"Generating technical submodel features...\")\n",
    "    \n",
    "    df = pd.DataFrame(index=base_df.index)\n",
    "    df['close'] = base_df['technical_gld_close']\n",
    "    df['high'] = base_df['technical_gld_high']\n",
    "    df['low'] = base_df['technical_gld_low']\n",
    "    \n",
    "    # Compute features\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['tr'] = np.maximum(df['high'] - df['low'], \n",
    "                          np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                    abs(df['low'] - df['close'].shift(1))))\n",
    "    df['atr'] = df['tr'].rolling(14).mean()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train 2-component HMM\n",
    "    X = df[['log_return', 'atr']].values\n",
    "    valid_mask = np.isfinite(X).all(axis=1)\n",
    "    X_clean = X[valid_mask]\n",
    "    \n",
    "    model = hmmlearn_hmm.GaussianHMM(n_components=2, covariance_type='full', n_iter=100, random_state=42)\n",
    "    model.fit(X_clean)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = np.full((len(df), 2), np.nan)\n",
    "    probs[valid_mask] = model.predict_proba(X[valid_mask])\n",
    "    probs_df = pd.DataFrame(probs, index=df.index).fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Identify uptrend state (positive mean return)\n",
    "    state_means = [model.means_[i][0] for i in range(2)]\n",
    "    uptrend_state = np.argmax(state_means)\n",
    "    \n",
    "    output = pd.DataFrame(index=base_df.index)\n",
    "    output['tech_trend_regime_prob'] = probs_df.iloc[:, uptrend_state].reindex(base_df.index).fillna(0)\n",
    "    \n",
    "    # Mean reversion z-score (RSI-based)\n",
    "    close_reindexed = df['close'].reindex(base_df.index).fillna(method='ffill')\n",
    "    delta = close_reindexed.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    rsi_ma = rsi.rolling(20).mean()\n",
    "    rsi_std = rsi.rolling(60).std()\n",
    "    output['tech_mean_reversion_z'] = ((rsi - rsi_ma) / rsi_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    # Volatility regime (ATR percentile)\n",
    "    atr_reindexed = df['atr'].reindex(base_df.index).fillna(method='ffill')\n",
    "    atr_percentile = atr_reindexed.rolling(60).apply(lambda x: (x.iloc[-1] > x).sum() / len(x))\n",
    "    output['tech_volatility_regime'] = atr_percentile.fillna(0.5)\n",
    "    \n",
    "    print(f\"  Technical features shape: {output.shape}\")\n",
    "    return output\n",
    "\n",
    "technical_features = generate_technical_features(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cross-Asset Submodel ===\n",
    "def generate_cross_asset_features(base_df):\n",
    "    \"\"\"\n",
    "    Generate cross-asset submodel features.\n",
    "    Based on cross_asset_1 parameters.\n",
    "    \"\"\"\n",
    "    print(\"Generating cross-asset submodel features...\")\n",
    "    \n",
    "    df = pd.DataFrame(index=base_df.index)\n",
    "    df['gold'] = base_df['technical_gld_close']\n",
    "    df['silver'] = base_df['cross_asset_silver_close']\n",
    "    df['sp500'] = base_df['cross_asset_sp500_close']\n",
    "    \n",
    "    # Compute features\n",
    "    df['gs_ratio'] = df['gold'] / df['silver']\n",
    "    df['gs_ratio_change'] = df['gs_ratio'].pct_change()\n",
    "    df['sp500_return'] = df['sp500'].pct_change()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train 2-component HMM\n",
    "    X = df[['gs_ratio_change', 'sp500_return']].values\n",
    "    valid_mask = np.isfinite(X).all(axis=1)\n",
    "    X_clean = X[valid_mask]\n",
    "    \n",
    "    model = hmmlearn_hmm.GaussianHMM(n_components=2, covariance_type='diag', n_iter=100, random_state=42)\n",
    "    model.fit(X_clean)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = np.full((len(df), 2), np.nan)\n",
    "    probs[valid_mask] = model.predict_proba(X[valid_mask])\n",
    "    probs_df = pd.DataFrame(probs, index=df.index).fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Identify risk-off state (negative SP500 return mean)\n",
    "    state_means_sp500 = [model.means_[i][1] for i in range(2)]\n",
    "    risk_off_state = np.argmin(state_means_sp500)\n",
    "    \n",
    "    output = pd.DataFrame(index=base_df.index)\n",
    "    output['xasset_regime_prob'] = probs_df.iloc[:, risk_off_state].reindex(base_df.index).fillna(0)\n",
    "    \n",
    "    # Recession signal (SP500 below 200-day MA)\n",
    "    sp500_reindexed = df['sp500'].reindex(base_df.index).fillna(method='ffill')\n",
    "    sp500_ma200 = sp500_reindexed.rolling(200).mean()\n",
    "    output['xasset_recession_signal'] = (sp500_reindexed < sp500_ma200).astype(int).fillna(0)\n",
    "    \n",
    "    # Divergence (G/S ratio z-score)\n",
    "    gs_ratio_reindexed = df['gs_ratio'].reindex(base_df.index).fillna(method='ffill')\n",
    "    gs_ma = gs_ratio_reindexed.rolling(60).mean()\n",
    "    gs_std = gs_ratio_reindexed.rolling(60).std()\n",
    "    output['xasset_divergence'] = ((gs_ratio_reindexed - gs_ma) / gs_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    print(f\"  Cross-asset features shape: {output.shape}\")\n",
    "    return output\n",
    "\n",
    "cross_asset_features = generate_cross_asset_features(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Yield Curve Submodel ===\n",
    "def generate_yield_curve_features(base_df):\n",
    "    \"\"\"\n",
    "    Generate yield curve submodel features.\n",
    "    Note: yc_regime_prob is excluded (constant).\n",
    "    \"\"\"\n",
    "    print(\"Generating yield curve submodel features...\")\n",
    "    \n",
    "    df = pd.DataFrame(index=base_df.index)\n",
    "    df['spread'] = base_df['yield_curve_yield_spread']\n",
    "    \n",
    "    # Compute features\n",
    "    df['spread_velocity'] = df['spread'].diff()\n",
    "    df['spread_ma'] = df['spread'].rolling(20).mean()\n",
    "    df['spread_std'] = df['spread'].rolling(60).std()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    output = pd.DataFrame(index=base_df.index)\n",
    "    \n",
    "    # Spread velocity z-score\n",
    "    velocity_reindexed = df['spread_velocity'].reindex(base_df.index).fillna(0)\n",
    "    velocity_ma = velocity_reindexed.rolling(60).mean()\n",
    "    velocity_std = velocity_reindexed.rolling(60).std()\n",
    "    output['yc_spread_velocity_z'] = ((velocity_reindexed - velocity_ma) / velocity_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    # Curvature z-score (2nd derivative)\n",
    "    spread_reindexed = df['spread'].reindex(base_df.index).fillna(method='ffill')\n",
    "    curvature = spread_reindexed.diff().diff()\n",
    "    curvature_ma = curvature.rolling(60).mean()\n",
    "    curvature_std = curvature.rolling(60).std()\n",
    "    output['yc_curvature_z'] = ((curvature - curvature_ma) / curvature_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    print(f\"  Yield curve features shape: {output.shape}\")\n",
    "    return output\n",
    "\n",
    "yield_curve_features = generate_yield_curve_features(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETF Flow Submodel ===\n",
    "def generate_etf_flow_features(base_df):\n",
    "    \"\"\"\n",
    "    Generate ETF flow submodel features.\n",
    "    \"\"\"\n",
    "    print(\"Generating ETF flow submodel features...\")\n",
    "    \n",
    "    df = pd.DataFrame(index=base_df.index)\n",
    "    df['volume'] = base_df['etf_flow_gld_volume']\n",
    "    df['close'] = base_df['etf_flow_gld_close']\n",
    "    \n",
    "    # Compute features\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    df['price_return'] = df['close'].pct_change()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train 2-component HMM\n",
    "    X = df[['volume_change', 'price_return']].values\n",
    "    valid_mask = np.isfinite(X).all(axis=1)\n",
    "    X_clean = X[valid_mask]\n",
    "    \n",
    "    model = hmmlearn_hmm.GaussianHMM(n_components=2, covariance_type='diag', n_iter=100, random_state=42)\n",
    "    model.fit(X_clean)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = np.full((len(df), 2), np.nan)\n",
    "    probs[valid_mask] = model.predict_proba(X[valid_mask])\n",
    "    probs_df = pd.DataFrame(probs, index=df.index).fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Identify high-flow state\n",
    "    state_means_volume = [model.means_[i][0] for i in range(2)]\n",
    "    high_flow_state = np.argmax(state_means_volume)\n",
    "    \n",
    "    output = pd.DataFrame(index=base_df.index)\n",
    "    output['etf_regime_prob'] = probs_df.iloc[:, high_flow_state].reindex(base_df.index).fillna(0)\n",
    "    \n",
    "    # Capital intensity (volume * price)\n",
    "    volume_reindexed = df['volume'].reindex(base_df.index).fillna(method='ffill')\n",
    "    close_reindexed = df['close'].reindex(base_df.index).fillna(method='ffill')\n",
    "    capital = volume_reindexed * close_reindexed\n",
    "    capital_ma = capital.rolling(20).mean()\n",
    "    capital_std = capital.rolling(60).std()\n",
    "    output['etf_capital_intensity'] = ((capital - capital_ma) / capital_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    # Price-volume divergence\n",
    "    price_z = ((close_reindexed - close_reindexed.rolling(20).mean()) / close_reindexed.rolling(60).std()).fillna(0)\n",
    "    volume_z = ((volume_reindexed - volume_reindexed.rolling(20).mean()) / volume_reindexed.rolling(60).std()).fillna(0)\n",
    "    output['etf_pv_divergence'] = (price_z - volume_z).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    print(f\"  ETF flow features shape: {output.shape}\")\n",
    "    return output\n",
    "\n",
    "etf_flow_features = generate_etf_flow_features(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inflation Expectation Submodel ===\n",
    "def generate_inflation_expectation_features(base_df):\n",
    "    \"\"\"\n",
    "    Generate inflation expectation submodel features.\n",
    "    \"\"\"\n",
    "    print(\"Generating inflation expectation submodel features...\")\n",
    "    \n",
    "    df = pd.DataFrame(index=base_df.index)\n",
    "    df['ie'] = base_df['inflation_expectation_inflation_expectation']\n",
    "    \n",
    "    # Compute features\n",
    "    df['ie_change'] = df['ie'].diff()\n",
    "    df['ie_vol_5d'] = df['ie_change'].rolling(5).std()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train 2-component HMM\n",
    "    X = df[['ie_change', 'ie_vol_5d']].values\n",
    "    valid_mask = np.isfinite(X).all(axis=1)\n",
    "    X_clean = X[valid_mask]\n",
    "    \n",
    "    model = hmmlearn_hmm.GaussianHMM(n_components=2, covariance_type='full', n_iter=100, random_state=42)\n",
    "    model.fit(X_clean)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = np.full((len(df), 2), np.nan)\n",
    "    probs[valid_mask] = model.predict_proba(X[valid_mask])\n",
    "    probs_df = pd.DataFrame(probs, index=df.index).fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Identify high-variance state\n",
    "    state_vars = [model.covars_[i][0, 0] for i in range(2)]\n",
    "    high_var_state = np.argmax(state_vars)\n",
    "    \n",
    "    output = pd.DataFrame(index=base_df.index)\n",
    "    output['ie_regime_prob'] = probs_df.iloc[:, high_var_state].reindex(base_df.index).fillna(0)\n",
    "    \n",
    "    # Anchoring z-score\n",
    "    ie_vol_reindexed = df['ie_vol_5d'].reindex(base_df.index).fillna(method='ffill')\n",
    "    ie_vol_ma = ie_vol_reindexed.rolling(60).mean()\n",
    "    ie_vol_std = ie_vol_reindexed.rolling(60).std()\n",
    "    output['ie_anchoring_z'] = ((ie_vol_reindexed - ie_vol_ma) / ie_vol_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    # IE-gold sensitivity (correlation)\n",
    "    ie_change_reindexed = df['ie_change'].reindex(base_df.index).fillna(0)\n",
    "    gold_return = base_df['technical_gld_close'].pct_change()\n",
    "    rolling_corr = ie_change_reindexed.rolling(5).corr(gold_return)\n",
    "    corr_ma = rolling_corr.rolling(60).mean()\n",
    "    corr_std = rolling_corr.rolling(60).std()\n",
    "    output['ie_gold_sensitivity_z'] = ((rolling_corr - corr_ma) / corr_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    print(f\"  Inflation expectation features shape: {output.shape}\")\n",
    "    return output\n",
    "\n",
    "inflation_expectation_features = generate_inflation_expectation_features(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNY Demand Submodel ===\n",
    "def generate_cny_demand_features(base_df):\n",
    "    \"\"\"\n",
    "    Generate CNY demand submodel features.\n",
    "    \"\"\"\n",
    "    print(\"Generating CNY demand submodel features...\")\n",
    "    \n",
    "    df = pd.DataFrame(index=base_df.index)\n",
    "    df['cny_usd'] = base_df['cny_demand_cny_usd']\n",
    "    \n",
    "    # Compute features\n",
    "    df['cny_change'] = df['cny_usd'].pct_change()\n",
    "    df['cny_vol'] = df['cny_change'].rolling(10).std()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train 2-component HMM\n",
    "    X = df[['cny_change', 'cny_vol']].values\n",
    "    valid_mask = np.isfinite(X).all(axis=1)\n",
    "    X_clean = X[valid_mask]\n",
    "    \n",
    "    model = hmmlearn_hmm.GaussianHMM(n_components=2, covariance_type='diag', n_iter=100, random_state=42)\n",
    "    model.fit(X_clean)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    probs = np.full((len(df), 2), np.nan)\n",
    "    probs[valid_mask] = model.predict_proba(X[valid_mask])\n",
    "    probs_df = pd.DataFrame(probs, index=df.index).fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Identify high-volatility state\n",
    "    state_vars = [model.covars_[i][1] for i in range(2)]\n",
    "    high_vol_state = np.argmax(state_vars)\n",
    "    \n",
    "    output = pd.DataFrame(index=base_df.index)\n",
    "    output['cny_regime_prob'] = probs_df.iloc[:, high_vol_state].reindex(base_df.index).fillna(0)\n",
    "    \n",
    "    # Momentum z-score\n",
    "    cny_change_reindexed = df['cny_change'].reindex(base_df.index).fillna(0)\n",
    "    momentum = cny_change_reindexed.rolling(20).mean()\n",
    "    momentum_ma = momentum.rolling(60).mean()\n",
    "    momentum_std = momentum.rolling(60).std()\n",
    "    output['cny_momentum_z'] = ((momentum - momentum_ma) / momentum_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    # Volatility regime z-score\n",
    "    cny_vol_reindexed = df['cny_vol'].reindex(base_df.index).fillna(method='ffill')\n",
    "    vol_ma = cny_vol_reindexed.rolling(60).mean()\n",
    "    vol_std = cny_vol_reindexed.rolling(60).std()\n",
    "    output['cny_vol_regime_z'] = ((cny_vol_reindexed - vol_ma) / vol_std).clip(-4, 4).fillna(0)\n",
    "    \n",
    "    print(f\"  CNY demand features shape: {output.shape}\")\n",
    "    return output\n",
    "\n",
    "cny_demand_features = generate_cny_demand_features(base_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from base features\n",
    "target = base_df['gold_return_next'].copy()\n",
    "base_features = base_df.drop(columns=['gold_return_next'])\n",
    "\n",
    "# Merge all features\n",
    "all_features = base_features.copy()\n",
    "all_features = all_features.join(vix_features, how='inner')\n",
    "all_features = all_features.join(technical_features, how='inner')\n",
    "all_features = all_features.join(cross_asset_features, how='inner')\n",
    "all_features = all_features.join(yield_curve_features, how='inner')\n",
    "all_features = all_features.join(etf_flow_features, how='inner')\n",
    "all_features = all_features.join(inflation_expectation_features, how='inner')\n",
    "all_features = all_features.join(cny_demand_features, how='inner')\n",
    "\n",
    "# Align target\n",
    "target = target.reindex(all_features.index)\n",
    "\n",
    "# Drop NaN rows\n",
    "valid_mask = ~(all_features.isna().any(axis=1) | target.isna())\n",
    "all_features = all_features[valid_mask]\n",
    "target = target[valid_mask]\n",
    "\n",
    "print(f\"\\nMerged data:\")\n",
    "print(f\"  Features shape: {all_features.shape}\")\n",
    "print(f\"  Target shape: {target.shape}\")\n",
    "print(f\"  Date range: {all_features.index.min().date()} to {all_features.index.max().date()}\")\n",
    "print(f\"\\nFeature columns ({len(all_features.columns)}):\")\n",
    "for i, col in enumerate(all_features.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 70/15/15 (time-series order)\n",
    "n = len(all_features)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "X_train = all_features.iloc[:train_end].values\n",
    "y_train = target.iloc[:train_end].values\n",
    "dates_train = all_features.iloc[:train_end].index\n",
    "\n",
    "X_val = all_features.iloc[train_end:val_end].values\n",
    "y_val = target.iloc[train_end:val_end].values\n",
    "dates_val = all_features.iloc[train_end:val_end].index\n",
    "\n",
    "X_test = all_features.iloc[val_end:].values\n",
    "y_test = target.iloc[val_end:].values\n",
    "dates_test = all_features.iloc[val_end:].index\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Train: {len(X_train)} samples ({len(X_train)/n*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(X_val)} samples ({len(X_val)/n*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(X_test)} samples ({len(X_test)/n*100:.1f}%)\")\n",
    "\n",
    "# Compute test set up/down ratio\n",
    "test_up = (y_test > 0).sum()\n",
    "test_down = (y_test < 0).sum()\n",
    "test_zero = (y_test == 0).sum()\n",
    "print(f\"\\nTest set composition:\")\n",
    "print(f\"  Up days: {test_up} ({test_up/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Down days: {test_down} ({test_down/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Zero days: {test_zero}\")\n",
    "print(f\"  Naive always-up DA: {test_up / (test_up + test_down) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred, y_true, conf_threshold=0.005):\n",
    "    \"\"\"\n",
    "    Compute all evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predicted returns (%)\n",
    "        y_true: Actual returns (%)\n",
    "        conf_threshold: Confidence threshold for high-confidence DA\n",
    "    \n",
    "    Returns:\n",
    "        dict of metrics\n",
    "    \"\"\"\n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(y_pred - y_true))\n",
    "    \n",
    "    # Direction Accuracy (exclude zeros)\n",
    "    nonzero_mask = (y_true != 0) & (y_pred != 0)\n",
    "    if nonzero_mask.sum() > 0:\n",
    "        da = np.mean(np.sign(y_pred[nonzero_mask]) == np.sign(y_true[nonzero_mask]))\n",
    "    else:\n",
    "        da = 0.5\n",
    "    \n",
    "    # High-Confidence DA\n",
    "    hc_mask = (np.abs(y_pred) > conf_threshold) & nonzero_mask\n",
    "    if hc_mask.sum() >= 0.2 * len(y_pred):\n",
    "        hc_da = np.mean(np.sign(y_pred[hc_mask]) == np.sign(y_true[hc_mask]))\n",
    "        hc_coverage = hc_mask.sum() / len(y_pred)\n",
    "    else:\n",
    "        hc_da = 0.0\n",
    "        hc_coverage = hc_mask.sum() / len(y_pred)\n",
    "    \n",
    "    # Sharpe Ratio (with 5bps transaction cost per day)\n",
    "    cost_pct = 5.0 / 100.0  # 5 bps = 0.05%\n",
    "    strategy_returns = np.sign(y_pred) * y_true\n",
    "    net_returns = strategy_returns - cost_pct\n",
    "    \n",
    "    if len(net_returns) > 1 and np.std(net_returns) > 0:\n",
    "        sharpe = (np.mean(net_returns) / np.std(net_returns)) * np.sqrt(252)\n",
    "    else:\n",
    "        sharpe = 0.0\n",
    "    \n",
    "    return {\n",
    "        'mae': float(mae),\n",
    "        'direction_accuracy': float(da),\n",
    "        'high_confidence_da': float(hc_da),\n",
    "        'sharpe_ratio': float(sharpe),\n",
    "        'hc_coverage': float(hc_coverage),\n",
    "        'n_samples': len(y_pred)\n",
    "    }\n",
    "\n",
    "def composite_eval(y_pred, dtrain):\n",
    "    \"\"\"\n",
    "    Custom evaluation metric for XGBoost early stopping.\n",
    "    Lower is better.\n",
    "    \"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    \n",
    "    # MAE component\n",
    "    mae = np.mean(np.abs(y_pred - y_true))\n",
    "    \n",
    "    # DA component\n",
    "    nonzero = (y_true != 0) & (y_pred != 0)\n",
    "    if nonzero.sum() > 0:\n",
    "        da = np.mean(np.sign(y_pred[nonzero]) == np.sign(y_true[nonzero]))\n",
    "    else:\n",
    "        da = 0.5\n",
    "    \n",
    "    # Composite: lower is better\n",
    "    score = mae - 0.5 * da\n",
    "    \n",
    "    return 'composite', float(score)\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function.\n",
    "    Maximizes weighted composite of DA, HC-DA, MAE, and Sharpe.\n",
    "    \"\"\"\n",
    "    # Sample hyperparameters\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5.0, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 2.0),\n",
    "        'tree_method': 'hist',\n",
    "        'disable_default_eval_metric': 1,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42 + trial.number,\n",
    "    }\n",
    "    \n",
    "    penalty_factor = trial.suggest_float('directional_penalty', 1.5, 5.0)\n",
    "    conf_threshold = trial.suggest_float('confidence_threshold', 0.002, 0.015)\n",
    "    \n",
    "    # Custom objective with directional penalty\n",
    "    def obj_fn(y_pred, dtrain):\n",
    "        y_true = dtrain.get_label()\n",
    "        sign_agree = (y_pred * y_true) > 0\n",
    "        penalty = np.where(sign_agree, 1.0, penalty_factor)\n",
    "        residual = y_pred - y_true\n",
    "        grad = penalty * np.sign(residual)\n",
    "        hess = penalty * np.ones_like(y_pred)\n",
    "        return grad, hess\n",
    "    \n",
    "    # Train\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    try:\n",
    "        bst = xgb.train(\n",
    "            params, dtrain,\n",
    "            num_boost_round=1000,\n",
    "            obj=obj_fn,\n",
    "            evals=[(dval, 'val')],\n",
    "            custom_metric=composite_eval,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        return -np.inf\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = bst.predict(dval)\n",
    "    \n",
    "    # Check for degenerate predictions\n",
    "    if np.std(val_pred) < 0.001:\n",
    "        print(f\"Trial {trial.number}: degenerate predictions (std={np.std(val_pred):.6f})\")\n",
    "        return -np.inf\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(val_pred, y_val, conf_threshold)\n",
    "    \n",
    "    # Normalize metrics to [0, 1]\n",
    "    sharpe_norm = np.clip((metrics['sharpe_ratio'] + 3.0) / 6.0, 0.0, 1.0)\n",
    "    da_norm = np.clip((metrics['direction_accuracy'] - 0.3) / 0.4, 0.0, 1.0)\n",
    "    mae_norm = np.clip((1.0 - metrics['mae']) / 0.5, 0.0, 1.0)\n",
    "    hc_da_norm = np.clip((metrics['high_confidence_da'] - 0.3) / 0.4, 0.0, 1.0) if metrics['high_confidence_da'] > 0 else 0.0\n",
    "    \n",
    "    # Weighted composite (Sharpe 40%, DA 25%, HC-DA 20%, MAE 15%)\n",
    "    objective_value = (\n",
    "        0.40 * sharpe_norm +\n",
    "        0.25 * da_norm +\n",
    "        0.20 * hc_da_norm +\n",
    "        0.15 * mae_norm\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    trial.set_user_attr('val_mae', metrics['mae'])\n",
    "    trial.set_user_attr('val_da', metrics['direction_accuracy'])\n",
    "    trial.set_user_attr('val_hc_da', metrics['high_confidence_da'])\n",
    "    trial.set_user_attr('val_sharpe', metrics['sharpe_ratio'])\n",
    "    trial.set_user_attr('val_hc_coverage', metrics['hc_coverage'])\n",
    "    trial.set_user_attr('n_estimators', int(bst.best_iteration + 1))\n",
    "    \n",
    "    return objective_value\n",
    "\n",
    "print(\"\\n=== Running Optuna HPO (50 trials, 4-hour timeout) ===\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    optuna_objective,\n",
    "    n_trials=50,\n",
    "    timeout=14400,  # 4 hours\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOptuna optimization complete!\")\n",
    "print(f\"  Best objective value: {study.best_value:.6f}\")\n",
    "print(f\"  Best trial: #{study.best_trial.number}\")\n",
    "print(f\"  Trials completed: {len(study.trials)}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nBest trial metrics:\")\n",
    "for key, value in study.best_trial.user_attrs.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Training final model with best hyperparameters ===\")\n",
    "\n",
    "best_params = study.best_params.copy()\n",
    "penalty_factor = best_params.pop('directional_penalty')\n",
    "conf_threshold = best_params.pop('confidence_threshold')\n",
    "\n",
    "# Add fixed parameters\n",
    "best_params.update({\n",
    "    'tree_method': 'hist',\n",
    "    'disable_default_eval_metric': 1,\n",
    "    'verbosity': 0,\n",
    "    'seed': 42,\n",
    "})\n",
    "\n",
    "# Custom objective\n",
    "def final_obj_fn(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    sign_agree = (y_pred * y_true) > 0\n",
    "    penalty = np.where(sign_agree, 1.0, penalty_factor)\n",
    "    residual = y_pred - y_true\n",
    "    grad = penalty * np.sign(residual)\n",
    "    hess = penalty * np.ones_like(y_pred)\n",
    "    return grad, hess\n",
    "\n",
    "# Train\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "final_model = xgb.train(\n",
    "    best_params, dtrain,\n",
    "    num_boost_round=1000,\n",
    "    obj=final_obj_fn,\n",
    "    evals=[(dval, 'val')],\n",
    "    custom_metric=composite_eval,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=10,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal model trained!\")\n",
    "print(f\"  Best iteration: {final_model.best_iteration}\")\n",
    "print(f\"  Total boosting rounds: {final_model.best_iteration + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation on All Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Evaluating on all splits ===\")\n",
    "\n",
    "# Predict\n",
    "train_pred = final_model.predict(dtrain)\n",
    "val_pred = final_model.predict(dval)\n",
    "test_pred = final_model.predict(dtest)\n",
    "\n",
    "# Compute metrics\n",
    "train_metrics = compute_metrics(train_pred, y_train, conf_threshold)\n",
    "val_metrics = compute_metrics(val_pred, y_val, conf_threshold)\n",
    "test_metrics = compute_metrics(test_pred, y_test, conf_threshold)\n",
    "\n",
    "print(\"\\nTrain metrics:\")\n",
    "for key, value in train_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nValidation metrics:\")\n",
    "for key, value in val_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Overfit ratios\n",
    "mae_overfit_val = val_metrics['mae'] / (train_metrics['mae'] + 1e-10)\n",
    "mae_overfit_test = test_metrics['mae'] / (train_metrics['mae'] + 1e-10)\n",
    "da_gap_test = (train_metrics['direction_accuracy'] - test_metrics['direction_accuracy']) * 100\n",
    "\n",
    "print(\"\\nOverfit diagnostics:\")\n",
    "print(f\"  MAE val/train ratio: {mae_overfit_val:.3f}\")\n",
    "print(f\"  MAE test/train ratio: {mae_overfit_test:.3f}\")\n",
    "print(f\"  DA train-test gap: {da_gap_test:.2f}pp\")\n",
    "\n",
    "# Prediction distribution\n",
    "print(\"\\nTest prediction distribution:\")\n",
    "print(f\"  Mean: {np.mean(test_pred):.6f}\")\n",
    "print(f\"  Std: {np.std(test_pred):.6f}\")\n",
    "print(f\"  Min: {np.min(test_pred):.6f}\")\n",
    "print(f\"  Max: {np.max(test_pred):.6f}\")\n",
    "print(f\"  % positive predictions: {(test_pred > 0).sum() / len(test_pred) * 100:.1f}%\")\n",
    "\n",
    "# Naive always-up comparison\n",
    "naive_da = test_up / (test_up + test_down)\n",
    "print(f\"\\nNaive always-up DA: {naive_da:.4f}\")\n",
    "print(f\"Model DA improvement: {(test_metrics['direction_accuracy'] - naive_da) * 100:.2f}pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_dict = final_model.get_score(importance_type='gain')\n",
    "\n",
    "# Map feature indices to names\n",
    "feature_names = list(all_features.columns)\n",
    "importance_list = [(feature_names[int(k.replace('f', ''))], v) \n",
    "                   for k, v in importance_dict.items()]\n",
    "importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 20 feature importances:\")\n",
    "for i, (name, score) in enumerate(importance_list[:20], 1):\n",
    "    print(f\"  {i:2d}. {name:40s}: {score:.0f}\")\n",
    "\n",
    "# Check submodel feature usage\n",
    "submodel_prefixes = ['vix_', 'tech_', 'xasset_', 'yc_', 'etf_', 'ie_', 'cny_']\n",
    "submodel_importance = {prefix: 0 for prefix in submodel_prefixes}\n",
    "\n",
    "for name, score in importance_list:\n",
    "    for prefix in submodel_prefixes:\n",
    "        if name.startswith(prefix):\n",
    "            submodel_importance[prefix] += score\n",
    "            break\n",
    "\n",
    "print(\"\\nSubmodel feature importance (total gain):\")\n",
    "for prefix, score in sorted(submodel_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {prefix:10s}: {score:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Saving results ===\")\n",
    "\n",
    "# 1. training_result.json\n",
    "result = {\n",
    "    'feature': 'meta_model',\n",
    "    'attempt': 1,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_type': 'XGBoost',\n",
    "    'n_features': all_features.shape[1],\n",
    "    'best_params': {\n",
    "        **best_params,\n",
    "        'directional_penalty': penalty_factor,\n",
    "        'confidence_threshold': conf_threshold,\n",
    "        'n_estimators_used': int(final_model.best_iteration + 1)\n",
    "    },\n",
    "    'metrics': {\n",
    "        'train': train_metrics,\n",
    "        'val': val_metrics,\n",
    "        'test': test_metrics\n",
    "    },\n",
    "    'overfit_ratios': {\n",
    "        'mae_val_train': float(mae_overfit_val),\n",
    "        'mae_test_train': float(mae_overfit_test),\n",
    "        'da_train_test_gap_pp': float(da_gap_test)\n",
    "    },\n",
    "    'feature_importance_top20': dict(importance_list[:20]),\n",
    "    'submodel_importance': submodel_importance,\n",
    "    'naive_always_up_da_test': float(naive_da),\n",
    "    'optuna_summary': {\n",
    "        'n_trials': len(study.trials),\n",
    "        'best_trial': study.best_trial.number,\n",
    "        'best_value': float(study.best_value),\n",
    "        'trial_details': [\n",
    "            {\n",
    "                'trial': t.number,\n",
    "                'value': float(t.value) if t.value is not None else None,\n",
    "                'params': t.params,\n",
    "                'user_attrs': t.user_attrs\n",
    "            }\n",
    "            for t in study.trials\n",
    "        ]\n",
    "    },\n",
    "    'prediction_distribution': {\n",
    "        'test_pct_positive': float((test_pred > 0).sum() / len(test_pred)),\n",
    "        'test_pred_std': float(np.std(test_pred)),\n",
    "        'test_pred_mean': float(np.mean(test_pred))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print(\"  Saved: training_result.json\")\n",
    "\n",
    "# 2. model.json\n",
    "final_model.save_model('model.json')\n",
    "print(\"  Saved: model.json\")\n",
    "\n",
    "# 3. predictions_test.csv\n",
    "predictions_df = pd.DataFrame({\n",
    "    'date': list(dates_train) + list(dates_val) + list(dates_test),\n",
    "    'split': ['train'] * len(dates_train) + ['val'] * len(dates_val) + ['test'] * len(dates_test),\n",
    "    'prediction': np.concatenate([train_pred, val_pred, test_pred]),\n",
    "    'actual': np.concatenate([y_train, y_val, y_test])\n",
    "})\n",
    "predictions_df['direction_correct'] = np.sign(predictions_df['prediction']) == np.sign(predictions_df['actual'])\n",
    "predictions_df['high_confidence'] = np.abs(predictions_df['prediction']) > conf_threshold\n",
    "predictions_df.to_csv('predictions_test.csv', index=False)\n",
    "print(\"  Saved: predictions_test.csv\")\n",
    "\n",
    "# 4. submodel_output.csv (for compatibility)\n",
    "full_pred = final_model.predict(xgb.DMatrix(all_features.values))\n",
    "submodel_output = pd.DataFrame({\n",
    "    'meta_prediction': full_pred\n",
    "}, index=all_features.index)\n",
    "submodel_output.to_csv('submodel_output.csv')\n",
    "print(\"  Saved: submodel_output.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Meta-Model Training Complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Direction Accuracy: {test_metrics['direction_accuracy']*100:.2f}% (target: >56%)\")\n",
    "print(f\"  High-Confidence DA: {test_metrics['high_confidence_da']*100:.2f}% (target: >60%)\")\n",
    "print(f\"  MAE: {test_metrics['mae']:.4f}% (target: <0.75%)\")\n",
    "print(f\"  Sharpe Ratio: {test_metrics['sharpe_ratio']:.3f} (target: >0.8)\")\n",
    "\n",
    "# Check if targets met\n",
    "targets_met = [\n",
    "    test_metrics['direction_accuracy'] > 0.56,\n",
    "    test_metrics['high_confidence_da'] > 0.60,\n",
    "    test_metrics['mae'] < 0.75,\n",
    "    test_metrics['sharpe_ratio'] > 0.8\n",
    "]\n",
    "\n",
    "print(f\"\\nTargets met: {sum(targets_met)}/4\")\n",
    "if all(targets_met):\n",
    "    print(\"\\nüéâ ALL TARGETS MET! Phase 3 complete.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some targets not met. Attempt 2 may be needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
