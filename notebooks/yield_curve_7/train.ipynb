{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel Training — Yield Curve Attempt 7\n",
    "\n",
    "**Approach**: 2Y Policy Velocity + 2Y-10Y Slope Velocity\n",
    "\n",
    "The 2-year Treasury yield is the most Fed-policy-sensitive point on the curve and captures expected monetary policy trajectory over the next 2 years — the most actionable window for gold positioning. Unlike att2's (DGS10-DGS3MO) which anchors to the overnight rate, this approach anchors to the policy-expectation-driven 2Y.\n",
    "\n",
    "**Features**:\n",
    "1. `yc_2y_vel_z`: Z-scored daily change in DGS2 (2Y Treasury yield velocity)\n",
    "2. `yc_2y10y_vel_z`: Z-scored daily change in (DGS10 − DGS2) slope (classic steepener/flattener)\n",
    "\n",
    "**Notebook flow**: Data fetch → Feature engineering → Optuna HPO (z-score window) → Quality checks → Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# === 2. Constants ===\n",
    "FEATURE_NAME = \"yield_curve\"\n",
    "ATTEMPT = 7\n",
    "OUTPUT_COLUMNS = ['yc_2y_vel_z', 'yc_2y10y_vel_z']\n",
    "CLIP_RANGE = (-4, 4)\n",
    "\n",
    "print(f\"Yield Curve Submodel — Attempt {ATTEMPT}\")\n",
    "print(f\"Approach: 2Y Policy Velocity + 2Y-10Y Slope Velocity\")\n",
    "print(f\"Output columns: {OUTPUT_COLUMNS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Dataset Path Resolution (standard block from MEMORY.md) ===\n",
    "import glob as _glob\n",
    "\n",
    "PROBE_FILES = ['base_features.csv', 'base_features_raw.csv', 'vix.csv']\n",
    "candidates = [\n",
    "    '/kaggle/input/gold-prediction-submodels',\n",
    "    '/kaggle/input/datasets/bigbigzabuton/gold-prediction-submodels'\n",
    "]\n",
    "DATASET_PATH = None\n",
    "for c in candidates:\n",
    "    if os.path.isdir(c) and any(f in os.listdir(c) for f in PROBE_FILES):\n",
    "        DATASET_PATH = c\n",
    "        break\n",
    "    elif os.path.isdir(c):\n",
    "        print(f'Dir exists but probe files missing: {c} -> {os.listdir(c)[:5]}')\n",
    "\n",
    "if DATASET_PATH is None:\n",
    "    raise RuntimeError(\n",
    "        f'Dataset not found. Tried: {candidates}. '\n",
    "        f'/kaggle/input/: {os.listdir(\"/kaggle/input\")}'\n",
    "    )\n",
    "print(f\"DATASET_PATH = {DATASET_PATH}\")\n",
    "print(f\"Contents: {os.listdir(DATASET_PATH)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === 4. FRED API Key ===\n# Do NOT raise if key is missing — will fall back to FRED public CSV endpoints (no key required).\nFRED_API_KEY = None\ntry:\n    from kaggle_secrets import UserSecretsClient\n    FRED_API_KEY = UserSecretsClient().get_secret(\"FRED_API_KEY\")\n    print(\"FRED_API_KEY loaded from Kaggle Secrets\")\nexcept Exception as e:\n    FRED_API_KEY = os.environ.get('FRED_API_KEY')\n    if FRED_API_KEY:\n        print(\"FRED_API_KEY loaded from environment variable\")\n    else:\n        print(f\"WARNING: FRED_API_KEY not available ({e}). Will use FRED public CSV endpoints (no key required).\")\n\ntry:\n    from fredapi import Fred\nexcept ImportError:\n    import subprocess, sys\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'fredapi', '-q'])\n    from fredapi import Fred\n\nif FRED_API_KEY:\n    print(\"FRED API: will use fredapi with key\")\nelse:\n    print(\"FRED API: will use public CSV fallback (no key)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === 5. Fetch Yield Data from FRED ===\n\ndef fetch_fred_series(ticker, start, api_key=None):\n    \"\"\"Fetch FRED series via fredapi (if key available) or public CSV fallback (no key needed).\"\"\"\n    if api_key:\n        try:\n            client = Fred(api_key=api_key)\n            s = client.get_series(ticker, observation_start=start)\n            print(f\"  {ticker}: loaded via fredapi ({len(s.dropna())} obs)\")\n            return s\n        except Exception as e:\n            print(f\"  {ticker}: fredapi failed ({e}), falling back to public CSV...\")\n    # Fallback: FRED public CSV endpoint (no API key required)\n    url = f'https://fred.stlouisfed.org/graph/fredgraph.csv?id={ticker}'\n    df = pd.read_csv(url, index_col=0, parse_dates=True, na_values='.')\n    s = df.iloc[:, 0]\n    s.index = pd.to_datetime(s.index)\n    s = s[s.index >= pd.Timestamp(start)]\n    print(f\"  {ticker}: loaded via public CSV ({len(s.dropna())} obs)\")\n    return s\n\n\nprint(\"Fetching FRED yield data...\")\nDATA_START = '2014-06-01'  # Buffer for rolling window warmup\n\ndgs10 = fetch_fred_series('DGS10', DATA_START, FRED_API_KEY)\ndgs2  = fetch_fred_series('DGS2',  DATA_START, FRED_API_KEY)\ndgs3m = fetch_fred_series('DGS3MO', DATA_START, FRED_API_KEY)\n\n# Forward-fill up to 3 days for weekends/holidays (FRED data gaps)\ndgs10 = dgs10.ffill(limit=3)\ndgs2  = dgs2.ffill(limit=3)\ndgs3m = dgs3m.ffill(limit=3)\n\n# Align to common dates (inner join)\nyields_df = pd.DataFrame({\n    'dgs10': dgs10,\n    'dgs2':  dgs2,\n    'dgs3m': dgs3m,\n}).dropna()\n\nprint(f\"Yield data: {len(yields_df)} rows\")\nprint(f\"  Date range: {yields_df.index[0].date()} to {yields_df.index[-1].date()}\")\nprint(f\"  DGS10 range: {yields_df['dgs10'].min():.2f}% to {yields_df['dgs10'].max():.2f}%\")\nprint(f\"  DGS2  range: {yields_df['dgs2'].min():.2f}% to {yields_df['dgs2'].max():.2f}%\")\nprint(f\"  DGS3MO range: {yields_df['dgs3m'].min():.2f}% to {yields_df['dgs3m'].max():.2f}%\")\n\n# Sanity check: 2Y-10Y spread\nspread_2y10y = yields_df['dgs10'] - yields_df['dgs2']\nprint(f\"  10Y-2Y spread range: {spread_2y10y.min():.2f}% to {spread_2y10y.max():.2f}%\")\nprint(f\"  Current 10Y-2Y: {spread_2y10y.iloc[-1]:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === 6. Fetch Gold Price for Target ===\nimport yfinance as yf\nprint(\"Fetching gold price data...\")\n\ngold = yf.download('GC=F', start=DATA_START, auto_adjust=True, progress=False)\nif gold.empty or len(gold) < 100:\n    raise ValueError(f\"GC=F download returned insufficient data: {len(gold)} rows\")\n\n# Handle yfinance multi-index columns (newer yfinance versions return MultiIndex)\nif isinstance(gold.columns, pd.MultiIndex):\n    gold_close = gold['Close'].iloc[:, 0]\nelse:\n    gold_close = gold['Close'].squeeze()\n\ngold_ret_next = gold_close.pct_change().shift(-1) * 100  # next-day return in %\ngold_ret_next.index = pd.to_datetime(gold_ret_next.index).tz_localize(None)\ngold_ret_next.name = 'gold_return_next'\n\nprint(f\"Gold data: {len(gold_ret_next.dropna())} valid observations\")\nprint(f\"  Date range: {gold_ret_next.dropna().index[0].date()} to {gold_ret_next.dropna().index[-1].date()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Load Base Features for Date Alignment ===\n",
    "bf_path = os.path.join(DATASET_PATH, 'base_features.csv')\n",
    "if not os.path.exists(bf_path):\n",
    "    bf_path = os.path.join(DATASET_PATH, 'base_features_raw.csv')\n",
    "    print(f\"Using base_features_raw.csv\")\n",
    "\n",
    "base_features = pd.read_csv(bf_path, parse_dates=['Date'], index_col='Date')\n",
    "print(f\"Base features: {len(base_features)} rows\")\n",
    "print(f\"  Date range: {base_features.index[0].date()} to {base_features.index[-1].date()}\")\n",
    "print(f\"  Columns: {list(base_features.columns[:5])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Feature Generation Functions ===\n",
    "\n",
    "def rolling_zscore(x, window):\n",
    "    \"\"\"Rolling z-score with NaN handling.\"\"\"\n",
    "    min_per = max(window // 2, 10)\n",
    "    m = x.rolling(window, min_periods=min_per).mean()\n",
    "    s = x.rolling(window, min_periods=min_per).std()\n",
    "    z = (x - m) / s\n",
    "    z = z.replace([np.inf, -np.inf], np.nan)\n",
    "    return z\n",
    "\n",
    "\n",
    "def generate_features(yields_df, zscore_window):\n",
    "    \"\"\"\n",
    "    Generate 2Y policy velocity and 2Y-10Y slope velocity z-scores.\n",
    "    \n",
    "    Features:\n",
    "    1. yc_2y_vel_z: Z-scored daily change in DGS2\n",
    "       - Captures Fed policy expectation velocity\n",
    "       - Rising = Fed expected to hike more → gold-negative\n",
    "       - Falling = Fed expected to cut → gold-positive\n",
    "    \n",
    "    2. yc_2y10y_vel_z: Z-scored daily change in (DGS10 - DGS2)\n",
    "       - Classic steepener/flattener signal\n",
    "       - Rising (steepening) = growth optimism / recession recovery → complex gold signal\n",
    "       - Falling (flattening) = recession fears / Fed overtightening → gold-positive (safe haven)\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=yields_df.index)\n",
    "    \n",
    "    # Feature 1: 2Y yield velocity z-score\n",
    "    dgs2_vel = yields_df['dgs2'].diff()\n",
    "    features['yc_2y_vel_z'] = rolling_zscore(dgs2_vel, zscore_window).clip(*CLIP_RANGE)\n",
    "    \n",
    "    # Feature 2: 10Y-2Y spread velocity z-score (steepener/flattener)\n",
    "    spread_2y10y = yields_df['dgs10'] - yields_df['dgs2']\n",
    "    spread_vel = spread_2y10y.diff()\n",
    "    features['yc_2y10y_vel_z'] = rolling_zscore(spread_vel, zscore_window).clip(*CLIP_RANGE)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "print(\"Feature generation functions defined\")\n",
    "print(f\"Features: {OUTPUT_COLUMNS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. MI Computation Helper ===\n",
    "\n",
    "def compute_mi(feature, target, n_bins=20):\n",
    "    \"\"\"MI between feature and target using quantile binning.\"\"\"\n",
    "    valid = feature.dropna().index.intersection(target.dropna().index)\n",
    "    if len(valid) < 50:\n",
    "        return 0.0\n",
    "    f = feature[valid]\n",
    "    t = target[valid]\n",
    "    try:\n",
    "        f_binned = pd.qcut(f, q=n_bins, labels=False, duplicates='drop')\n",
    "        t_binned = pd.qcut(t, q=n_bins, labels=False, duplicates='drop')\n",
    "        return float(mutual_info_score(f_binned, t_binned))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "print(\"MI computation helper defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. Data Split ===\n",
    "common_dates = (\n",
    "    yields_df.index\n",
    "    .intersection(base_features.index)\n",
    "    .intersection(gold_ret_next.dropna().index)\n",
    "    .sort_values()\n",
    ")\n",
    "\n",
    "n = len(common_dates)\n",
    "train_end = int(n * 0.70)\n",
    "val_end   = int(n * 0.85)\n",
    "\n",
    "train_dates = common_dates[:train_end]\n",
    "val_dates   = common_dates[train_end:val_end]\n",
    "test_dates  = common_dates[val_end:]\n",
    "\n",
    "print(f\"Common dates: {n} total\")\n",
    "print(f\"  Train: {len(train_dates)} ({train_dates[0].date()} to {train_dates[-1].date()})\")\n",
    "print(f\"  Val:   {len(val_dates)}   ({val_dates[0].date()} to {val_dates[-1].date()})\")\n",
    "print(f\"  Test:  {len(test_dates)}  ({test_dates[0].date()} to {test_dates[-1].date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 11. Optuna HPO ===\n",
    "\n",
    "val_target = gold_ret_next.reindex(common_dates)[common_dates.isin(val_dates)]\n",
    "\n",
    "def objective(trial):\n",
    "    zscore_window = trial.suggest_categorical('zscore_window', [20, 30, 45, 60, 90, 120])\n",
    "    \n",
    "    features = generate_features(yields_df, zscore_window)\n",
    "    \n",
    "    mi_sum = 0.0\n",
    "    for col in OUTPUT_COLUMNS:\n",
    "        feat_val = features[col].reindex(val_dates)\n",
    "        mi_sum += compute_mi(feat_val, val_target)\n",
    "    \n",
    "    return mi_sum\n",
    "\n",
    "\n",
    "print(\"Running Optuna HPO (z-score window selection)...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(objective, n_trials=25, timeout=300, show_progress_bar=False)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_value  = study.best_value\n",
    "\n",
    "print(f\"\\nOptuna complete: {len(study.trials)} trials\")\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best MI sum (val): {best_value:.4f}\")\n",
    "\n",
    "print(\"\\n=== All Optuna Trials (sorted by MI sum) ===\")\n",
    "for t in sorted(study.trials, key=lambda x: x.value or 0, reverse=True):\n",
    "    print(f\"  window={t.params['zscore_window']:>4d}  MI_sum={t.value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 12. Generate Final Features with Best Params ===\n",
    "print(f\"\\nGenerating final features with zscore_window={best_params['zscore_window']}...\")\n",
    "final_features = generate_features(yields_df, zscore_window=best_params['zscore_window'])\n",
    "\n",
    "print(f\"Generated features shape: {final_features.shape}\")\n",
    "print(f\"Sample (last 5 rows):\")\n",
    "print(final_features.tail().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 13. Quality Checks ===\n",
    "print(\"\\n=== GATE 1: Quality Checks ===\")\n",
    "\n",
    "autocorr_results = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    ac = final_features[col].dropna().autocorr(lag=1)\n",
    "    autocorr_results[col] = float(ac)\n",
    "    status = \"PASS\" if abs(ac) < 0.95 else \"FAIL\"\n",
    "    print(f\"  Autocorr(lag=1) {col}: {ac:.4f} [{status}]\")\n",
    "\n",
    "# Internal correlation\n",
    "corr_matrix = final_features[OUTPUT_COLUMNS].dropna().corr()\n",
    "internal_corr = float(corr_matrix.iloc[0, 1])\n",
    "print(f\"\\n  Internal correlation ({OUTPUT_COLUMNS[0]} vs {OUTPUT_COLUMNS[1]}): {internal_corr:.4f}\")\n",
    "\n",
    "# VIF check\n",
    "from numpy.linalg import inv as np_inv\n",
    "X = final_features[OUTPUT_COLUMNS].dropna().values\n",
    "cm = np.corrcoef(X.T)\n",
    "try:\n",
    "    inv_cm = np_inv(cm)\n",
    "    vif_values = np.diag(inv_cm)\n",
    "    for col, v in zip(OUTPUT_COLUMNS, vif_values):\n",
    "        status = \"PASS\" if v < 10 else \"FAIL\"\n",
    "        print(f\"  VIF {col}: {v:.3f} [{status}]\")\n",
    "    vif_max = float(np.max(vif_values))\n",
    "except Exception as e:\n",
    "    print(f\"  VIF calculation failed: {e}\")\n",
    "    vif_max = None\n",
    "\n",
    "# NaN check\n",
    "print(\"\\n  NaN counts:\")\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    n_nan = final_features[col].isna().sum()\n",
    "    n_total = len(final_features)\n",
    "    print(f\"    {col}: {n_nan}/{n_total} ({100*n_nan/n_total:.1f}%)\")\n",
    "\n",
    "# Descriptive stats\n",
    "print(\"\\n  Descriptive statistics:\")\n",
    "print(final_features[OUTPUT_COLUMNS].describe().round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 14. MI per Feature (Validation Set) ===\n",
    "print(\"\\n=== GATE 2: Information Content ===\")\n",
    "\n",
    "individual_mi = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    feat_val = final_features[col].reindex(val_dates)\n",
    "    mi = compute_mi(feat_val, val_target)\n",
    "    individual_mi[col] = mi\n",
    "    print(f\"  MI({col}, gold_return_next) = {mi:.4f}\")\n",
    "\n",
    "print(f\"  Total MI sum: {sum(individual_mi.values()):.4f}\")\n",
    "\n",
    "# Rolling correlation stability check\n",
    "print(\"\\n  Rolling correlation std (stability):\")\n",
    "stability = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    rolling_corr = final_features[col].dropna().rolling(63).corr(\n",
    "        gold_ret_next.reindex(final_features.index)\n",
    "    )\n",
    "    std = float(rolling_corr.std())\n",
    "    stability[col] = std\n",
    "    status = \"PASS\" if std < 0.15 else \"FAIL\"\n",
    "    print(f\"    {col}: {std:.4f} [{status}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 15. Correlation vs Attempt 2 Features (for Gate 2 VIF) ===\n",
    "print(\"\\n=== Correlation with Attempt 2 yield_curve features ===\")\n",
    "\n",
    "# Load yield_curve.csv from dataset (contains att2 production features)\n",
    "yc_path = os.path.join(DATASET_PATH, 'yield_curve.csv')\n",
    "if os.path.exists(yc_path):\n",
    "    yc_df = pd.read_csv(yc_path, parse_dates=['Date'], index_col='Date')\n",
    "    print(f\"  Loaded yield_curve.csv: {yc_df.shape}, columns: {list(yc_df.columns)}\")\n",
    "    \n",
    "    # Compute correlations\n",
    "    max_corr_overall = 0.0\n",
    "    for new_col in OUTPUT_COLUMNS:\n",
    "        max_corr = 0.0\n",
    "        for att2_col in yc_df.columns:\n",
    "            common = final_features[new_col].dropna().index.intersection(yc_df[att2_col].dropna().index)\n",
    "            if len(common) > 100:\n",
    "                corr = abs(float(final_features[new_col][common].corr(yc_df[att2_col][common])))\n",
    "                max_corr = max(max_corr, corr)\n",
    "        print(f\"  Max |corr| of {new_col} with att2 features: {max_corr:.4f}\")\n",
    "        max_corr_overall = max(max_corr_overall, max_corr)\n",
    "    \n",
    "    # Also check correlation with att2's 3m-10y spread velocity proxy\n",
    "    dgs3m_series = yields_df['dgs3m']\n",
    "    spread_3m10y = yields_df['dgs10'] - yields_df['dgs3m']\n",
    "    spread_3m10y_vel = spread_3m10y.diff()\n",
    "    att2_proxy = rolling_zscore(spread_3m10y_vel, best_params['zscore_window']).clip(*CLIP_RANGE)\n",
    "    \n",
    "    for new_col in OUTPUT_COLUMNS:\n",
    "        common = final_features[new_col].dropna().index.intersection(att2_proxy.dropna().index)\n",
    "        corr = abs(float(final_features[new_col][common].corr(att2_proxy[common])))\n",
    "        print(f\"  Corr({new_col}, att2_proxy_3m10y_vel_z): {corr:.4f}\")\nelse:\n",
    "    print(\"  yield_curve.csv not found in dataset. Skipping att2 correlation check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 16. Align to Base Features Dates and Save ===\n",
    "print(\"\\n=== Aligning to base_features date range ===\")\n",
    "\n",
    "output = final_features[OUTPUT_COLUMNS].reindex(base_features.index)\n",
    "\n",
    "# Forward-fill up to 3 days for minor gaps (holidays etc.)\n",
    "output = output.ffill(limit=3)\n",
    "\n",
    "# Drop rows that are entirely NaN (warmup period)\n",
    "output = output.dropna(how='all')\n",
    "output.index.name = 'Date'\n",
    "\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Date range: {output.index[0].date()} to {output.index[-1].date()}\")\n",
    "print(f\"  NaN per column after alignment:\")\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    n_nan = output[col].isna().sum()\n",
    "    print(f\"    {col}: {n_nan} ({100*n_nan/len(output):.1f}%)\")\n",
    "\n",
    "print(f\"\\n  Sample output (last 5 rows):\")\n",
    "print(output.tail().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === 17. Save Outputs ===\noutput.to_csv(\"/kaggle/working/submodel_output.csv\")\nprint(f\"Saved /kaggle/working/submodel_output.csv ({len(output)} rows x {len(OUTPUT_COLUMNS)} columns)\")\n\n# Save training result JSON\nresult = {\n    \"feature\": FEATURE_NAME,\n    \"attempt\": ATTEMPT,\n    \"timestamp\": datetime.now().isoformat(),\n    \"approach\": \"2Y Policy Velocity + 2Y-10Y Slope Velocity\",\n    \"description\": (\n        \"Z-scored daily changes in the 2Y Treasury yield (yc_2y_vel_z: Fed policy expectations) \"\n        \"and the 10Y-2Y spread (yc_2y10y_vel_z: classic steepener/flattener). \"\n        \"Uses the policy-sensitive 2Y anchor vs att2's overnight 3M anchor. \"\n        \"2 features, both velocity-based (daily changes), no level variables.\"\n    ),\n    \"output_columns\": OUTPUT_COLUMNS,\n    \"best_params\": best_params,\n    \"n_trials\": len(study.trials),\n    \"best_mi_sum_val\": float(best_value),\n    \"metrics\": {\n        \"autocorrelations\": autocorr_results,\n        \"individual_mi_val\": individual_mi,\n        \"internal_correlation\": internal_corr,\n        \"stability\": stability,\n        \"vif_max\": vif_max,\n    },\n    \"output_shape\": list(output.shape),\n    \"output_nan_counts\": {col: int(output[col].isna().sum()) for col in OUTPUT_COLUMNS},\n    \"gate1_expected\": \"PASS (velocity features, near-zero autocorr expected)\",\n    \"gate2_expected\": \"UNCERTAIN (2Y is different tenor than att2; some overlap possible)\",\n    \"gate3_expected\": \"UNCERTAIN (2Y policy channel is key for gold; different from att6)\",\n}\n\nwith open(\"/kaggle/working/training_result.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(result, f, indent=2, default=str)\n\nprint(\"Saved /kaggle/working/training_result.json\")\nprint(\"\\n=== Training Complete ===\")\nprint(f\"Feature: {FEATURE_NAME} | Attempt: {ATTEMPT}\")\nprint(f\"Output columns: {OUTPUT_COLUMNS}\")\nprint(f\"Best zscore_window: {best_params['zscore_window']}\")\nprint(f\"Best MI sum (val): {best_value:.4f}\")\nprint(f\"Autocorr: {autocorr_results}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}