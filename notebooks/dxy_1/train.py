"""
Gold Prediction SubModel Training - DXY Attempt 1 (Revised)
Self-contained: Data fetch -> Preprocessing -> GMM + PCA + Vol -> Optuna HPO -> Save results
Generated by builder_model agent (revised to use Kaggle pre-installed libraries only)
"""

# ============================================================
# 1. IMPORTS AND SETUP
# ============================================================
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_regression
from sklearn.model_selection import TimeSeriesSplit
import optuna
import json
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)

print(f"=== Gold SubModel Training: DXY attempt 1 (Revised) ===")
print(f"Started: {datetime.now().isoformat()}")

# ============================================================
# 2. DATA FETCHING
# ============================================================

def fetch_data():
    """
    Fetch DXY and 6 constituent currency pairs from Yahoo Finance.
    Returns aligned dataframe with all 7 series.
    """
    tickers = {
        'DXY': 'DX-Y.NYB',
        'EURUSD': 'EURUSD=X',
        'USDJPY': 'JPY=X',
        'GBPUSD': 'GBPUSD=X',
        'USDCAD': 'CAD=X',
        'USDSEK': 'SEK=X',
        'USDCHF': 'CHF=X'
    }

    print("\n[Data Fetch] Downloading currency data from Yahoo Finance...")

    dfs = {}
    for name, ticker in tickers.items():
        try:
            df = yf.download(ticker, start='2014-12-01', progress=False)
            if df.empty:
                print(f"[WARN] {ticker} returned empty dataframe")
                continue

            # Handle both single-ticker (Series) and multi-ticker (DataFrame) formats
            if 'Close' in df.columns:
                close_series = df['Close']
            elif isinstance(df.columns, pd.MultiIndex):
                # MultiIndex case: ('Close', ticker)
                close_series = df[('Close', ticker)]
            else:
                # Fallback: assume single column
                close_series = df.iloc[:, 0]

            # Ensure it's a Series and rename
            if isinstance(close_series, pd.DataFrame):
                close_series = close_series.iloc[:, 0]

            close_series.name = name
            dfs[name] = close_series
            print(f"[OK] {ticker}: {len(df)} rows, latest={close_series.iloc[-1]:.4f}")
        except Exception as e:
            print(f"[FAIL] {ticker}: {e}")
            raise

    # Combine all series
    combined = pd.concat(dfs.values(), axis=1, join='inner')
    print(f"\n[Data Fetch] Combined: {len(combined)} rows after inner join")
    print(f"Date range: {combined.index[0]} to {combined.index[-1]}")

    return combined

# ============================================================
# 3. PREPROCESSING
# ============================================================

def preprocess_data(df):
    """
    Compute log-returns, normalize currency directions, align dates.
    Returns dataframe with DXY returns and 6 currency returns (all normalized to USD-strength direction).
    """
    print("\n[Preprocessing] Computing log-returns and normalizing directions...")

    # Compute log-returns
    returns = np.log(df / df.shift(1)).dropna()

    # Normalize currency directions
    # EURUSD and GBPUSD: higher value = weaker USD, so negate returns
    # Others (JPY, CAD, SEK, CHF): higher value = stronger USD, keep as-is
    returns['EURUSD'] = -returns['EURUSD']
    returns['GBPUSD'] = -returns['GBPUSD']

    print(f"[OK] Returns computed: {len(returns)} rows")
    print(f"Date range: {returns.index[0]} to {returns.index[-1]}")

    # Forward-fill gaps up to 3 days
    returns = returns.ffill(limit=3)

    # Drop any remaining NaN
    returns = returns.dropna()
    print(f"[OK] After gap-filling and NaN removal: {len(returns)} rows")

    return returns

# ============================================================
# 4. FEATURE ENGINEERING
# ============================================================

def compute_gmm_regime(dxy_returns, n_components=2, random_state=42):
    """
    Use Gaussian Mixture Model instead of HMM for regime detection.
    GMM clusters returns into states without temporal transition modeling.

    Returns: probability of being in high-volatility regime
    """
    print(f"\n[GMM Regime] Fitting {n_components}-component GMM...")

    X = dxy_returns.values.reshape(-1, 1)

    gmm = GaussianMixture(
        n_components=n_components,
        covariance_type='full',
        random_state=random_state,
        max_iter=200
    )
    gmm.fit(X)

    # Get posterior probabilities
    proba = gmm.predict_proba(X)

    # Identify high-variance component
    variances = gmm.covariances_.flatten()
    high_var_idx = np.argmax(variances)

    regime_prob = proba[:, high_var_idx]

    print(f"[OK] GMM fitted. Component variances: {variances}")
    print(f"[OK] High-variance component: {high_var_idx}, mean prob: {regime_prob.mean():.3f}")

    return regime_prob

def compute_rolling_pca_divergence(currency_returns, window=60):
    """
    Rolling PCA on 6 currency returns to measure cross-currency divergence.
    Returns: 1 - explained_variance_ratio[0], scaled to [0, 1]
    """
    print(f"\n[PCA Divergence] Computing {window}-day rolling PCA...")

    divergence = []
    dates = []

    for i in range(window - 1, len(currency_returns)):
        window_data = currency_returns.iloc[i - window + 1:i + 1].values

        if window_data.shape[0] < window:
            divergence.append(np.nan)
            dates.append(currency_returns.index[i])
            continue

        pca = PCA(n_components=6)
        pca.fit(window_data)

        # Divergence = 1 - PC1 explained variance
        div = 1.0 - pca.explained_variance_ratio_[0]
        divergence.append(div)
        dates.append(currency_returns.index[i])

    div_series = pd.Series(divergence, index=dates, name='raw_divergence')

    # Expanding window min-max scaling (no lookahead)
    expanding_min = div_series.expanding().min()
    expanding_max = div_series.expanding().max()
    scaled = (div_series - expanding_min) / (expanding_max - expanding_min + 1e-9)

    print(f"[OK] PCA divergence computed: mean={div_series.mean():.3f}, scaled mean={scaled.mean():.3f}")

    return scaled

def compute_realized_volatility_zscore(dxy_returns, window=20):
    """
    20-day realized volatility with expanding z-score normalization.
    Returns: z-score of realized volatility (unbounded)
    """
    print(f"\n[Volatility Z-Score] Computing {window}-day realized vol...")

    # Realized volatility
    realized_vol = dxy_returns.rolling(window).std()

    # Expanding z-score (no lookahead)
    expanding_mean = realized_vol.expanding().mean()
    expanding_std = realized_vol.expanding().std()
    z_score = (realized_vol - expanding_mean) / (expanding_std + 1e-9)

    print(f"[OK] Volatility z-score: mean={z_score.mean():.3f}, std={z_score.std():.3f}")

    return z_score

def generate_features(returns, gmm_components=2, pca_window=60, vol_window=20):
    """
    Generate all three submodel features.
    Returns: DataFrame with 3 columns aligned to returns index.
    """
    print("\n" + "="*70)
    print("FEATURE GENERATION")
    print("="*70)

    dxy_returns = returns['DXY']
    currency_returns = returns[['EURUSD', 'USDJPY', 'GBPUSD', 'USDCAD', 'USDSEK', 'USDCHF']]

    # Component 1: GMM regime probability
    regime_prob = compute_gmm_regime(dxy_returns, n_components=gmm_components)

    # Component 2: Rolling PCA divergence
    pca_div = compute_rolling_pca_divergence(currency_returns, window=pca_window)

    # Component 3: Realized volatility z-score
    vol_z = compute_realized_volatility_zscore(dxy_returns, window=vol_window)

    # Combine into single dataframe
    features = pd.DataFrame({
        'dxy_regime_probability': regime_prob,
        'dxy_cross_currency_div': np.nan,
        'dxy_volatility_z': vol_z
    }, index=returns.index)

    # Align PCA divergence (starts later due to rolling window)
    features.loc[pca_div.index, 'dxy_cross_currency_div'] = pca_div.values

    # Drop warmup period (first 60 rows to ensure PCA is computed)
    features = features.iloc[60:].copy()

    print(f"\n[OK] Features generated: {features.shape}")
    print(f"[OK] Date range: {features.index[0]} to {features.index[-1]}")
    print(f"[OK] NaN count: {features.isna().sum().to_dict()}")

    # Drop any remaining NaN
    features = features.dropna()
    print(f"[OK] After NaN removal: {features.shape}")

    return features

# ============================================================
# 5. OPTUNA HPO
# ============================================================

def objective(trial, returns):
    """
    Optuna objective: optimize hyperparameters for feature generation.
    Evaluation metric: average mutual information with gold returns.
    """
    # Hyperparameters
    gmm_components = trial.suggest_int('gmm_components', 2, 3)
    pca_window = trial.suggest_int('pca_window', 40, 80, step=10)
    vol_window = trial.suggest_int('vol_window', 10, 30, step=5)

    try:
        features = generate_features(
            returns,
            gmm_components=gmm_components,
            pca_window=pca_window,
            vol_window=vol_window
        )

        if len(features) < 100:
            return -999.0

        # Load gold returns for MI calculation
        try:
            # Kaggle Dataset path: /kaggle/input/{dataset-slug}/{filename}
            target_path = '/kaggle/input/gold-price-target/target.csv'
            gold_df = pd.read_csv(target_path, index_col=0, parse_dates=True)

            # Handle column name (either 'gold_return' or 'gold_return_next')
            if 'gold_return' in gold_df.columns:
                gold_returns = gold_df['gold_return']
            elif 'gold_return_next' in gold_df.columns:
                gold_returns = gold_df['gold_return_next']
            else:
                raise ValueError(f"Target column not found. Available: {gold_df.columns.tolist()}")

            gold_returns = gold_returns.reindex(features.index).dropna()

            # Align features and gold returns
            common_idx = features.index.intersection(gold_returns.index)
            if len(common_idx) < 100:
                return -999.0

            features_aligned = features.loc[common_idx]
            gold_aligned = gold_returns.loc[common_idx].values

            # Compute MI for each feature
            mi_scores = []
            for col in features_aligned.columns:
                X = features_aligned[col].values.reshape(-1, 1)
                mi = mutual_info_regression(X, gold_aligned, random_state=42)[0]
                mi_scores.append(mi)

            avg_mi = np.mean(mi_scores)

            print(f"\n[Trial {trial.number}] gmm={gmm_components}, pca_w={pca_window}, vol_w={vol_window} -> MI={avg_mi:.6f}")

            return avg_mi

        except FileNotFoundError:
            print("[WARN] target.csv not found, using dummy score")
            return -999.0

    except Exception as e:
        print(f"[Trial {trial.number}] FAILED: {e}")
        return -999.0

def run_hpo(returns, n_trials=20):
    """
    Run Optuna hyperparameter optimization.
    Returns: best_params, best_value
    """
    print("\n" + "="*70)
    print(f"OPTUNA HPO ({n_trials} trials)")
    print("="*70)

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42)
    )

    study.optimize(
        lambda trial: objective(trial, returns),
        n_trials=n_trials,
        show_progress_bar=True
    )

    print(f"\n[OK] Best trial: {study.best_trial.number}")
    print(f"[OK] Best params: {study.best_params}")
    print(f"[OK] Best MI: {study.best_value:.6f}")

    return study.best_params, study.best_value

# ============================================================
# 6. MAIN EXECUTION
# ============================================================

if __name__ == "__main__":
    try:
        # Fetch data
        raw_data = fetch_data()

        # Preprocess
        returns = preprocess_data(raw_data)

        # HPO
        best_params, best_mi = run_hpo(returns, n_trials=30)

        # Generate features with best params
        print("\n" + "="*70)
        print("FINAL FEATURE GENERATION")
        print("="*70)

        final_features = generate_features(
            returns,
            gmm_components=best_params['gmm_components'],
            pca_window=best_params['pca_window'],
            vol_window=best_params['vol_window']
        )

        # Save outputs
        print("\n" + "="*70)
        print("SAVING OUTPUTS")
        print("="*70)

        final_features.to_csv('submodel_output.csv')
        print(f"[OK] Saved submodel_output.csv: {final_features.shape}")

        # Training result metadata
        result = {
            "feature": "dxy",
            "attempt": 1,
            "timestamp": datetime.now().isoformat(),
            "method": "GMM + Rolling PCA + Realized Vol Z-Score",
            "best_params": best_params,
            "best_mi": float(best_mi),
            "output_shape": list(final_features.shape),
            "output_columns": list(final_features.columns),
            "date_range": {
                "start": str(final_features.index[0]),
                "end": str(final_features.index[-1])
            },
            "descriptive_stats": {
                col: {
                    "mean": float(final_features[col].mean()),
                    "std": float(final_features[col].std()),
                    "min": float(final_features[col].min()),
                    "max": float(final_features[col].max())
                }
                for col in final_features.columns
            }
        }

        with open('training_result.json', 'w') as f:
            json.dump(result, f, indent=2)

        print(f"[OK] Saved training_result.json")

        print("\n" + "="*70)
        print("TRAINING COMPLETE")
        print("="*70)
        print(f"Output: {final_features.shape[0]} rows x {final_features.shape[1]} columns")
        print(f"Columns: {list(final_features.columns)}")
        print(f"Best MI: {best_mi:.6f}")

    except Exception as e:
        print(f"\n[FATAL ERROR] {e}")
        import traceback
        traceback.print_exc()

        # Save error info
        error_result = {
            "feature": "dxy",
            "attempt": 1,
            "timestamp": datetime.now().isoformat(),
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc()
        }

        with open('training_result.json', 'w') as f:
            json.dump(error_result, f, indent=2)

        raise
