"""
Gold Prediction SubModel Training - DXY Attempt 1
Self-contained: Data fetch → Preprocessing → HMM + PCA + Vol → Optuna HPO → Save results
Generated by builder_model agent
"""

# ============================================================
# 1. IMPORTS AND SETUP
# ============================================================
import subprocess
import sys

# Install hmmlearn (not pre-installed on Kaggle)
print("Installing hmmlearn...")
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'hmmlearn', '-q'])

import numpy as np
import pandas as pd
import yfinance as yf
from hmmlearn.hmm import GaussianHMM
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_regression
import optuna
import json
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)

print(f"=== Gold SubModel Training: DXY attempt 1 ===")
print(f"Started: {datetime.now().isoformat()}")

# ============================================================
# 2. DATA FETCHING
# ============================================================

def fetch_data():
    """
    Fetch DXY and 6 constituent currency pairs from Yahoo Finance.
    Returns aligned dataframe with all 7 series.
    """
    tickers = {
        'DXY': 'DX-Y.NYB',
        'EURUSD': 'EURUSD=X',
        'USDJPY': 'JPY=X',
        'GBPUSD': 'GBPUSD=X',
        'USDCAD': 'CAD=X',
        'USDSEK': 'SEK=X',
        'USDCHF': 'CHF=X'
    }

    # Start date with 60-day buffer for rolling windows
    start_date = '2014-12-01'
    end_date = '2025-02-13'

    print(f"\nFetching data from {start_date} to {end_date}...")

    data_frames = {}
    for name, ticker in tickers.items():
        try:
            print(f"Downloading {name} ({ticker})...")
            df = yf.download(ticker, start=start_date, end=end_date, progress=False)
            if df.empty:
                raise ValueError(f"No data returned for {ticker}")

            # Extract Close column
            if isinstance(df.columns, pd.MultiIndex):
                close_series = df['Close'].iloc[:, 0]
            else:
                close_series = df['Close']
            close_series.name = name
            data_frames[name] = close_series
            print(f"  -> {len(df)} rows fetched")
        except Exception as e:
            raise RuntimeError(f"Failed to fetch {name} ({ticker}): {e}")

    # Combine all series
    raw_prices = pd.concat(data_frames.values(), axis=1)
    raw_prices.index.name = 'Date'

    # Forward-fill missing values (max 3 days)
    raw_prices = raw_prices.ffill(limit=3)

    # Drop rows with any remaining NaN
    initial_rows = len(raw_prices)
    raw_prices = raw_prices.dropna()
    dropped_rows = initial_rows - len(raw_prices)
    print(f"\nDropped {dropped_rows} rows with missing data")
    print(f"Final dataset: {len(raw_prices)} rows from {raw_prices.index[0]} to {raw_prices.index[-1]}")

    return raw_prices


def preprocess_data(raw_prices):
    """
    Compute log-returns and apply direction normalization for currency pairs.
    Returns: (dxy_returns, currency_returns_df, full_index)
    """
    # Compute log-returns
    log_returns = np.log(raw_prices / raw_prices.shift(1))
    log_returns = log_returns.dropna()

    # Extract DXY returns
    dxy_returns = log_returns['DXY']

    # Extract 6 currency returns with direction normalization
    # EURUSD and GBPUSD are inverse to DXY (higher = weaker USD), so negate
    # JPY, CAD, SEK, CHF are same direction (higher = stronger USD), keep as-is
    currency_returns = pd.DataFrame({
        'EURUSD': -log_returns['EURUSD'],  # Negate (inverse)
        'USDJPY': log_returns['USDJPY'],   # Keep
        'GBPUSD': -log_returns['GBPUSD'],  # Negate (inverse)
        'USDCAD': log_returns['USDCAD'],   # Keep
        'USDSEK': log_returns['USDSEK'],   # Keep
        'USDCHF': log_returns['USDCHF']    # Keep
    })

    print(f"\nReturns computed: {len(dxy_returns)} observations")
    print(f"DXY returns - mean: {dxy_returns.mean():.6f}, std: {dxy_returns.std():.6f}")

    return dxy_returns, currency_returns, log_returns.index


# ============================================================
# 3. FEATURE GENERATION FUNCTIONS
# ============================================================

def generate_regime_feature(dxy_returns, n_components=2, covariance_type='full', n_init=5):
    """
    Fit HMM and return P(high-variance state).

    Args:
        dxy_returns: Series of DXY log-returns
        n_components: Number of HMM states (2 or 3)
        covariance_type: HMM covariance type ('full' or 'diag')
        n_init: Number of EM initializations

    Returns:
        Array of probabilities for high-variance state
    """
    model = GaussianHMM(
        n_components=n_components,
        covariance_type=covariance_type,
        n_iter=100,
        tol=1e-4,
        random_state=42,
        init_params='stmc',
        n_init=n_init
    )

    X = dxy_returns.values.reshape(-1, 1)
    model.fit(X)
    probs = model.predict_proba(X)

    # Identify which state has higher variance
    state_vars = []
    for i in range(n_components):
        if covariance_type == 'full':
            state_vars.append(float(model.covars_[i][0, 0]))
        elif covariance_type == 'diag':
            state_vars.append(float(model.covars_[i][0]))
        else:  # spherical
            state_vars.append(float(model.covars_[i]))

    high_var_state = np.argmax(state_vars)
    return probs[:, high_var_state]


def generate_divergence_feature(currency_returns_df, pca_window=60):
    """
    Rolling PCA divergence: 1 - PC1_explained_variance_ratio.
    Uses expanding MinMax normalization to prevent lookahead.

    Args:
        currency_returns_df: DataFrame with 6 currency returns
        pca_window: Rolling window size for PCA

    Returns:
        Array of normalized divergence values [0, 1]
    """
    n = len(currency_returns_df)
    divergence = np.full(n, np.nan)

    for i in range(pca_window - 1, n):
        window_data = currency_returns_df.iloc[i - pca_window + 1: i + 1].values
        if np.any(np.isnan(window_data)):
            continue

        pca = PCA(n_components=min(6, pca_window))
        pca.fit(window_data)
        divergence[i] = 1.0 - pca.explained_variance_ratio_[0]

    # Expanding MinMax normalization (no lookahead)
    result = np.full(n, np.nan)
    for i in range(pca_window - 1, n):
        past_vals = divergence[pca_window - 1: i + 1]
        past_valid = past_vals[~np.isnan(past_vals)]
        if len(past_valid) < 2:
            result[i] = 0.5  # default for insufficient history
        else:
            vmin, vmax = past_valid.min(), past_valid.max()
            if vmax - vmin < 1e-10:
                result[i] = 0.5
            else:
                result[i] = (divergence[i] - vmin) / (vmax - vmin)

    return result


def generate_volatility_feature(dxy_returns, vol_window=20):
    """
    Realized volatility z-score with expanding normalization.

    Args:
        dxy_returns: Series of DXY log-returns
        vol_window: Window size for realized volatility

    Returns:
        Array of volatility z-scores
    """
    vol = dxy_returns.rolling(vol_window).std()

    # Expanding z-score (no lookahead)
    expanding_mean = vol.expanding().mean()
    expanding_std = vol.expanding().std()
    z_score = (vol - expanding_mean) / expanding_std
    z_score = z_score.clip(-4, 4)  # clip extreme outliers

    return z_score.values


def generate_all_features(dxy_returns, currency_returns, config):
    """
    Generate all 3 features using given config.

    Returns:
        DataFrame with 3 columns
    """
    regime = generate_regime_feature(
        dxy_returns,
        n_components=config['hmm_n_components'],
        covariance_type=config['hmm_covariance_type'],
        n_init=config['hmm_n_init']
    )

    divergence = generate_divergence_feature(
        currency_returns,
        pca_window=config['pca_window']
    )

    vol_z = generate_volatility_feature(
        dxy_returns,
        vol_window=config['vol_window']
    )

    # Create DataFrame
    features_df = pd.DataFrame({
        'dxy_regime_probability': regime,
        'dxy_cross_currency_div': divergence,
        'dxy_volatility_z': vol_z
    }, index=dxy_returns.index)

    return features_df


# ============================================================
# 4. OPTUNA OBJECTIVE
# ============================================================

def objective(trial, dxy_returns, currency_returns, target_series, val_start_idx, val_end_idx):
    """
    Optuna objective: Maximize sum of mutual information on validation set.

    Args:
        trial: Optuna trial object
        dxy_returns: Full DXY returns series
        currency_returns: Full currency returns DataFrame
        target_series: Full target series (gold_return_next)
        val_start_idx: Validation start index
        val_end_idx: Validation end index

    Returns:
        MI sum score (to maximize)
    """
    # Suggest hyperparameters
    config = {
        'hmm_n_components': trial.suggest_categorical('hmm_n_components', [2, 3]),
        'hmm_covariance_type': trial.suggest_categorical('hmm_covariance_type', ['full', 'diag']),
        'hmm_n_init': trial.suggest_categorical('hmm_n_init', [3, 5, 10]),
        'pca_window': trial.suggest_categorical('pca_window', [40, 60, 90]),
        'vol_window': trial.suggest_categorical('vol_window', [10, 20, 30])
    }

    try:
        # Generate features
        features_df = generate_all_features(dxy_returns, currency_returns, config)

        # Extract validation period
        val_features = features_df.iloc[val_start_idx:val_end_idx]
        val_target = target_series.iloc[val_start_idx:val_end_idx]

        # Remove NaN rows
        valid_mask = ~(val_features.isna().any(axis=1) | val_target.isna())
        val_features_clean = val_features[valid_mask]
        val_target_clean = val_target[valid_mask]

        if len(val_features_clean) < 50:
            return 0.0  # Insufficient data

        # Compute MI for each feature
        mi_sum = 0.0
        for col in val_features_clean.columns:
            mi = mutual_info_regression(
                val_features_clean[[col]].values,
                val_target_clean.values,
                random_state=42
            )[0]
            mi_sum += mi

        return mi_sum

    except Exception as e:
        print(f"Trial failed: {e}")
        return 0.0


# ============================================================
# 5. MAIN EXECUTION
# ============================================================

if __name__ == "__main__":

    # === Fetch and preprocess data ===
    raw_prices = fetch_data()
    dxy_returns, currency_returns, full_index = preprocess_data(raw_prices)

    print(f"\n=== Data Summary ===")
    print(f"Total observations: {len(dxy_returns)}")
    print(f"Date range: {full_index[0]} to {full_index[-1]}")

    # === Load target variable ===
    # For Kaggle: Fetch gold price and compute returns
    print("\n=== Fetching Gold Price for Target ===")
    gold_df = yf.download('GC=F', start='2014-12-01', end='2025-02-13', progress=False)
    if isinstance(gold_df.columns, pd.MultiIndex):
        gold_close = gold_df['Close'].iloc[:, 0]
    else:
        gold_close = gold_df['Close']

    # Compute next-day return
    gold_return = np.log(gold_close / gold_close.shift(1))
    gold_return_next = gold_return.shift(-1)  # Next day's return

    # Align with DXY data
    aligned_target = gold_return_next.reindex(full_index)
    print(f"Target variable: {(~aligned_target.isna()).sum()} non-NaN values")

    # === Data split: 70/15/15 ===
    n_total = len(dxy_returns)
    train_end_idx = int(n_total * 0.70)
    val_end_idx = int(n_total * 0.85)

    print(f"\n=== Data Split ===")
    print(f"Train: 0 to {train_end_idx} ({train_end_idx} samples)")
    print(f"Val: {train_end_idx} to {val_end_idx} ({val_end_idx - train_end_idx} samples)")
    print(f"Test: {val_end_idx} to {n_total} ({n_total - val_end_idx} samples)")

    # === Run Optuna HPO ===
    print("\n=== Running Optuna HPO ===")
    study = optuna.create_study(
        direction='maximize',
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),
        sampler=optuna.samplers.TPESampler(seed=42)
    )

    study.optimize(
        lambda trial: objective(
            trial,
            dxy_returns,
            currency_returns,
            aligned_target,
            train_end_idx,
            val_end_idx
        ),
        n_trials=30,
        timeout=300,  # 5 minutes
        show_progress_bar=True
    )

    best_params = study.best_params
    best_value = study.best_value
    n_completed = len(study.trials)

    print(f"\n=== Optuna Results ===")
    print(f"Best params: {best_params}")
    print(f"Best MI sum: {best_value:.6f}")
    print(f"Trials completed: {n_completed}")

    # === Generate final output with best params ===
    print("\n=== Generating Final Output ===")
    final_features = generate_all_features(dxy_returns, currency_returns, best_params)

    # Forward-fill NaN values in the warmup period
    final_features = final_features.ffill().bfill()

    # Trim to match base_features date range (2015-01-30 to 2025-02-12)
    target_start = pd.Timestamp('2015-01-30')
    target_end = pd.Timestamp('2025-02-12')
    final_features = final_features.loc[
        (final_features.index >= target_start) & (final_features.index <= target_end)
    ]

    print(f"Output shape: {final_features.shape}")
    print(f"Output date range: {final_features.index[0]} to {final_features.index[-1]}")
    print(f"Output columns: {list(final_features.columns)}")

    # === Quality checks ===
    print("\n=== Output Quality Checks ===")
    for col in final_features.columns:
        col_data = final_features[col]
        print(f"{col}:")
        print(f"  Mean: {col_data.mean():.6f}, Std: {col_data.std():.6f}")
        print(f"  Min: {col_data.min():.6f}, Max: {col_data.max():.6f}")
        print(f"  NaN count: {col_data.isna().sum()}")

        # Check for constant values
        if col_data.std() < 0.01:
            print(f"  WARNING: {col} has very low variance (std={col_data.std():.6f})")

    # === Save results ===
    print("\n=== Saving Results ===")

    # Save submodel output
    final_features.to_csv("submodel_output.csv")
    print("Saved: submodel_output.csv")

    # Save best params
    with open("model_params.json", "w") as f:
        json.dump(best_params, f, indent=2)
    print("Saved: model_params.json")

    # Save training result summary
    result = {
        "feature": "dxy",
        "attempt": 1,
        "timestamp": datetime.now().isoformat(),
        "best_params": best_params,
        "optuna_trials_completed": n_completed,
        "optuna_best_value": float(best_value),
        "output_shape": list(final_features.shape),
        "output_columns": list(final_features.columns),
        "data_info": {
            "total_samples": n_total,
            "train_samples": train_end_idx,
            "val_samples": val_end_idx - train_end_idx,
            "test_samples": n_total - val_end_idx,
            "output_date_range": {
                "start": str(final_features.index[0]),
                "end": str(final_features.index[-1])
            }
        },
        "output_statistics": {
            col: {
                "mean": float(final_features[col].mean()),
                "std": float(final_features[col].std()),
                "min": float(final_features[col].min()),
                "max": float(final_features[col].max()),
                "nan_count": int(final_features[col].isna().sum())
            }
            for col in final_features.columns
        }
    }

    with open("training_result.json", "w") as f:
        json.dump(result, f, indent=2)
    print("Saved: training_result.json")

    print(f"\n=== Training Complete! ===")
    print(f"Finished: {datetime.now().isoformat()}")
