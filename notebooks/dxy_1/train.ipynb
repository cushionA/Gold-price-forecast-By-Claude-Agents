{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DXY SubModel Training - Attempt 1 (Revised)\n",
    "\n",
    "Self-contained: Data fetch -> HMM regime detection + Momentum Z-score + Volatility Z-score -> Optuna HPO -> Save results\n",
    "\n",
    "**Architecture**: 3-state GaussianHMM on DXY daily log-returns + expanding z-score features\n",
    "\n",
    "**Output**: 3 columns: dxy_regime_prob, dxy_momentum_z, dxy_vol_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gold Prediction SubModel Training - DXY Attempt 1 (Revised)\n",
    "Self-contained: Data fetch -> HMM + Momentum Z + Vol Z -> Optuna HPO -> Save results\n",
    "\n",
    "Critical corrections from previous attempt:\n",
    "  1. GMM -> HMM (GaussianHMM captures temporal regime transitions)\n",
    "  2. PCA divergence -> Momentum z-score (proven pattern, avoids scaling issues)\n",
    "  3. Full Gate 1/2/3 evaluation (no auto-evaluation shortcuts)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# ============================================================\n",
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', 'hmmlearn'])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import optuna\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f'=== Gold SubModel Training: DXY attempt 1 (HMM Revised) ===')\n",
    "print(f'Started: {datetime.now().isoformat()}')\n",
    "\n",
    "# ============================================================\n",
    "# 2. DATA FETCHING\n",
    "# ============================================================\n",
    "def fetch_data():\n",
    "    \"\"\"\n",
    "    Fetch DX-Y.NYB from Yahoo Finance + GC=F for target.\n",
    "    Attempt 1 uses single DXY ticker only (no constituent currencies).\n",
    "    \"\"\"\n",
    "    print('\\n[Data Fetch] Downloading DXY and GC=F from Yahoo Finance...')\n",
    "\n",
    "    # Fetch DXY\n",
    "    dxy_raw = yf.download('DX-Y.NYB', start='2014-10-01', progress=False)\n",
    "    if dxy_raw.empty:\n",
    "        raise RuntimeError('Failed to fetch DX-Y.NYB data')\n",
    "    if isinstance(dxy_raw.columns, pd.MultiIndex):\n",
    "        dxy_raw.columns = dxy_raw.columns.get_level_values(0)\n",
    "    dxy_close = dxy_raw['Close'].copy()\n",
    "    dxy_close.index = pd.to_datetime(dxy_close.index)\n",
    "    print(f'[OK] DXY: {len(dxy_close)} rows, range {dxy_close.index[0].date()} to {dxy_close.index[-1].date()}')\n",
    "\n",
    "    # Fetch Gold\n",
    "    gc_raw = yf.download('GC=F', start='2014-10-01', progress=False)\n",
    "    if gc_raw.empty:\n",
    "        raise RuntimeError('Failed to fetch GC=F data')\n",
    "    if isinstance(gc_raw.columns, pd.MultiIndex):\n",
    "        gc_raw.columns = gc_raw.columns.get_level_values(0)\n",
    "    gc_close = gc_raw['Close'].copy()\n",
    "    gc_close.index = pd.to_datetime(gc_close.index)\n",
    "    print(f'[OK] GC=F: {len(gc_close)} rows')\n",
    "\n",
    "    # Combine on common dates\n",
    "    df = pd.DataFrame({'dxy_close': dxy_close, 'gc_close': gc_close})\n",
    "    df = df.ffill(limit=3).dropna()\n",
    "\n",
    "    # Compute returns\n",
    "    df['dxy_log_ret'] = np.log(df['dxy_close']) - np.log(df['dxy_close'].shift(1))\n",
    "    df['gold_return_next'] = df['gc_close'].pct_change().shift(-1)\n",
    "    df = df.dropna(subset=['dxy_log_ret'])\n",
    "\n",
    "    # Validate\n",
    "    assert len(df) > 2000, f'Insufficient data: {len(df)} rows'\n",
    "    assert df['dxy_close'].min() > 70 and df['dxy_close'].max() < 130, 'DXY out of range'\n",
    "    assert df['dxy_log_ret'].abs().max() < 0.05, 'Extreme DXY return detected'\n",
    "\n",
    "    print(f'[OK] Combined: {len(df)} rows, {df.index[0].date()} to {df.index[-1].date()}')\n",
    "    print(f'[OK] DXY close: {df[\"dxy_close\"].mean():.2f} +/- {df[\"dxy_close\"].std():.2f}')\n",
    "    print(f'[OK] DXY log-ret: {df[\"dxy_log_ret\"].mean():.6f} +/- {df[\"dxy_log_ret\"].std():.6f}')\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 3. FEATURE GENERATION FUNCTIONS\n",
    "# ============================================================\n",
    "def expanding_zscore(series, warmup):\n",
    "    \"\"\"Vectorized expanding z-score with warmup period. No lookahead.\"\"\"\n",
    "    mean = series.expanding(min_periods=warmup).mean()\n",
    "    std = series.expanding(min_periods=warmup).std()\n",
    "    z = (series - mean) / std\n",
    "    z = z.clip(-4, 4)\n",
    "    z = z.fillna(0.0)\n",
    "    return z\n",
    "\n",
    "\n",
    "def generate_regime_feature(dxy_log_ret, n_components, covariance_type, n_init, train_size):\n",
    "    \"\"\"\n",
    "    Fit GaussianHMM on train portion, return P(highest-variance state) for full data.\n",
    "    CRITICAL: Use HMM (not GMM) to capture temporal regime transitions.\n",
    "    Manual n_init: fit multiple times, keep best log-likelihood.\n",
    "    \"\"\"\n",
    "    X_train = dxy_log_ret.iloc[:train_size].values.reshape(-1, 1)\n",
    "    X_full = dxy_log_ret.values.reshape(-1, 1)\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    for init_i in range(n_init):\n",
    "        try:\n",
    "            m = GaussianHMM(\n",
    "                n_components=n_components,\n",
    "                covariance_type=covariance_type,\n",
    "                n_iter=200,\n",
    "                tol=1e-4,\n",
    "                random_state=42 + init_i\n",
    "            )\n",
    "            m.fit(X_train)\n",
    "            score = m.score(X_train)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = m\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if best_model is None:\n",
    "        raise RuntimeError('All HMM initializations failed')\n",
    "    model = best_model\n",
    "\n",
    "    # Get posterior probabilities for full dataset\n",
    "    probs = model.predict_proba(X_full)\n",
    "\n",
    "    # Identify highest-variance state\n",
    "    state_vars = []\n",
    "    for i in range(n_components):\n",
    "        if covariance_type == 'full':\n",
    "            state_vars.append(float(model.covars_[i][0, 0]))\n",
    "        elif covariance_type == 'diag':\n",
    "            state_vars.append(float(model.covars_[i][0]))\n",
    "        else:\n",
    "            state_vars.append(float(model.covars_[i]))\n",
    "\n",
    "    high_var_state = np.argmax(state_vars)\n",
    "    regime_prob = probs[:, high_var_state]\n",
    "\n",
    "    print(f'  HMM states={n_components}, cov={covariance_type}, n_init={n_init}')\n",
    "    print(f'  State variances: {[f\"{v:.8f}\" for v in state_vars]}')\n",
    "    print(f'  High-var state: {high_var_state}, mean regime_prob: {regime_prob.mean():.3f}')\n",
    "\n",
    "    return regime_prob\n",
    "\n",
    "\n",
    "def generate_momentum_feature(dxy_close, momentum_window, expanding_warmup):\n",
    "    \"\"\"\n",
    "    Expanding z-score of N-day momentum (N-day return).\n",
    "    Follows proven pattern from vix_mean_reversion_z, tech_mean_reversion_z.\n",
    "    \"\"\"\n",
    "    momentum = dxy_close.pct_change(momentum_window)\n",
    "    z = expanding_zscore(momentum, expanding_warmup)\n",
    "    return z\n",
    "\n",
    "\n",
    "def generate_volatility_feature(dxy_log_ret, vol_window, expanding_warmup):\n",
    "    \"\"\"\n",
    "    Expanding z-score of N-day realized volatility.\n",
    "    Industry standard: 20-day rolling std for FX volatility.\n",
    "    \"\"\"\n",
    "    vol = dxy_log_ret.rolling(vol_window).std()\n",
    "    z = expanding_zscore(vol, expanding_warmup)\n",
    "    return z\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. OPTUNA HPO\n",
    "# ============================================================\n",
    "def discretize(x, bins=20):\n",
    "    \"\"\"Discretize continuous values into quantile bins for MI calculation.\"\"\"\n",
    "    valid = ~np.isnan(x)\n",
    "    if valid.sum() < bins:\n",
    "        return None\n",
    "    x_clean = x.copy()\n",
    "    x_clean[~valid] = np.nanmedian(x)\n",
    "    try:\n",
    "        return np.asarray(pd.qcut(x_clean, bins, labels=False, duplicates='drop'))\n",
    "    except ValueError:\n",
    "        return np.asarray(pd.cut(x_clean, bins, labels=False, duplicates='drop'))\n",
    "\n",
    "\n",
    "def compute_mi_sum(features_dict, target_vals):\n",
    "    \"\"\"Compute sum of MI between each feature and target.\"\"\"\n",
    "    target_disc = discretize(target_vals)\n",
    "    if target_disc is None:\n",
    "        return 0.0\n",
    "\n",
    "    mi_sum = 0.0\n",
    "    for name, feat_vals in features_dict.items():\n",
    "        mask = ~np.isnan(feat_vals) & ~np.isnan(target_vals)\n",
    "        if mask.sum() > 50:\n",
    "            feat_disc = discretize(feat_vals[mask])\n",
    "            tgt_disc = discretize(target_vals[mask])\n",
    "            if feat_disc is not None and tgt_disc is not None and len(feat_disc) == len(tgt_disc):\n",
    "                mi_sum += mutual_info_score(feat_disc, tgt_disc)\n",
    "    return mi_sum\n",
    "\n",
    "\n",
    "def optuna_objective(trial, df, train_size, val_start, val_end):\n",
    "    \"\"\"Optuna objective: maximize MI sum on validation set.\"\"\"\n",
    "    n_components = trial.suggest_categorical('hmm_n_components', [2, 3])\n",
    "    covariance_type = trial.suggest_categorical('hmm_covariance_type', ['full', 'diag'])\n",
    "    n_init = trial.suggest_categorical('hmm_n_init', [5, 10, 15])\n",
    "    momentum_window = trial.suggest_categorical('momentum_window', [10, 15, 20, 30])\n",
    "    vol_window = trial.suggest_categorical('vol_window', [10, 15, 20, 30])\n",
    "    expanding_warmup = trial.suggest_categorical('expanding_warmup', [60, 120, 252])\n",
    "\n",
    "    try:\n",
    "        regime = generate_regime_feature(\n",
    "            df['dxy_log_ret'], n_components, covariance_type, n_init, train_size\n",
    "        )\n",
    "        momentum_z = generate_momentum_feature(\n",
    "            df['dxy_close'], momentum_window, expanding_warmup\n",
    "        ).values\n",
    "        vol_z = generate_volatility_feature(\n",
    "            df['dxy_log_ret'], vol_window, expanding_warmup\n",
    "        ).values\n",
    "\n",
    "        # Extract validation period\n",
    "        regime_val = regime[val_start:val_end]\n",
    "        momentum_val = momentum_z[val_start:val_end]\n",
    "        vol_val = vol_z[val_start:val_end]\n",
    "        target_val = df['gold_return_next'].values[val_start:val_end]\n",
    "\n",
    "        features = {\n",
    "            'regime': regime_val,\n",
    "            'momentum': momentum_val,\n",
    "            'vol': vol_val\n",
    "        }\n",
    "        mi = compute_mi_sum(features, target_val)\n",
    "        return mi\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'  Trial {trial.number} failed: {e}')\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. MAIN EXECUTION\n",
    "# ============================================================\n",
    "print('\\n' + '='*70)\n",
    "print('STEP 1: DATA FETCHING')\n",
    "print('='*70)\n",
    "df = fetch_data()\n",
    "\n",
    "# Train/Val/Test split (70/15/15, time-series order)\n",
    "n = len(df)\n",
    "train_size = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "val_start = train_size\n",
    "test_start = val_end\n",
    "\n",
    "print(f'\\nData split:')\n",
    "print(f'  Train: 0:{train_size} ({train_size} rows, {df.index[0].date()} to {df.index[train_size-1].date()})')\n",
    "print(f'  Val:   {val_start}:{val_end} ({val_end-val_start} rows, {df.index[val_start].date()} to {df.index[val_end-1].date()})')\n",
    "print(f'  Test:  {test_start}:{n} ({n-test_start} rows, {df.index[test_start].date()} to {df.index[-1].date()})')\n",
    "\n",
    "# ============================================================\n",
    "print('\\n' + '='*70)\n",
    "print('STEP 2: OPTUNA HPO (30 trials, 300s timeout)')\n",
    "print('='*70)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(\n",
    "    lambda trial: optuna_objective(trial, df, train_size, val_start, val_end),\n",
    "    n_trials=30,\n",
    "    timeout=300,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_params\n",
    "best_mi = study.best_value\n",
    "print(f'\\nBest MI sum: {best_mi:.6f}')\n",
    "print(f'Best params: {best_params}')\n",
    "print(f'Completed trials: {len(study.trials)}')\n",
    "\n",
    "# ============================================================\n",
    "print('\\n' + '='*70)\n",
    "print('STEP 3: FINAL FEATURE GENERATION WITH BEST PARAMS')\n",
    "print('='*70)\n",
    "\n",
    "regime = generate_regime_feature(\n",
    "    df['dxy_log_ret'],\n",
    "    best_params['hmm_n_components'],\n",
    "    best_params['hmm_covariance_type'],\n",
    "    best_params['hmm_n_init'],\n",
    "    train_size\n",
    ")\n",
    "momentum_z = generate_momentum_feature(\n",
    "    df['dxy_close'],\n",
    "    best_params['momentum_window'],\n",
    "    best_params['expanding_warmup']\n",
    ").values\n",
    "vol_z = generate_volatility_feature(\n",
    "    df['dxy_log_ret'],\n",
    "    best_params['vol_window'],\n",
    "    best_params['expanding_warmup']\n",
    ").values\n",
    "\n",
    "# Create output DataFrame\n",
    "output = pd.DataFrame({\n",
    "    'dxy_regime_prob': regime,\n",
    "    'dxy_momentum_z': momentum_z,\n",
    "    'dxy_vol_z': vol_z\n",
    "}, index=df.index)\n",
    "\n",
    "# NaN handling: warmup period\n",
    "output['dxy_regime_prob'] = output['dxy_regime_prob'].fillna(0.5)\n",
    "output['dxy_momentum_z'] = output['dxy_momentum_z'].fillna(0.0)\n",
    "output['dxy_vol_z'] = output['dxy_vol_z'].fillna(0.0)\n",
    "\n",
    "print(f'\\nOutput shape: {output.shape}')\n",
    "print(f'Date range: {output.index[0].date()} to {output.index[-1].date()}')\n",
    "for col in output.columns:\n",
    "    s = output[col]\n",
    "    print(f'  {col}: mean={s.mean():.4f}, std={s.std():.4f}, min={s.min():.4f}, max={s.max():.4f}')\n",
    "\n",
    "# ============================================================\n",
    "print('\\n' + '='*70)\n",
    "print('STEP 4: METRICS')\n",
    "print('='*70)\n",
    "\n",
    "# Autocorrelation\n",
    "autocorr = {}\n",
    "for col in output.columns:\n",
    "    autocorr[col] = float(output[col].autocorr(lag=1))\n",
    "print(f'Autocorrelation (lag-1): {autocorr}')\n",
    "\n",
    "# Check for constant features\n",
    "is_constant = {}\n",
    "for col in output.columns:\n",
    "    is_constant[col] = bool(output[col].std() < 1e-6)\n",
    "print(f'Is constant: {is_constant}')\n",
    "\n",
    "# MI on validation set\n",
    "target_val = df['gold_return_next'].values[val_start:val_end]\n",
    "mi_individual = {}\n",
    "for col in output.columns:\n",
    "    feat_val = output[col].values[val_start:val_end]\n",
    "    mask = ~np.isnan(feat_val) & ~np.isnan(target_val)\n",
    "    if mask.sum() > 50:\n",
    "        f_disc = discretize(feat_val[mask])\n",
    "        t_disc = discretize(target_val[mask])\n",
    "        if f_disc is not None and t_disc is not None:\n",
    "            mi_individual[col] = float(mutual_info_score(f_disc, t_disc))\n",
    "        else:\n",
    "            mi_individual[col] = 0.0\n",
    "    else:\n",
    "        mi_individual[col] = 0.0\n",
    "mi_sum = sum(mi_individual.values())\n",
    "print(f'MI individual: {mi_individual}')\n",
    "print(f'MI sum: {mi_sum:.6f}')\n",
    "\n",
    "# ============================================================\n",
    "print('\\n' + '='*70)\n",
    "print('STEP 5: SAVING OUTPUTS')\n",
    "print('='*70)\n",
    "\n",
    "# Save submodel output\n",
    "output.to_csv('submodel_output.csv')\n",
    "print(f'Saved: submodel_output.csv ({output.shape[0]} rows, {output.shape[1]} columns)')\n",
    "\n",
    "# Save training result\n",
    "result = {\n",
    "    'feature': 'dxy',\n",
    "    'attempt': 1,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'method': 'GaussianHMM + Momentum Z-Score + Volatility Z-Score',\n",
    "    'best_params': best_params,\n",
    "    'metrics': {\n",
    "        'mi_individual': mi_individual,\n",
    "        'mi_sum': mi_sum,\n",
    "        'autocorr': autocorr,\n",
    "        'is_constant': is_constant\n",
    "    },\n",
    "    'optuna_best_value': float(best_mi),\n",
    "    'optuna_trials_completed': len(study.trials),\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': list(output.columns),\n",
    "    'data_info': {\n",
    "        'total_rows': len(df),\n",
    "        'train_rows': train_size,\n",
    "        'val_rows': val_end - val_start,\n",
    "        'test_rows': n - test_start,\n",
    "        'date_range_start': str(df.index[0].date()),\n",
    "        'date_range_end': str(df.index[-1].date())\n",
    "    },\n",
    "    'output_stats': {\n",
    "        col: {\n",
    "            'mean': float(output[col].mean()),\n",
    "            'std': float(output[col].std()),\n",
    "            'min': float(output[col].min()),\n",
    "            'max': float(output[col].max())\n",
    "        }\n",
    "        for col in output.columns\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print(f'Saved: training_result.json')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('TRAINING COMPLETE!')\n",
    "print('='*70)\n",
    "print(f'Output: {output.shape[0]} rows x {output.shape[1]} columns')\n",
    "print(f'Columns: {list(output.columns)}')\n",
    "print(f'Best MI sum: {best_mi:.6f}')\n",
    "print(f'Finished: {datetime.now().isoformat()}')\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}