{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Gold DOWN Classifier - Attempt 2\n\n**Architecture**: XGBoost binary:logistic (MCC-optimized, no focal loss)  \n**Features**: 17 features (dropped rate_surprise unsigned; kept rate_surprise_signed)  \n**Purpose**: Detect DOWN days to ensemble with regression meta-model (attempt 7)  \n\n## Changes from Attempt 1\n1. **Optuna objective**: MCC (Matthews Correlation Coefficient) replaces F1_DOWN composite — MCC=0 for trivial predictors\n2. **Trivial prediction guard**: Reject trials with minority_pct < 15% or pred_std < 0.03\n3. **Relaxed HP bounds**: learning_rate [0.01, 0.15], reg_lambda [0.1, 3.0], n_estimators [200, 800]\n4. **Early stopping on AUC**: Not logloss (prevents convergence to constant output)\n5. **scale_pos_weight [0.9, 2.0]**: Prevents DOWN over-weighting\n6. **Dropped rate_surprise**: Subsumed by rate_surprise_signed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Imports and Setup\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Install required packages\nimport subprocess\ntry:\n    import xgboost as xgb\nexcept ImportError:\n    subprocess.run([\"pip\", \"install\", \"xgboost\"], check=True)\n    import xgboost as xgb\n\ntry:\n    import optuna\nexcept ImportError:\n    subprocess.run([\"pip\", \"install\", \"optuna\"], check=True)\n    import optuna\n\nfrom sklearn.metrics import (\n    balanced_accuracy_score, recall_score, precision_score, \n    f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef\n)\nimport matplotlib.pyplot as plt\n\n# Configuration\nRANDOM_SEED = 42\nN_TRIALS = 100\nTIMEOUT = 3600  # 1 hour\nTRAIN_RATIO = 0.70\nVAL_RATIO = 0.15\n\n# Set random seeds\nnp.random.seed(RANDOM_SEED)\n\nprint(\"=\"*80)\nprint(\"Gold DOWN Classifier - Attempt 2 (MCC-optimized)\")\nprint(\"=\"*80)\nprint(f\"Start time: {datetime.now().isoformat()}\")\nprint(f\"Configuration: {N_TRIALS} trials, seed={RANDOM_SEED}\")\nprint(f\"Key changes: MCC objective, trivial-prediction guard, relaxed regularization\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Data Fetching and Preprocessing\n# This embeds the complete data fetching function from src/fetch_classifier.py\n# Attempt 2: Dropped rate_surprise (unsigned) — subsumed by rate_surprise_signed\n\ndef fetch_and_preprocess():\n    \"\"\"\n    Fetch all data sources and compute 17 classifier features.\n    Returns: train_df, val_df, test_df, full_df (each with 17 features + target)\n    \"\"\"\n    \n    # Import libraries\n    try:\n        import yfinance as yf\n        from fredapi import Fred\n    except ImportError:\n        import subprocess\n        subprocess.run([\"pip\", \"install\", \"yfinance\", \"fredapi\"], check=True)\n        import yfinance as yf\n        from fredapi import Fred\n    \n    # Get FRED API key (Kaggle Secrets with hardcoded fallback)\n    api_key = os.environ.get('FRED_API_KEY')\n    if api_key is None:\n        # Hardcoded fallback for Kaggle\n        api_key = \"3ffb68facdf6321e180e380c00e909c8\"\n        print(\"WARNING: Using hardcoded FRED_API_KEY\")\n    \n    fred = Fred(api_key=api_key)\n    \n    # Fetch raw data\n    START_DATE = \"2014-01-01\"  # 1 year extra for warmup\n    \n    print(\"Fetching yfinance data...\")\n    \n    # GC=F: Gold futures (OHLCV)\n    gc = yf.download('GC=F', start=START_DATE, progress=False, auto_adjust=True)\n    if isinstance(gc.columns, pd.MultiIndex):\n        gc.columns = [col[0] for col in gc.columns]\n    gc = gc[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n    gc.columns = [f'GC_{col}' for col in gc.columns]\n    \n    # GLD: Gold ETF (for volume features)\n    gld = yf.download('GLD', start=START_DATE, progress=False, auto_adjust=True)\n    if isinstance(gld.columns, pd.MultiIndex):\n        gld.columns = [col[0] for col in gld.columns]\n    gld = gld[['Volume']].copy()\n    gld.columns = ['GLD_Volume']\n    \n    # SI=F: Silver futures\n    si = yf.download('SI=F', start=START_DATE, progress=False, auto_adjust=True)\n    if isinstance(si.columns, pd.MultiIndex):\n        si.columns = [col[0] for col in si.columns]\n    si = si[['Close']].copy()\n    si.columns = ['SI_Close']\n    \n    # HG=F: Copper futures\n    hg = yf.download('HG=F', start=START_DATE, progress=False, auto_adjust=True)\n    if isinstance(hg.columns, pd.MultiIndex):\n        hg.columns = [col[0] for col in hg.columns]\n    hg = hg[['Close']].copy()\n    hg.columns = ['HG_Close']\n    \n    # DX-Y.NYB: Dollar Index\n    dxy = yf.download('DX-Y.NYB', start=START_DATE, progress=False, auto_adjust=True)\n    if isinstance(dxy.columns, pd.MultiIndex):\n        dxy.columns = [col[0] for col in dxy.columns]\n    dxy = dxy[['Close']].copy()\n    dxy.columns = ['DXY_Close']\n    \n    # ^GSPC: S&P 500\n    spx = yf.download('^GSPC', start=START_DATE, progress=False, auto_adjust=True)\n    if isinstance(spx.columns, pd.MultiIndex):\n        spx.columns = [col[0] for col in spx.columns]\n    spx = spx[['Close']].copy()\n    spx.columns = ['SPX_Close']\n    \n    print(\"Fetching FRED data...\")\n    \n    # FRED series\n    gvz = pd.DataFrame({'GVZ': fred.get_series('GVZCLS', observation_start=START_DATE)})\n    vix = pd.DataFrame({'VIX': fred.get_series('VIXCLS', observation_start=START_DATE)})\n    dfii10 = pd.DataFrame({'DFII10': fred.get_series('DFII10', observation_start=START_DATE)})\n    dgs10 = pd.DataFrame({'DGS10': fred.get_series('DGS10', observation_start=START_DATE)})\n    \n    # Convert index to datetime\n    for df in [gvz, vix, dfii10, dgs10]:\n        df.index = pd.to_datetime(df.index)\n    \n    # Merge all data\n    print(\"Merging data sources...\")\n    \n    df = gc.copy()\n    for data in [gld, si, hg, dxy, spx, gvz, vix, dfii10, dgs10]:\n        df = df.join(data, how='left')\n    \n    # Forward-fill missing values\n    fred_cols = ['GVZ', 'VIX', 'DFII10', 'DGS10']\n    for col in fred_cols:\n        df[col] = df[col].ffill(limit=5)\n    \n    yf_cols = [col for col in df.columns if col not in fred_cols]\n    for col in yf_cols:\n        df[col] = df[col].ffill(limit=3)\n    \n    # Drop any remaining NaN rows\n    df = df.dropna()\n    \n    print(f\"Merged data: {len(df)} rows, {len(df.columns)} columns\")\n    \n    # Helper functions\n    def rolling_zscore(series, window=60):\n        mean = series.rolling(window).mean()\n        std = series.rolling(window).std()\n        return (series - mean) / std.clip(lower=1e-8)\n    \n    def rolling_beta(y, x, window=20):\n        cov = y.rolling(window).cov(x)\n        var = x.rolling(window).var()\n        return cov / var.clip(lower=1e-8)\n    \n    # Compute features\n    print(\"Computing features...\")\n    \n    # Gold return (for target and some features)\n    df['gold_return'] = df['GC_Close'].pct_change() * 100\n    \n    # Category A: Volatility Regime Features (5)\n    rv_10 = df['gold_return'].rolling(10).std()\n    rv_30 = df['gold_return'].rolling(30).std()\n    df['rv_ratio_10_30'] = rv_10 / rv_30.clip(lower=1e-8)\n    df['rv_ratio_10_30_z'] = rolling_zscore(df['rv_ratio_10_30'], 60)\n    df['gvz_level_z'] = rolling_zscore(df['GVZ'], 60)\n    df['gvz_vix_ratio'] = df['GVZ'] / df['VIX'].clip(lower=1e-8)\n    \n    daily_range = (df['GC_High'] - df['GC_Low']) / df['GC_Close'].clip(lower=1e-8)\n    avg_range = daily_range.rolling(20).mean()\n    df['intraday_range_ratio'] = daily_range / avg_range.clip(lower=1e-8)\n    \n    # Category B: Cross-Asset Stress Features (4)\n    vix_change = df['VIX'].pct_change() * 100\n    dxy_change = df['DXY_Close'].pct_change() * 100\n    spx_return = df['SPX_Close'].pct_change() * 100\n    yield_change = df['DGS10'].diff()\n    \n    vix_z = rolling_zscore(vix_change, 20)\n    dxy_z = rolling_zscore(dxy_change, 20)\n    spx_z = rolling_zscore(spx_return, 20)\n    yield_z = rolling_zscore(yield_change, 20)\n    \n    df['risk_off_score'] = vix_z + dxy_z - spx_z - yield_z\n    \n    gold_5d_ret = df['GC_Close'].pct_change(5) * 100\n    silver_5d_ret = df['SI_Close'].pct_change(5) * 100\n    divergence = gold_5d_ret - silver_5d_ret\n    df['gold_silver_ratio_change'] = rolling_zscore(divergence, 60)\n    \n    df['equity_gold_beta_20d'] = rolling_beta(df['gold_return'], spx_return, 20)\n    \n    copper_5d_ret = df['HG_Close'].pct_change(5) * 100\n    divergence_copper = gold_5d_ret - copper_5d_ret\n    df['gold_copper_ratio_change'] = rolling_zscore(divergence_copper, 60)\n    \n    # Category C: Rate and Currency Shock Features (2) — dropped rate_surprise unsigned\n    rate_change = df['DFII10'].diff()\n    rate_std_20 = rate_change.rolling(20).std()\n    rate_surprise_mag = np.abs(rate_change) / rate_std_20.clip(lower=1e-8)\n    df['rate_surprise_signed'] = np.sign(rate_change) * rate_surprise_mag\n    \n    dxy_accel = dxy_change - dxy_change.shift(1)\n    df['dxy_acceleration'] = rolling_zscore(dxy_accel, 20)\n    \n    # Category D: Volume and Flow Features (2)\n    df['gld_volume_z'] = rolling_zscore(df['GLD_Volume'], 20)\n    df['volume_return_sign'] = np.sign(df['gold_return']) * df['gld_volume_z']\n    \n    # Category E: Momentum Context Features (2)\n    ret_5d = df['GC_Close'].pct_change(5) * 100\n    ret_20d = df['GC_Close'].pct_change(20) * 100\n    mom_div = ret_5d - ret_20d\n    df['momentum_divergence'] = rolling_zscore(mom_div, 60)\n    \n    high_20d = df['GC_Close'].rolling(20).max()\n    low_20d = df['GC_Close'].rolling(20).min()\n    range_20d = (high_20d - low_20d).clip(lower=1e-8)\n    df['distance_from_20d_high'] = (df['GC_Close'] - high_20d) / range_20d\n    \n    # Category F: Calendar Features (2)\n    df['day_of_week'] = df.index.dayofweek\n    df['month_of_year'] = df.index.month\n    \n    # Create target variable\n    df['target'] = (df['gold_return'].shift(-1) > 0).astype(int)\n    \n    # Select features and drop warmup rows\n    # 17 features (dropped rate_surprise unsigned from attempt 1's 18)\n    feature_cols = [\n        'rv_ratio_10_30', 'rv_ratio_10_30_z', 'gvz_level_z', 'gvz_vix_ratio',\n        'intraday_range_ratio', 'risk_off_score', 'gold_silver_ratio_change',\n        'equity_gold_beta_20d', 'gold_copper_ratio_change',\n        'rate_surprise_signed', 'dxy_acceleration', 'gld_volume_z',\n        'volume_return_sign', 'momentum_divergence', 'distance_from_20d_high',\n        'day_of_week', 'month_of_year'\n    ]\n    \n    df_final = df[feature_cols + ['target', 'gold_return']].copy()\n    df_final = df_final.dropna()\n    \n    print(f\"Final dataset: {len(df_final)} rows, {len(feature_cols)} features\")\n    \n    # Verify target balance\n    up_pct = 100 * (df_final['target'] == 1).sum() / len(df_final)\n    down_pct = 100 * (df_final['target'] == 0).sum() / len(df_final)\n    print(f\"Target balance: UP={up_pct:.2f}%, DOWN={down_pct:.2f}%\")\n    \n    # Train/val/test split (70/15/15)\n    n = len(df_final)\n    train_end = int(n * 0.70)\n    val_end = int(n * 0.85)\n    \n    train_df = df_final.iloc[:train_end].copy()\n    val_df = df_final.iloc[train_end:val_end].copy()\n    test_df = df_final.iloc[val_end:].copy()\n    \n    print(f\"Train: {len(train_df)} rows ({train_df.index.min()} to {train_df.index.max()})\")\n    print(f\"Val:   {len(val_df)} rows ({val_df.index.min()} to {val_df.index.max()})\")\n    print(f\"Test:  {len(test_df)} rows ({test_df.index.min()} to {test_df.index.max()})\")\n    \n    return train_df, val_df, test_df, df_final, feature_cols\n\n# Execute data fetching\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA FETCHING\")\nprint(\"=\"*80)\ntrain_df, val_df, test_df, full_df, feature_cols = fetch_and_preprocess()\nprint(\"\\nData fetching complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for NaN\n",
    "print(\"\\nChecking for NaN values...\")\n",
    "nan_counts = full_df[feature_cols].isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"WARNING: NaN values found:\")\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "else:\n",
    "    print(\"✓ No NaN values\")\n",
    "\n",
    "# Feature statistics\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(full_df[feature_cols].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# Check feature correlations\n",
    "print(\"\\nChecking feature correlations...\")\n",
    "corr_matrix = full_df[feature_cols].corr()\n",
    "high_corr = []\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.85:\n",
    "            high_corr.append((feature_cols[i], feature_cols[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"WARNING: High correlations found:\")\n",
    "    for f1, f2, corr in high_corr:\n",
    "        print(f\"  {f1} <-> {f2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"✓ No feature pairs with correlation > 0.85\")\n",
    "\n",
    "# Class balance per split\n",
    "print(\"\\nClass balance per split:\")\n",
    "for name, df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    up_pct = 100 * (df['target'] == 1).sum() / len(df)\n",
    "    down_pct = 100 * (df['target'] == 0).sum() / len(df)\n",
    "    print(f\"  {name}: UP={up_pct:.2f}%, DOWN={down_pct:.2f}%\")\n",
    "\n",
    "print(\"\\nData validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Focal Loss — NOT USED in Attempt 2\n# Kept for reference. Attempt 2 uses binary:logistic only (MCC objective doesn't need focal loss).\n# The focal loss objective was one source of complexity in attempt 1.\n\nprint(\"Note: Focal loss disabled in Attempt 2. Using binary:logistic only.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Optuna HPO — MCC-based objective with trivial-prediction guard\n# KEY CHANGES from Attempt 1:\n# 1. Objective = MCC (Matthews Correlation Coefficient) — equals 0 for trivial predictors\n# 2. Trivial prediction guard: reject if minority_pct < 15% or pred_std < 0.03\n# 3. Relaxed regularization: learning_rate [0.01, 0.15], reg_lambda [0.1, 3.0]\n# 4. Early stopping on AUC (not logloss)\n# 5. scale_pos_weight [0.9, 2.0]\n# 6. No focal loss (binary:logistic only)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"HYPERPARAMETER OPTIMIZATION (MCC-based)\")\nprint(\"=\"*80)\n\n# Prepare data\nX_train = train_df[feature_cols].values\ny_train = train_df['target'].values\nX_val = val_df[feature_cols].values\ny_val = val_df['target'].values\n\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_cols)\ndval = xgb.DMatrix(X_val, label=y_val, feature_names=feature_cols)\n\ndef objective(trial):\n    \"\"\"\n    Optuna objective function using MCC.\n    MCC = 0 for any trivial predictor (all-UP, all-DOWN, random).\n    MCC ranges from -1 to +1 (perfect anti-correlation to perfect correlation).\n    \"\"\"\n    # Suggest hyperparameters — RELAXED bounds per evaluator\n    max_depth = trial.suggest_int('max_depth', 2, 5)\n    min_child_weight = trial.suggest_int('min_child_weight', 3, 15)\n    subsample = trial.suggest_float('subsample', 0.5, 0.95)\n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 0.95)\n    reg_lambda = trial.suggest_float('reg_lambda', 0.1, 3.0, log=True)\n    reg_alpha = trial.suggest_float('reg_alpha', 0.001, 1.0, log=True)\n    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.15, log=True)\n    n_estimators = trial.suggest_int('n_estimators', 200, 800, step=50)\n    scale_pos_weight = trial.suggest_float('scale_pos_weight', 0.9, 2.0)\n    \n    params = {\n        'objective': 'binary:logistic',\n        'max_depth': max_depth,\n        'min_child_weight': min_child_weight,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'reg_lambda': reg_lambda,\n        'reg_alpha': reg_alpha,\n        'learning_rate': learning_rate,\n        'scale_pos_weight': scale_pos_weight,\n        'tree_method': 'hist',\n        'verbosity': 0,\n        'seed': RANDOM_SEED + trial.number,\n        'eval_metric': 'auc',  # Changed from logloss to AUC\n    }\n    \n    # Train model with AUC-based early stopping\n    evals = [(dtrain, 'train'), (dval, 'val')]\n    model = xgb.train(\n        params,\n        dtrain,\n        num_boost_round=n_estimators,\n        evals=evals,\n        early_stopping_rounds=50,\n        verbose_eval=False\n    )\n    \n    # Predictions\n    y_val_pred_proba = model.predict(dval)\n    y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n    \n    # === TRIVIAL PREDICTION GUARD ===\n    pred_std = np.std(y_val_pred_proba)\n    minority_pct = min(np.mean(y_val_pred == 0), np.mean(y_val_pred == 1))\n    \n    if pred_std < 0.03:\n        trial.set_user_attr('rejected', 'pred_std < 0.03')\n        return -1.0  # Force reject\n    \n    if minority_pct < 0.15:\n        trial.set_user_attr('rejected', f'minority_pct={minority_pct:.3f} < 0.15')\n        return -1.0  # Force reject\n    \n    # === MCC OBJECTIVE ===\n    mcc = matthews_corrcoef(y_val, y_val_pred)\n    \n    # If MCC is negative, the model is anti-correlated (worse than random)\n    if mcc < 0:\n        trial.set_user_attr('rejected', f'negative MCC={mcc:.4f}')\n        return -1.0\n    \n    # Also compute AUC for secondary optimization: MCC + 0.2*AUC\n    try:\n        roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n    except ValueError:\n        roc_auc = 0.5\n    \n    # Composite: primary MCC + secondary AUC boost\n    objective_value = mcc + 0.2 * (roc_auc - 0.5)\n    \n    # Store metrics for analysis\n    balanced_acc = balanced_accuracy_score(y_val, y_val_pred)\n    trial.set_user_attr('mcc', float(mcc))\n    trial.set_user_attr('balanced_acc', float(balanced_acc))\n    trial.set_user_attr('roc_auc', float(roc_auc))\n    trial.set_user_attr('pred_std', float(pred_std))\n    trial.set_user_attr('minority_pct', float(minority_pct))\n    trial.set_user_attr('down_recall', float(recall_score(y_val, y_val_pred, pos_label=0, zero_division=0)))\n    trial.set_user_attr('up_recall', float(recall_score(y_val, y_val_pred, pos_label=1, zero_division=0)))\n    trial.set_user_attr('down_precision', float(precision_score(y_val, y_val_pred, pos_label=0, zero_division=0)))\n    trial.set_user_attr('f1_down', float(f1_score(y_val, y_val_pred, pos_label=0, zero_division=0)))\n    trial.set_user_attr('rejected', 'no')\n    \n    return objective_value\n\n# Run Optuna\nprint(f\"\\nRunning Optuna with {N_TRIALS} trials...\")\nprint(\"Objective: MCC + 0.2*(AUC-0.5)\")\nprint(\"Guards: reject if pred_std < 0.03 or minority_pct < 15% or MCC < 0\")\nprint()\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(\n    direction='maximize',\n    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n)\n\nstudy.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)\n\n# Analyze trial results\nrejected_count = sum(1 for t in study.trials if t.value is not None and t.value <= -1.0)\nvalid_count = len(study.trials) - rejected_count\nprint(f\"\\nOptuna complete!\")\nprint(f\"  Total trials: {len(study.trials)}\")\nprint(f\"  Valid trials: {valid_count}\")\nprint(f\"  Rejected (trivial): {rejected_count}\")\n\nif study.best_value > -1.0:\n    bt = study.best_trial\n    print(f\"\\nBest trial #{bt.number}:\")\n    print(f\"  Objective value: {bt.value:.4f}\")\n    print(f\"  MCC: {bt.user_attrs.get('mcc', 'N/A')}\")\n    print(f\"  Balanced Acc: {bt.user_attrs.get('balanced_acc', 'N/A')}\")\n    print(f\"  ROC-AUC: {bt.user_attrs.get('roc_auc', 'N/A')}\")\n    print(f\"  Pred Std: {bt.user_attrs.get('pred_std', 'N/A')}\")\n    print(f\"  Minority Pct: {bt.user_attrs.get('minority_pct', 'N/A')}\")\n    print(f\"  DOWN recall: {bt.user_attrs.get('down_recall', 'N/A')}\")\n    print(f\"  UP recall: {bt.user_attrs.get('up_recall', 'N/A')}\")\n    print(f\"  DOWN precision: {bt.user_attrs.get('down_precision', 'N/A')}\")\n    print(f\"  F1 DOWN: {bt.user_attrs.get('f1_down', 'N/A')}\")\n    print(f\"\\n  Params: {study.best_params}\")\n    best_params = study.best_params\nelse:\n    print(\"\\nWARNING: ALL trials were rejected! No valid model found.\")\n    print(\"This means none of the 100 HP configurations produced non-trivial predictions.\")\n    print(\"The features may be fundamentally inadequate for this task.\")\n    best_params = None\n\n# Show top 5 valid trials\nprint(\"\\nTop 5 valid trials:\")\nvalid_trials = [t for t in study.trials if t.value is not None and t.value > -1.0]\nvalid_trials.sort(key=lambda t: t.value, reverse=True)\nfor i, t in enumerate(valid_trials[:5]):\n    attrs = t.user_attrs\n    print(f\"  #{t.number}: obj={t.value:.4f}, MCC={attrs.get('mcc','?')}, \"\n          f\"AUC={attrs.get('roc_auc','?')}, \"\n          f\"DOWN_recall={attrs.get('down_recall','?')}, \"\n          f\"UP_recall={attrs.get('up_recall','?')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Train Final Model with Best Params\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL MODEL TRAINING\")\nprint(\"=\"*80)\n\nif best_params is None:\n    print(\"ERROR: No valid model found in HPO. Cannot proceed with final training.\")\n    print(\"All 100 trials produced trivial predictions.\")\n    # Create dummy metrics for reporting\n    train_metrics = val_metrics = test_metrics = {\n        'balanced_acc': 0.5, 'down_recall': 0.0, 'down_precision': 0.0,\n        'f1_down': 0.0, 'up_recall': 0.0, 'up_precision': 0.0,\n        'roc_auc': 0.5, 'mcc': 0.0\n    }\n    final_model = None\nelse:\n    # Prepare final params — binary:logistic only (no focal loss)\n    final_params = {\n        'objective': 'binary:logistic',\n        'max_depth': best_params['max_depth'],\n        'min_child_weight': best_params['min_child_weight'],\n        'subsample': best_params['subsample'],\n        'colsample_bytree': best_params['colsample_bytree'],\n        'reg_lambda': best_params['reg_lambda'],\n        'reg_alpha': best_params['reg_alpha'],\n        'learning_rate': best_params['learning_rate'],\n        'scale_pos_weight': best_params['scale_pos_weight'],\n        'tree_method': 'hist',\n        'verbosity': 0,\n        'seed': RANDOM_SEED,\n        'eval_metric': 'auc',  # Changed from logloss\n    }\n    \n    print(f\"Using binary:logistic with MCC-optimized params\")\n    print(f\"  learning_rate={best_params['learning_rate']:.4f}\")\n    print(f\"  n_estimators={best_params['n_estimators']}\")\n    print(f\"  max_depth={best_params['max_depth']}\")\n    print(f\"  reg_lambda={best_params['reg_lambda']:.4f}\")\n    print(f\"  scale_pos_weight={best_params['scale_pos_weight']:.4f}\")\n    \n    # Train final model with AUC-based early stopping\n    evals = [(dtrain, 'train'), (dval, 'val')]\n    final_model = xgb.train(\n        final_params,\n        dtrain,\n        num_boost_round=best_params['n_estimators'],\n        evals=evals,\n        early_stopping_rounds=50,\n        verbose_eval=False\n    )\n    \n    # Prepare test set\n    X_test = test_df[feature_cols].values\n    y_test = test_df['target'].values\n    dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_cols)\n    \n    # Predictions on all splits (binary:logistic outputs probabilities directly)\n    train_pred_proba = final_model.predict(dtrain)\n    train_pred = (train_pred_proba > 0.5).astype(int)\n    \n    val_pred_proba = final_model.predict(dval)\n    val_pred = (val_pred_proba > 0.5).astype(int)\n    \n    test_pred_proba = final_model.predict(dtest)\n    test_pred = (test_pred_proba > 0.5).astype(int)\n    \n    # Compute metrics for all splits\n    def compute_metrics(y_true, y_pred, y_pred_proba, split_name):\n        metrics = {\n            'balanced_acc': balanced_accuracy_score(y_true, y_pred),\n            'down_recall': recall_score(y_true, y_pred, pos_label=0, zero_division=0),\n            'down_precision': precision_score(y_true, y_pred, pos_label=0, zero_division=0),\n            'f1_down': f1_score(y_true, y_pred, pos_label=0, zero_division=0),\n            'up_recall': recall_score(y_true, y_pred, pos_label=1, zero_division=0),\n            'up_precision': precision_score(y_true, y_pred, pos_label=1, zero_division=0),\n            'mcc': matthews_corrcoef(y_true, y_pred),\n        }\n        try:\n            metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n        except ValueError:\n            metrics['roc_auc'] = 0.5\n        \n        # Prediction distribution\n        pred_std = np.std(y_pred_proba)\n        down_pct = 100 * np.mean(y_pred == 0)\n        up_pct = 100 * np.mean(y_pred == 1)\n        \n        print(f\"\\n{split_name} Metrics:\")\n        print(f\"  MCC: {metrics['mcc']:.4f}\")\n        print(f\"  Balanced Accuracy: {metrics['balanced_acc']:.4f}\")\n        print(f\"  DOWN Recall: {metrics['down_recall']:.4f}\")\n        print(f\"  DOWN Precision: {metrics['down_precision']:.4f}\")\n        print(f\"  DOWN F1: {metrics['f1_down']:.4f}\")\n        print(f\"  UP Recall: {metrics['up_recall']:.4f}\")\n        print(f\"  UP Precision: {metrics['up_precision']:.4f}\")\n        print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n        print(f\"  Pred Std: {pred_std:.4f}\")\n        print(f\"  Prediction Balance: DOWN={down_pct:.1f}%, UP={up_pct:.1f}%\")\n        \n        return metrics\n    \n    train_metrics = compute_metrics(y_train, train_pred, train_pred_proba, \"TRAIN\")\n    val_metrics = compute_metrics(y_val, val_pred, val_pred_proba, \"VALIDATION\")\n    test_metrics = compute_metrics(y_test, test_pred, test_pred_proba, \"TEST\")\n    \n    # Confusion matrix (test set)\n    print(\"\\nTest Set Confusion Matrix:\")\n    cm = confusion_matrix(y_test, test_pred)\n    print(\"             Predicted\")\n    print(\"           DOWN    UP\")\n    print(f\"Actual DOWN  {cm[0,0]:3d}  {cm[0,1]:3d}\")\n    print(f\"       UP    {cm[1,0]:3d}  {cm[1,1]:3d}\")\n    \n    # Check for trivial prediction\n    test_down_pct = 100 * np.mean(test_pred == 0)\n    if test_down_pct > 90 or test_down_pct < 10:\n        print(f\"\\nWARNING: Near-trivial prediction detected! DOWN predictions = {test_down_pct:.1f}%\")\n    elif test_down_pct > 80 or test_down_pct < 20:\n        print(f\"\\nCAUTION: Imbalanced predictions. DOWN predictions = {test_down_pct:.1f}%\")\n    else:\n        print(f\"\\nPrediction balance OK. DOWN predictions = {test_down_pct:.1f}%\")\n    \n    print(\"\\nFinal model training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Feature Importance\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEATURE IMPORTANCE\")\nprint(\"=\"*80)\n\nif final_model is None:\n    print(\"No model available. Skipping feature importance.\")\n    importance_df = pd.DataFrame(columns=['feature', 'gain', 'gain_pct'])\nelse:\n    # Get feature importance (gain-based)\n    importance = final_model.get_score(importance_type='gain')\n    importance_df = pd.DataFrame([\n        {'feature': k, 'gain': v} for k, v in importance.items()\n    ]).sort_values('gain', ascending=False)\n    \n    # Normalize to percentages\n    importance_df['gain_pct'] = 100 * importance_df['gain'] / importance_df['gain'].sum()\n    \n    print(\"\\nFeature Importance (Gain):\")\n    for idx, row in importance_df.iterrows():\n        print(f\"  {row['feature']:30s} {row['gain_pct']:6.2f}%\")\n    \n    print(f\"\\nTop 5 features: {', '.join(importance_df.head(5)['feature'].tolist())}\")\n    print(f\"Bottom 3 features: {', '.join(importance_df.tail(3)['feature'].tolist())}\")\n    \n    # Check concentration\n    top_feature_pct = importance_df.iloc[0]['gain_pct']\n    if top_feature_pct > 30:\n        print(f\"\\nWARNING: Top feature dominance at {top_feature_pct:.2f}%\")\n    else:\n        print(f\"\\n Feature importance well-distributed (top feature: {top_feature_pct:.2f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Ensemble with Regression Model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: This section would load regression predictions from the dataset.\n",
    "# For now, we'll simulate ensemble metrics based on classifier performance.\n",
    "\n",
    "print(\"\\nNote: Full ensemble evaluation requires regression model predictions.\")\n",
    "print(\"Ensemble integration will be completed post-training.\")\n",
    "\n",
    "# Placeholder for threshold optimization\n",
    "# This would iterate over thresholds and compute ensemble DA + Sharpe\n",
    "optimal_threshold = 0.55  # Default\n",
    "\n",
    "print(f\"\\nOptimal threshold (default): {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Generate Classifier Output\nprint(\"\\n\" + \"=\"*80)\nprint(\"GENERATING CLASSIFIER OUTPUT\")\nprint(\"=\"*80)\n\nif final_model is None:\n    print(\"ERROR: No valid model. Cannot generate classifier output.\")\n    classifier_output = pd.DataFrame({\n        'Date': full_df.index.strftime('%Y-%m-%d'),\n        'p_up': 0.5,\n        'p_down': 0.5,\n        'predicted_direction': 1  # Default UP\n    })\nelse:\n    # Prepare full dataset predictions\n    X_full = full_df[feature_cols].values\n    dfull = xgb.DMatrix(X_full, feature_names=feature_cols)\n    full_pred_proba = final_model.predict(dfull)  # P(UP)\n    \n    # Create output DataFrame\n    classifier_output = pd.DataFrame({\n        'Date': full_df.index.strftime('%Y-%m-%d'),\n        'p_up': full_pred_proba,\n        'p_down': 1 - full_pred_proba,\n        'predicted_direction': (full_pred_proba > 0.5).astype(int)\n    })\n\n# Save classifier output\nclassifier_output.to_csv('classifier.csv', index=False)\nprint(f\"\\nClassifier output saved: {len(classifier_output)} rows\")\nprint(f\"  Date range: {classifier_output['Date'].min()} to {classifier_output['Date'].max()}\")\nprint(f\"  UP predictions: {(classifier_output['predicted_direction'] == 1).sum()} ({100*(classifier_output['predicted_direction'] == 1).sum()/len(classifier_output):.2f}%)\")\nprint(f\"  DOWN predictions: {(classifier_output['predicted_direction'] == 0).sum()} ({100*(classifier_output['predicted_direction'] == 0).sum()/len(classifier_output):.2f}%)\")\n\n# P(DOWN) distribution\nprint(f\"\\nP(DOWN) distribution:\")\nprint(f\"  Mean: {classifier_output['p_down'].mean():.4f}\")\nprint(f\"  Std: {classifier_output['p_down'].std():.4f}\")\nprint(f\"  Min: {classifier_output['p_down'].min():.4f}\")\nprint(f\"  Max: {classifier_output['p_down'].max():.4f}\")\n\nif classifier_output['p_down'].std() < 0.05:\n    print(\"\\nWARNING: P(DOWN) distribution has low variance (< 0.05)\")\nelse:\n    print(f\"\\nP(DOWN) variance OK (std={classifier_output['p_down'].std():.4f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: 2026 Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2026 PREDICTIONS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter 2026 dates\n",
    "classifier_2026 = classifier_output[classifier_output['Date'] >= '2026-01-01'].copy()\n",
    "\n",
    "if len(classifier_2026) > 0:\n",
    "    print(f\"\\n2026 predictions: {len(classifier_2026)} days\")\n",
    "    print(f\"  UP predictions: {(classifier_2026['predicted_direction'] == 1).sum()}\")\n",
    "    print(f\"  DOWN predictions: {(classifier_2026['predicted_direction'] == 0).sum()}\")\n",
    "    print(f\"  Mean P(DOWN): {classifier_2026['p_down'].mean():.4f}\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(\"\\nSample 2026 predictions:\")\n",
    "    print(classifier_2026.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo 2026 data available in current dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: Save Training Results\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAVING RESULTS\")\nprint(\"=\"*80)\n\n# Optuna analysis\nrejected_count = sum(1 for t in study.trials if t.value is not None and t.value <= -1.0)\nvalid_count = len(study.trials) - rejected_count\n\n# Prepare training result JSON\ntraining_result = {\n    \"feature\": \"classifier\",\n    \"attempt\": 2,\n    \"timestamp\": datetime.now().isoformat(),\n    \"architecture\": \"XGBoost binary:logistic (MCC-optimized)\",\n    \"n_features\": len(feature_cols),\n    \"features\": feature_cols,\n    \"best_params\": best_params,\n    \"optuna_trials_completed\": len(study.trials),\n    \"optuna_valid_trials\": valid_count,\n    \"optuna_rejected_trials\": rejected_count,\n    \"optuna_best_value\": float(study.best_value) if study.best_value > -1.0 else None,\n    \"objective_function\": \"MCC + 0.2*(AUC-0.5)\",\n    \"trivial_guards\": {\n        \"min_pred_std\": 0.03,\n        \"min_minority_pct\": 0.15,\n        \"reject_negative_mcc\": True\n    },\n    \"changes_from_attempt_1\": [\n        \"Objective: F1_DOWN composite -> MCC + 0.2*AUC\",\n        \"Trivial guard: reject if pred_std < 0.03 or minority_pct < 15%\",\n        \"HP bounds relaxed: LR [0.01,0.15], reg_lambda [0.1,3.0]\",\n        \"Early stopping: logloss -> auc\",\n        \"scale_pos_weight: [0.8,1.5] -> [0.9,2.0]\",\n        \"Dropped rate_surprise (unsigned), 18->17 features\",\n        \"Removed focal loss option\"\n    ],\n    \"standalone_metrics\": {\n        \"train\": {k: float(v) for k, v in train_metrics.items()},\n        \"val\": {k: float(v) for k, v in val_metrics.items()},\n        \"test\": {k: float(v) for k, v in test_metrics.items()},\n    },\n    \"ensemble_metrics\": {\n        \"optimal_threshold\": 0.55,\n        \"note\": \"Full ensemble evaluation pending\"\n    },\n    \"class_balance\": {\n        \"train_up_pct\": float(100 * (y_train == 1).sum() / len(y_train)),\n        \"train_down_pct\": float(100 * (y_train == 0).sum() / len(y_train)),\n        \"val_up_pct\": float(100 * (y_val == 1).sum() / len(y_val)),\n        \"val_down_pct\": float(100 * (y_val == 0).sum() / len(y_val)),\n        \"test_up_pct\": float(100 * (y_test == 1).sum() / len(y_test)),\n        \"test_down_pct\": float(100 * (y_test == 0).sum() / len(y_test)),\n    },\n    \"p_down_distribution\": {\n        \"mean\": float(classifier_output['p_down'].mean()),\n        \"std\": float(classifier_output['p_down'].std()),\n        \"min\": float(classifier_output['p_down'].min()),\n        \"max\": float(classifier_output['p_down'].max()),\n    },\n    \"data_info\": {\n        \"train_samples\": len(train_df),\n        \"val_samples\": len(val_df),\n        \"test_samples\": len(test_df),\n        \"full_samples\": len(full_df),\n        \"train_date_range\": f\"{train_df.index.min()} to {train_df.index.max()}\",\n        \"val_date_range\": f\"{val_df.index.min()} to {val_df.index.max()}\",\n        \"test_date_range\": f\"{test_df.index.min()} to {test_df.index.max()}\",\n    }\n}\n\n# Add feature importance if model exists\nif final_model is not None:\n    importance = final_model.get_score(importance_type='gain')\n    importance_df_save = pd.DataFrame([\n        {'feature': k, 'gain': v} for k, v in importance.items()\n    ]).sort_values('gain', ascending=False)\n    importance_df_save['gain_pct'] = 100 * importance_df_save['gain'] / importance_df_save['gain'].sum()\n    \n    training_result[\"feature_importance\"] = {\n        \"ranked\": importance_df_save[['feature', 'gain_pct']].to_dict('records'),\n        \"top5\": importance_df_save.head(5)['feature'].tolist(),\n        \"bottom3\": importance_df_save.tail(3)['feature'].tolist(),\n    }\n\n# Save training result\nwith open('training_result.json', 'w', encoding='utf-8') as f:\n    json.dump(training_result, f, indent=2, default=str)\nprint(\"\\nTraining result saved: training_result.json\")\n\n# Save model\nif final_model is not None:\n    final_model.save_model('model.json')\n    print(\"Model saved: model.json\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*80)\nprint(f\"End time: {datetime.now().isoformat()}\")\nprint(\"\\nOutputs:\")\nprint(\"  - classifier.csv: Classifier predictions for all dates\")\nprint(\"  - training_result.json: Complete training metrics and metadata\")\nif final_model is not None:\n    print(\"  - model.json: Trained XGBoost model\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12: Diagnostic Plots\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSTIC PLOTS\")\nprint(\"=\"*80)\n\nif final_model is None:\n    print(\"No model available. Skipping diagnostic plots.\")\nelse:\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # 1. P(DOWN) distribution\n    axes[0, 0].hist(classifier_output['p_down'], bins=50, edgecolor='black', alpha=0.7)\n    axes[0, 0].set_title('P(DOWN) Distribution')\n    axes[0, 0].set_xlabel('P(DOWN)')\n    axes[0, 0].set_ylabel('Frequency')\n    axes[0, 0].axvline(0.5, color='red', linestyle='--', label='Decision Threshold')\n    axes[0, 0].legend()\n    \n    # 2. Feature Importance\n    if len(importance_df) > 0:\n        top_10 = importance_df.head(10)\n        axes[0, 1].barh(top_10['feature'], top_10['gain_pct'])\n        axes[0, 1].set_title('Top 10 Feature Importance (Gain %)')\n        axes[0, 1].set_xlabel('Gain %')\n        axes[0, 1].invert_yaxis()\n    \n    # 3. Confusion Matrix (Test)\n    cm = confusion_matrix(y_test, test_pred)\n    im = axes[1, 0].imshow(cm, cmap='Blues')\n    axes[1, 0].set_title('Test Set Confusion Matrix')\n    axes[1, 0].set_xlabel('Predicted')\n    axes[1, 0].set_ylabel('Actual')\n    axes[1, 0].set_xticks([0, 1])\n    axes[1, 0].set_yticks([0, 1])\n    axes[1, 0].set_xticklabels(['DOWN', 'UP'])\n    axes[1, 0].set_yticklabels(['DOWN', 'UP'])\n    for i in range(2):\n        for j in range(2):\n            text = axes[1, 0].text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n    plt.colorbar(im, ax=axes[1, 0])\n    \n    # 4. MCC Comparison across splits\n    metrics_names = ['MCC', 'Balanced Acc', 'ROC-AUC']\n    train_vals = [train_metrics['mcc'], train_metrics['balanced_acc'], train_metrics['roc_auc']]\n    val_vals = [val_metrics['mcc'], val_metrics['balanced_acc'], val_metrics['roc_auc']]\n    test_vals = [test_metrics['mcc'], test_metrics['balanced_acc'], test_metrics['roc_auc']]\n    \n    x = np.arange(len(metrics_names))\n    width = 0.25\n    axes[1, 1].bar(x - width, train_vals, width, label='Train')\n    axes[1, 1].bar(x, val_vals, width, label='Val')\n    axes[1, 1].bar(x + width, test_vals, width, label='Test')\n    axes[1, 1].set_title('Metrics Comparison Across Splits')\n    axes[1, 1].set_ylabel('Score')\n    axes[1, 1].set_xticks(x)\n    axes[1, 1].set_xticklabels(metrics_names)\n    axes[1, 1].legend()\n    axes[1, 1].set_ylim([-0.2, 1])\n    axes[1, 1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n    \n    plt.tight_layout()\n    plt.savefig('diagnostics.png', dpi=100, bbox_inches='tight')\n    print(\"\\nDiagnostic plots saved: diagnostics.png\")\n    plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}