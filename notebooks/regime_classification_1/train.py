"""
Gold Prediction - Regime Classification SubModel Training
Attempt 1: Gaussian Mixture Model for multi-dimensional macro regime detection
Generated by builder_model agent
"""

# ============================================================
# 1. IMPORTS
# ============================================================
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import optuna
from optuna.pruners import MedianPruner

# Random seed for reproducibility
np.random.seed(42)

print("=== Gold Regime Classification SubModel Training ===")
print(f"Started: {datetime.now().isoformat()}")

# ============================================================
# 2. DATA FETCHING
# ============================================================
def fetch_and_preprocess():
    """
    Self-contained data fetching for regime_classification submodel.
    Returns: (train_df, val_df, test_df, full_df)

    Features:
    - vix_z: z-score of VIX (20d rolling window)
    - yield_spread_z: z-score of (DGS10 - DGS2) (60d rolling window)
    - equity_return_z: z-score of S&P 500 5d return (60d rolling window)
    - gold_rvol_z: z-score of Gold 10d realized volatility (60d rolling window)
    """

    # === FRED API ===
    try:
        from fredapi import Fred
    except ImportError:
        import subprocess
        subprocess.run(["pip", "install", "fredapi"], check=True)
        from fredapi import Fred

    # FRED API key
    FRED_API_KEY = "3ffb68facdf6321e180e380c00e909c8"
    fred = Fred(FRED_API_KEY)

    # === Yahoo Finance ===
    try:
        import yfinance as yf
    except ImportError:
        import subprocess
        subprocess.run(["pip", "install", "yfinance"], check=True)
        import yfinance as yf

    # === 1. Fetch raw data ===
    print("Fetching FRED data...")
    vix = fred.get_series('VIXCLS', observation_start='2014-01-01')
    dgs10 = fred.get_series('DGS10', observation_start='2014-01-01')
    dgs2 = fred.get_series('DGS2', observation_start='2014-01-01')

    print("Fetching Yahoo data...")
    spx_data = yf.download('^GSPC', start='2014-01-01', progress=False)
    gold_data = yf.download('GC=F', start='2014-01-01', progress=False)

    # Extract Close prices (yfinance returns MultiIndex columns)
    if isinstance(spx_data.columns, pd.MultiIndex):
        spx_close = spx_data['Close'].iloc[:, 0]  # First ticker
    else:
        spx_close = spx_data['Close']

    if isinstance(gold_data.columns, pd.MultiIndex):
        gold_close = gold_data['Close'].iloc[:, 0]  # First ticker
    else:
        gold_close = gold_data['Close']

    # === 2. Convert to DataFrames and align indices ===
    df_vix = pd.DataFrame({'vix': vix})
    df_dgs10 = pd.DataFrame({'dgs10': dgs10})
    df_dgs2 = pd.DataFrame({'dgs2': dgs2})
    df_spx = pd.DataFrame({'spx': spx_close})
    df_gold = pd.DataFrame({'gold': gold_close})

    # Ensure datetime index
    for df in [df_vix, df_dgs10, df_dgs2, df_spx, df_gold]:
        df.index = pd.to_datetime(df.index)

    # Forward fill missing values (holidays, weekends)
    df_vix = df_vix.ffill(limit=5)
    df_dgs10 = df_dgs10.ffill(limit=5)
    df_dgs2 = df_dgs2.ffill(limit=5)
    df_spx = df_spx.ffill(limit=3)
    df_gold = df_gold.ffill(limit=3)

    # Inner join on date (only keep dates with all data available)
    df = df_vix.join(df_dgs10, how='inner')
    df = df.join(df_dgs2, how='inner')
    df = df.join(df_spx, how='inner')
    df = df.join(df_gold, how='inner')

    print(f"Raw data aligned: {len(df)} rows from {df.index.min()} to {df.index.max()}")

    # === 3. Feature engineering ===

    # vix_z: z-score of VIX (20d rolling window)
    vix_mean = df['vix'].rolling(20, min_periods=20).mean()
    vix_std = df['vix'].rolling(20, min_periods=20).std()
    df['vix_z'] = (df['vix'] - vix_mean) / vix_std
    df['vix_z'] = df['vix_z'].replace([np.inf, -np.inf], np.nan)

    # yield_spread_z: z-score of (DGS10 - DGS2) (60d rolling window)
    df['yield_spread'] = df['dgs10'] - df['dgs2']
    spread_mean = df['yield_spread'].rolling(60, min_periods=60).mean()
    spread_std = df['yield_spread'].rolling(60, min_periods=60).std()
    df['yield_spread_z'] = (df['yield_spread'] - spread_mean) / spread_std
    df['yield_spread_z'] = df['yield_spread_z'].replace([np.inf, -np.inf], np.nan)

    # equity_return_z: z-score of S&P 500 5d return (60d rolling window)
    df['spx_5d_ret'] = df['spx'].pct_change(5)
    ret_mean = df['spx_5d_ret'].rolling(60, min_periods=60).mean()
    ret_std = df['spx_5d_ret'].rolling(60, min_periods=60).std()
    df['equity_return_z'] = (df['spx_5d_ret'] - ret_mean) / ret_std
    df['equity_return_z'] = df['equity_return_z'].replace([np.inf, -np.inf], np.nan)

    # gold_rvol_z: z-score of Gold 10d realized volatility (60d rolling window)
    df['gold_log_ret'] = np.log(df['gold'] / df['gold'].shift(1))
    df['gold_rvol_10d'] = df['gold_log_ret'].rolling(10, min_periods=10).std() * np.sqrt(252)  # annualized
    rvol_mean = df['gold_rvol_10d'].rolling(60, min_periods=60).mean()
    rvol_std = df['gold_rvol_10d'].rolling(60, min_periods=60).std()
    df['gold_rvol_z'] = (df['gold_rvol_10d'] - rvol_mean) / rvol_std
    df['gold_rvol_z'] = df['gold_rvol_z'].replace([np.inf, -np.inf], np.nan)

    # Clip z-scores to [-4, 4] to handle extreme outliers
    for col in ['vix_z', 'yield_spread_z', 'equity_return_z', 'gold_rvol_z']:
        df[col] = df[col].clip(-4, 4)

    # === 4. Drop rows with NaN (from rolling window warmup) ===
    features = ['vix_z', 'yield_spread_z', 'equity_return_z', 'gold_rvol_z']
    df_clean = df[features].dropna()

    print(f"After dropping NaN: {len(df_clean)} rows from {df_clean.index.min()} to {df_clean.index.max()}")

    # === 5. Train/Val/Test split (70/15/15, time-series order) ===
    n = len(df_clean)
    train_end = int(n * 0.70)
    val_end = int(n * 0.85)

    train_df = df_clean.iloc[:train_end]
    val_df = df_clean.iloc[train_end:val_end]
    test_df = df_clean.iloc[val_end:]

    print(f"\nData split:")
    print(f"  Train: {len(train_df)} rows ({train_df.index.min()} to {train_df.index.max()})")
    print(f"  Val:   {len(val_df)} rows ({val_df.index.min()} to {val_df.index.max()})")
    print(f"  Test:  {len(test_df)} rows ({test_df.index.min()} to {test_df.index.max()})")

    return train_df, val_df, test_df, df_clean


# ============================================================
# 3. OPTUNA HPO
# ============================================================
def run_hpo(X_train, X_val, n_trials=50, timeout=300):
    """
    Hyperparameter optimization with Optuna.
    Objective: Minimize BIC on validation set.
    """

    def objective(trial):
        # Search space
        n_components = trial.suggest_categorical('n_components', [2, 3, 4])
        covariance_type = trial.suggest_categorical('covariance_type', ['diag', 'full'])
        reg_covar = trial.suggest_float('reg_covar', 1e-6, 1e-3, log=True)
        smoothing_window = trial.suggest_categorical('smoothing_window', [1, 3, 5, 7])
        use_pca = trial.suggest_categorical('use_pca', [False, True])

        # PCA preprocessing (optional)
        if use_pca:
            pca_components = trial.suggest_categorical('pca_components', [2, 3])
            pca = PCA(n_components=pca_components, random_state=42)
            X_train_input = pca.fit_transform(X_train)
            X_val_input = pca.transform(X_val)
        else:
            X_train_input = X_train
            X_val_input = X_val
            pca = None

        # Fit GMM
        gmm = GaussianMixture(
            n_components=n_components,
            covariance_type=covariance_type,
            reg_covar=reg_covar,
            n_init=20,
            init_params='kmeans',
            max_iter=200,
            tol=1e-4,
            random_state=42,
        )
        gmm.fit(X_train_input)

        # --- Quality checks (prune if degenerate) ---
        weights = gmm.weights_
        if np.min(weights) < 0.05:
            raise optuna.TrialPruned("Regime collapse: min weight < 5%")

        # Compute regime probs on validation set
        val_probs = gmm.predict_proba(X_val_input)

        # Apply smoothing (backward-looking)
        if smoothing_window > 1:
            val_probs_smooth = pd.DataFrame(val_probs).rolling(
                smoothing_window, min_periods=1
            ).mean().values
            # Re-normalize to sum to 1
            row_sums = val_probs_smooth.sum(axis=1, keepdims=True)
            row_sums = np.where(row_sums == 0, 1, row_sums)  # Avoid division by zero
            val_probs_smooth = val_probs_smooth / row_sums
        else:
            val_probs_smooth = val_probs

        # Check regime persistence on validation set
        dominant_regime = np.argmax(val_probs_smooth, axis=1)
        regime_changes = np.sum(np.diff(dominant_regime) != 0)
        avg_duration = len(dominant_regime) / max(regime_changes, 1)

        if avg_duration < 2 or avg_duration > 100:
            raise optuna.TrialPruned(f"Bad persistence: {avg_duration:.1f} days")

        # --- Objective: BIC on validation set (lower is better) ---
        val_bic = gmm.bic(X_val_input)

        # Secondary: penalize if regime balance is poor
        min_weight_penalty = -100 * np.min(weights)  # reward higher min weight

        return val_bic + min_weight_penalty  # minimize

    # Run optimization
    study = optuna.create_study(
        direction='minimize',
        sampler=optuna.samplers.TPESampler(seed=42),
        pruner=MedianPruner(n_warmup_steps=5)
    )

    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=False)

    print(f"\nOptuna HPO completed:")
    print(f"  Best value: {study.best_value:.2f}")
    print(f"  Best params: {study.best_params}")
    print(f"  Trials completed: {len(study.trials)}")

    return study.best_params, study.best_value, len(study.trials)


# ============================================================
# 4. FINAL MODEL TRAINING
# ============================================================
def train_final_model(X_train, X_val, X_full, best_params):
    """
    Train final GMM with best params and generate regime probabilities.
    """

    # Extract params
    n_components = best_params['n_components']
    covariance_type = best_params['covariance_type']
    reg_covar = best_params['reg_covar']
    smoothing_window = best_params['smoothing_window']
    use_pca = best_params['use_pca']

    # PCA preprocessing (optional)
    if use_pca:
        pca_components = best_params['pca_components']
        pca = PCA(n_components=pca_components, random_state=42)
        X_train_input = pca.fit_transform(X_train)
        X_val_input = pca.transform(X_val)
        X_full_input = pca.transform(X_full)
    else:
        X_train_input = X_train
        X_val_input = X_val
        X_full_input = X_full
        pca = None

    # Fit GMM
    gmm = GaussianMixture(
        n_components=n_components,
        covariance_type=covariance_type,
        reg_covar=reg_covar,
        n_init=20,
        init_params='kmeans',
        max_iter=200,
        tol=1e-4,
        random_state=42,
    )
    gmm.fit(X_train_input)

    # Compute metrics
    train_bic = gmm.bic(X_train_input)
    val_bic = gmm.bic(X_val_input)
    train_log_likelihood = gmm.score(X_train_input)
    val_log_likelihood = gmm.score(X_val_input)

    print(f"\nFinal model metrics:")
    print(f"  Train BIC: {train_bic:.2f}")
    print(f"  Val BIC: {val_bic:.2f}")
    print(f"  Train log-likelihood: {train_log_likelihood:.4f}")
    print(f"  Val log-likelihood: {val_log_likelihood:.4f}")
    print(f"  Component weights: {gmm.weights_}")

    # Generate regime probabilities on full dataset
    regime_probs_raw = gmm.predict_proba(X_full_input)

    # Apply smoothing (backward-looking)
    if smoothing_window > 1:
        regime_probs_smooth = pd.DataFrame(regime_probs_raw).rolling(
            smoothing_window, min_periods=1
        ).mean().values
        # Re-normalize rows to sum to 1.0
        row_sums = regime_probs_smooth.sum(axis=1, keepdims=True)
        row_sums = np.where(row_sums == 0, 1, row_sums)
        regime_probs_smooth = regime_probs_smooth / row_sums
    else:
        regime_probs_smooth = regime_probs_raw

    # Compute transition velocity
    diff = np.diff(regime_probs_smooth, axis=0)  # [N-1, K]
    velocity = np.sqrt(np.sum(diff ** 2, axis=1))  # [N-1]
    velocity = np.concatenate([[0.0], velocity])  # [N], prepend 0 for first day

    # Regime labeling (post-hoc, for interpretability)
    means = gmm.means_
    regime_labels = {}
    for k in range(n_components):
        if use_pca:
            # Inverse transform to get original feature space means
            mean_original = pca.inverse_transform(means[k:k+1])[0]
        else:
            mean_original = means[k]

        vix_z_mean = mean_original[0]
        equity_return_z_mean = mean_original[2]
        gold_rvol_z_mean = mean_original[3]

        # Label based on mean characteristics
        if vix_z_mean > 0.5 and equity_return_z_mean < -0.3:
            label = "Risk-Off"
        elif vix_z_mean < -0.3 and equity_return_z_mean > 0.3:
            label = "Risk-On"
        else:
            label = "Calm"

        regime_labels[k] = {
            "name": label,
            "vix_z": float(mean_original[0]),
            "yield_spread_z": float(mean_original[1]),
            "equity_return_z": float(mean_original[2]),
            "gold_rvol_z": float(mean_original[3]),
        }

    print(f"\nRegime labels:")
    for k, info in regime_labels.items():
        print(f"  Regime {k} ({info['name']}): vix_z={info['vix_z']:.2f}, "
              f"yield_spread_z={info['yield_spread_z']:.2f}, "
              f"equity_return_z={info['equity_return_z']:.2f}, "
              f"gold_rvol_z={info['gold_rvol_z']:.2f}")

    # Check regime balance
    regime_assignments = np.argmax(regime_probs_smooth, axis=1)
    regime_balance = {}
    for k in range(n_components):
        pct = np.mean(regime_assignments == k) * 100
        regime_balance[f"regime_{k}"] = float(pct)
        print(f"  Regime {k} captures {pct:.1f}% of observations")

    # Check avg duration
    regime_changes = np.sum(np.diff(regime_assignments) != 0)
    avg_duration = len(regime_assignments) / max(regime_changes, 1)
    print(f"  Average regime duration: {avg_duration:.1f} trading days")

    # Collapse detection
    collapse_warning = False
    for k in range(n_components):
        pct = regime_balance[f"regime_{k}"]
        if pct < 5.0:
            print(f"  WARNING: Regime {k} captures only {pct:.1f}% (degenerate)")
            collapse_warning = True
        if pct > 80.0:
            print(f"  WARNING: Regime {k} captures {pct:.1f}% (dominant)")
            collapse_warning = True

    metrics = {
        "train_bic": float(train_bic),
        "val_bic": float(val_bic),
        "train_log_likelihood": float(train_log_likelihood),
        "val_log_likelihood": float(val_log_likelihood),
        "overfit_ratio": float(val_log_likelihood / (train_log_likelihood + 1e-10)),
        "n_components": int(n_components),
        "covariance_type": covariance_type,
        "component_weights": gmm.weights_.tolist(),
        "regime_balance": regime_balance,
        "avg_regime_duration_days": float(avg_duration),
        "regime_labels": regime_labels,
        "collapse_warning": collapse_warning,
    }

    return regime_probs_smooth, velocity, metrics


# ============================================================
# 5. MAIN EXECUTION
# ============================================================
if __name__ == "__main__":
    # Fetch and preprocess data
    train_data, val_data, test_data, full_data = fetch_and_preprocess()

    # Convert to numpy arrays
    feature_cols = ['vix_z', 'yield_spread_z', 'equity_return_z', 'gold_rvol_z']
    X_train = train_data[feature_cols].values
    X_val = val_data[feature_cols].values
    X_test = test_data[feature_cols].values
    X_full = full_data[feature_cols].values

    print(f"\nInput data shapes:")
    print(f"  X_train: {X_train.shape}")
    print(f"  X_val: {X_val.shape}")
    print(f"  X_test: {X_test.shape}")
    print(f"  X_full: {X_full.shape}")

    # Run Optuna HPO
    print("\n=== Running Optuna HPO ===")
    best_params, best_value, n_trials = run_hpo(X_train, X_val, n_trials=50, timeout=300)

    # Train final model
    print("\n=== Training final model ===")
    regime_probs, velocity, metrics = train_final_model(X_train, X_val, X_full, best_params)

    # Create output DataFrame
    n_components = best_params['n_components']
    output_df = pd.DataFrame({
        'Date': full_data.index.strftime('%Y-%m-%d'),
    })

    for k in range(n_components):
        output_df[f'regime_prob_{k}'] = regime_probs[:, k]

    output_df['regime_transition_velocity'] = velocity

    print(f"\n=== Output DataFrame ===")
    print(f"Shape: {output_df.shape}")
    print(f"Columns: {list(output_df.columns)}")
    print(f"Date range: {output_df['Date'].min()} to {output_df['Date'].max()}")
    print(f"\nFirst 5 rows:")
    print(output_df.head())
    print(f"\nLast 5 rows:")
    print(output_df.tail())

    # Quality checks
    print(f"\n=== Quality Checks ===")
    print(f"NaN counts:")
    print(output_df.isna().sum())

    print(f"\nProbability sum validation (should be ~1.0):")
    prob_cols = [f'regime_prob_{k}' for k in range(n_components)]
    prob_sums = output_df[prob_cols].sum(axis=1)
    print(f"  Min sum: {prob_sums.min():.6f}")
    print(f"  Max sum: {prob_sums.max():.6f}")
    print(f"  Mean sum: {prob_sums.mean():.6f}")

    print(f"\nVelocity statistics:")
    print(f"  Min: {output_df['regime_transition_velocity'].min():.6f}")
    print(f"  Max: {output_df['regime_transition_velocity'].max():.6f}")
    print(f"  Mean: {output_df['regime_transition_velocity'].mean():.6f}")
    print(f"  Std: {output_df['regime_transition_velocity'].std():.6f}")

    # Save outputs
    print(f"\n=== Saving results ===")
    output_df.to_csv("submodel_output.csv", index=False)
    print("Saved submodel_output.csv")

    # Training result
    result = {
        "feature": "regime_classification",
        "attempt": 1,
        "timestamp": datetime.now().isoformat(),
        "best_params": best_params,
        "metrics": metrics,
        "output_shape": list(output_df.shape),
        "output_columns": list(output_df.columns),
        "optuna_n_trials": n_trials,
        "optuna_best_value": float(best_value),
        "data_info": {
            "train_samples": len(train_data),
            "val_samples": len(val_data),
            "test_samples": len(test_data),
            "full_samples": len(full_data),
            "date_range": {
                "start": full_data.index.min().strftime('%Y-%m-%d'),
                "end": full_data.index.max().strftime('%Y-%m-%d'),
            },
        },
    }

    with open("training_result.json", "w") as f:
        json.dump(result, f, indent=2)
    print("Saved training_result.json")

    print(f"\n=== Training complete! ===")
    print(f"Finished: {datetime.now().isoformat()}")
