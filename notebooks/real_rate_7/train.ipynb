{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel Training - real_rate Attempt 7\n",
    "\n",
    "**Method**: Real Yield Curve Shape Features (Daily DFII Multi-Tenor)\n",
    "\n",
    "**Approach**: Deterministic transformation of DFII5/DFII7/DFII10/DFII20/DFII30 daily series.\n",
    "Extracts slope change, curvature change, level change velocity, and slope regime indicator.\n",
    "No neural network - pure computation. Solves the monthly forward-fill root cause of attempts 3-5.\n",
    "\n",
    "**Self-contained**: Data fetch (FRED API) + Feature computation + Gate evaluation + Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install dependencies\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\nimport warnings\nimport urllib.request\nfrom io import StringIO\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\nprint(f'Started: {datetime.now().isoformat()}')\nprint('Libraries loaded successfully')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: FRED Public CSV Data Fetcher (No API key required)\ndef fetch_fred_series(series_id, start_date=None, end_date=None):\n    \"\"\"\n    Fetch FRED series using public CSV endpoint (no API key required).\n    Returns a pandas Series indexed by date.\n    \"\"\"\n    url = f'https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}'\n    try:\n        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n        with urllib.request.urlopen(req, timeout=60) as response:\n            csv_data = response.read().decode('utf-8')\n    except Exception as e:\n        raise RuntimeError(f'Failed to fetch FRED series {series_id}: {e}')\n\n    df = pd.read_csv(StringIO(csv_data))\n    # Columns: 'DATE' and series_id (or 'VALUE')\n    date_col = df.columns[0]\n    val_col = df.columns[1]\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.set_index(date_col)\n    # Replace '.' with NaN (FRED uses '.' for missing values)\n    series = pd.to_numeric(df[val_col], errors='coerce')\n    series.name = series_id\n    if start_date:\n        series = series[series.index >= pd.Timestamp(start_date)]\n    if end_date:\n        series = series[series.index <= pd.Timestamp(end_date)]\n    return series\n\nprint('FRED public CSV fetcher ready (no API key required)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Data Fetching - All 5 DFII series\nprint('=' * 60)\nprint('DATA FETCHING')\nprint('=' * 60)\n\n# Fetch with 1 year buffer before 2015 for z-score warmup (60-day window)\nFETCH_START = '2014-01-01'\nFETCH_END = '2025-02-28'\nSCHEMA_START = '2015-01-02'\nSCHEMA_END = '2025-02-12'\n\nseries_ids = ['DFII5', 'DFII7', 'DFII10', 'DFII20', 'DFII30']\nraw_series = {}\nfor sid in series_ids:\n    s = fetch_fred_series(sid, start_date=FETCH_START, end_date=FETCH_END)\n    raw_series[sid] = s\n    n_clean = s.dropna()\n    print(f'  {sid}: {len(n_clean)} obs, {n_clean.index[0].date()} to {n_clean.index[-1].date()}')\n\n# Combine into dataframe - keep rows where ALL 5 series are available\ndf_raw = pd.DataFrame(raw_series)\ndf_raw.index = pd.to_datetime(df_raw.index)\ndf_raw = df_raw.sort_index()\n\n# Drop rows where any series is missing (FRED occasional gaps are rare)\ndf = df_raw.dropna()\nprint(f'\\nCombined (all 5 non-null): {len(df)} obs, {df.index[0].date()} to {df.index[-1].date()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Intermediate Series Computation\n",
    "print('=' * 60)\n",
    "print('INTERMEDIATE SERIES')\n",
    "print('=' * 60)\n",
    "\n",
    "# Yield curve shape components\n",
    "df['slope'] = df['DFII30'] - df['DFII5']           # long-short spread\n",
    "df['curvature'] = 2 * df['DFII10'] - df['DFII5'] - df['DFII30']  # belly distortion (Nelson-Siegel factor 3)\n",
    "\n",
    "print(f'Slope range: {df[\"slope\"].min():.3f} to {df[\"slope\"].max():.3f} (mean={df[\"slope\"].mean():.3f})')\n",
    "print(f'Curvature range: {df[\"curvature\"].min():.3f} to {df[\"curvature\"].max():.3f} (mean={df[\"curvature\"].mean():.3f})')\n",
    "\n",
    "# Daily changes\n",
    "df['dfii10_chg'] = df['DFII10'].diff()\n",
    "df['slope_chg'] = df['slope'].diff()\n",
    "df['curvature_chg'] = df['curvature'].diff()\n",
    "\n",
    "print(f'DFII10 daily chg: [{df[\"dfii10_chg\"].min():.4f}, {df[\"dfii10_chg\"].max():.4f}]')\n",
    "print(f'Slope daily chg: [{df[\"slope_chg\"].min():.4f}, {df[\"slope_chg\"].max():.4f}]')\n",
    "print(f'Curvature daily chg: [{df[\"curvature_chg\"].min():.4f}, {df[\"curvature_chg\"].max():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Submodel Dataset Path Resolution\n",
    "print('=' * 60)\n",
    "print('DATASET PATH RESOLUTION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Resolve path to gold-prediction-submodels dataset\n",
    "# Try both API-created and web-created mount paths\n",
    "candidate_paths = [\n",
    "    '/kaggle/input/datasets/bigbigzabuton/gold-prediction-submodels',\n",
    "    '../input/gold-prediction-submodels',\n",
    "    '/kaggle/input/gold-prediction-submodels',\n",
    "]\n",
    "SUBMODEL_PATH = None\n",
    "for cp in candidate_paths:\n",
    "    if os.path.exists(cp):\n",
    "        SUBMODEL_PATH = cp\n",
    "        print(f'Found submodel dataset at: {cp}')\n",
    "        break\n",
    "\n",
    "if SUBMODEL_PATH is None:\n",
    "    # List available inputs for debugging\n",
    "    print('ERROR: gold-prediction-submodels not found!')\n",
    "    for root in ['/kaggle/input', '../input']:\n",
    "        if os.path.exists(root):\n",
    "            print(f'Available at {root}: {os.listdir(root)}')\n",
    "    raise RuntimeError('gold-prediction-submodels dataset not found. Check kernel-metadata.json dataset_sources.')\n",
    "\n",
    "# List available files\n",
    "submodel_files = os.listdir(SUBMODEL_PATH)\n",
    "print(f'Submodel files ({len(submodel_files)}): {submodel_files[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Feature Computation (Deterministic)\n",
    "print('=' * 60)\n",
    "print('FEATURE COMPUTATION')\n",
    "print('=' * 60)\n",
    "\n",
    "def rolling_zscore(series, window, min_periods_ratio=0.5):\n",
    "    \"\"\"Rolling z-score normalization. Returns nan during warmup.\"\"\"\n",
    "    min_p = max(10, int(window * min_periods_ratio))\n",
    "    mu = series.rolling(window, min_periods=min_p).mean()\n",
    "    sigma = series.rolling(window, min_periods=min_p).std()\n",
    "    # Avoid division by zero\n",
    "    sigma = sigma.where(sigma > 1e-10, np.nan)\n",
    "    z = (series - mu) / sigma\n",
    "    # Clip to reasonable range (beyond +-4 sigma is noise)\n",
    "    return z.clip(-4, 4)\n",
    "\n",
    "# Feature 1: rr_level_change_z - standardized daily change in 10Y real yield\n",
    "# Window=30: captures ~1.5 month regime of rate velocity\n",
    "rr_level_change_z = rolling_zscore(df['dfii10_chg'], 30)\n",
    "\n",
    "# Feature 2: rr_slope_chg_z - standardized daily change in slope (30Y-5Y)\n",
    "# Window=60: captures medium-term steepening/flattening dynamics\n",
    "rr_slope_chg_z = rolling_zscore(df['slope_chg'], 60)\n",
    "\n",
    "# Feature 3: rr_curvature_chg_z - standardized daily change in curvature\n",
    "# Window=60: captures belly distortion dynamics\n",
    "rr_curvature_chg_z = rolling_zscore(df['curvature_chg'], 60)\n",
    "\n",
    "# Feature 4: rr_slope_level_z - regime indicator (how inverted/steep the real curve is)\n",
    "# Window=60: tracks the current slope regime relative to recent history\n",
    "# Note: high autocorrelation (0.93) is expected and acceptable for a regime indicator\n",
    "rr_slope_level_z = rolling_zscore(df['slope'], 60)\n",
    "\n",
    "# Build output dataframe\n",
    "features = pd.DataFrame({\n",
    "    'rr_level_change_z': rr_level_change_z,\n",
    "    'rr_slope_chg_z': rr_slope_chg_z,\n",
    "    'rr_curvature_chg_z': rr_curvature_chg_z,\n",
    "    'rr_slope_level_z': rr_slope_level_z,\n",
    "}, index=df.index)\n",
    "\n",
    "print('Features computed:')\n",
    "for col in features.columns:\n",
    "    n_valid = features[col].notna().sum()\n",
    "    n_nan = features[col].isna().sum()\n",
    "    nan_pct = n_nan / len(features) * 100\n",
    "    ac = features[col].autocorr(1)\n",
    "    std = features[col].std()\n",
    "    print(f'  {col}: valid={n_valid}, nan={n_nan} ({nan_pct:.1f}%), autocorr={ac:.4f}, std={std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Align to Schema Date Range\n",
    "print('=' * 60)\n",
    "print('ALIGNMENT TO SCHEMA DATES')\n",
    "print('=' * 60)\n",
    "\n",
    "# Load gold prices for target computation and date alignment\n",
    "import yfinance as yf\n",
    "gold = yf.download('GC=F', start=FETCH_START, end=FETCH_END, auto_adjust=True, progress=False)\n",
    "gold_dates = gold.index\n",
    "print(f'Gold trading calendar: {len(gold_dates)} dates, {gold_dates[0].date()} to {gold_dates[-1].date()}')\n",
    "\n",
    "# Filter to gold trading dates (FRED reports on weekdays but may miss some holidays)\n",
    "# Also filter to schema range\n",
    "schema_dates = gold_dates[(gold_dates >= SCHEMA_START) & (gold_dates <= SCHEMA_END)]\n",
    "print(f'Schema dates: {len(schema_dates)} dates, {schema_dates[0].date()} to {schema_dates[-1].date()}')\n",
    "\n",
    "# Reindex features to schema dates\n",
    "# Use forward-fill ONLY for 1 day (handles rare FRED publication lag)\n",
    "features_aligned = features.reindex(schema_dates, method='ffill', limit=1)\n",
    "\n",
    "# Compute target variable: next-day gold return (%)\n",
    "gold_close = gold['Close'].squeeze()\n",
    "gold_return = gold_close.pct_change() * 100  # percentage return\n",
    "gold_return_next = gold_return.shift(-1)  # NEXT day's return (prediction target)\n",
    "gold_return_next_schema = gold_return_next.reindex(schema_dates)\n",
    "\n",
    "print(f'Features aligned shape: {features_aligned.shape}')\n",
    "print(f'NaN counts after alignment:')\n",
    "for col in features_aligned.columns:\n",
    "    n_nan = features_aligned[col].isna().sum()\n",
    "    print(f'  {col}: {n_nan} NaN ({n_nan/len(features_aligned)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Data Split (70/15/15)\n",
    "print('=' * 60)\n",
    "print('DATA SPLIT')\n",
    "print('=' * 60)\n",
    "\n",
    "# Use common dates (features + target both available)\n",
    "common_mask = features_aligned.notna().all(axis=1) & gold_return_next_schema.notna()\n",
    "common_dates = schema_dates[common_mask]\n",
    "print(f'Common dates (features + target): {len(common_dates)}')\n",
    "\n",
    "n = len(common_dates)\n",
    "n_train = int(n * 0.70)\n",
    "n_val = int(n * 0.15)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_dates = common_dates[:n_train]\n",
    "val_dates = common_dates[n_train:n_train + n_val]\n",
    "test_dates = common_dates[n_train + n_val:]\n",
    "\n",
    "print(f'Train: {len(train_dates)} ({train_dates[0].date()} to {train_dates[-1].date()})')\n",
    "print(f'Val:   {len(val_dates)} ({val_dates[0].date()} to {val_dates[-1].date()})')\n",
    "print(f'Test:  {len(test_dates)} ({test_dates[0].date()} to {test_dates[-1].date()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Gate 1 - Standalone Quality Check\n",
    "print('=' * 60)\n",
    "print('GATE 1: STANDALONE QUALITY')\n",
    "print('=' * 60)\n",
    "\n",
    "gate1_results = {}\n",
    "\n",
    "for col in features_aligned.columns:\n",
    "    feat = features_aligned[col].dropna()\n",
    "    nan_pct = features_aligned[col].isna().mean() * 100\n",
    "    std_val = feat.std()\n",
    "    autocorr = feat.autocorr(1)\n",
    "    \n",
    "    # Check for constant output\n",
    "    is_constant = std_val < 0.01\n",
    "    # Check for all NaN\n",
    "    is_all_nan = len(feat) == 0\n",
    "    # High autocorr only flagged if > 0.99 (our regime feature rr_slope_level_z ~0.93 is acceptable)\n",
    "    high_autocorr = autocorr > 0.99\n",
    "    \n",
    "    pass_gate1 = not is_constant and not is_all_nan and not high_autocorr\n",
    "    \n",
    "    gate1_results[col] = {\n",
    "        'nan_pct': float(nan_pct),\n",
    "        'std': float(std_val),\n",
    "        'autocorr': float(autocorr),\n",
    "        'is_constant': bool(is_constant),\n",
    "        'is_all_nan': bool(is_all_nan),\n",
    "        'high_autocorr': bool(high_autocorr),\n",
    "        'pass': bool(pass_gate1)\n",
    "    }\n",
    "    \n",
    "    status = 'PASS' if pass_gate1 else 'FAIL'\n",
    "    print(f'  {col}: {status} | nan={nan_pct:.1f}%, std={std_val:.4f}, autocorr={autocorr:.4f}')\n",
    "\n",
    "gate1_pass = all(r['pass'] for r in gate1_results.values())\n",
    "\n",
    "# Overfit ratio: N/A for deterministic features (no training = no overfitting)\n",
    "overfit_ratio = 1.0  # by definition\n",
    "print(f'\\nOverfit ratio: {overfit_ratio:.2f} (deterministic, no training)')\n",
    "print(f'Gate 1 Overall: {\"PASS\" if gate1_pass else \"FAIL\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Gate 2 - Information Gain (MI test)\n",
    "print('=' * 60)\n",
    "print('GATE 2: INFORMATION GAIN')\n",
    "print('=' * 60)\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def compute_mi(feature, target, n_bins=20):\n",
    "    \"\"\"Compute mutual information using quantile binning.\"\"\"\n",
    "    mask = feature.notna() & target.notna()\n",
    "    f = feature[mask]\n",
    "    t = target[mask]\n",
    "    if len(f) < 100:\n",
    "        return 0.0\n",
    "    try:\n",
    "        f_binned = pd.qcut(f, q=n_bins, labels=False, duplicates='drop')\n",
    "        t_binned = pd.qcut(t, q=n_bins, labels=False, duplicates='drop')\n",
    "        return float(mutual_info_score(f_binned, t_binned))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# Load base features to compute baseline MI\n",
    "base_mi_total = None\n",
    "try:\n",
    "    base_path = os.path.join(SUBMODEL_PATH, 'base_features.csv')\n",
    "    if not os.path.exists(base_path):\n",
    "        base_path = os.path.join(SUBMODEL_PATH, 'meta_model_input.csv')\n",
    "    base_df = pd.read_csv(base_path, index_col=0, parse_dates=True)\n",
    "    base_df.index = pd.to_datetime(base_df.index)\n",
    "    print(f'Loaded base features: {base_df.shape}')\n",
    "    \n",
    "    # Compute baseline MI for base features on test set\n",
    "    target_test = gold_return_next_schema.reindex(test_dates)\n",
    "    base_test = base_df.reindex(test_dates)\n",
    "    \n",
    "    base_mi_total = 0.0\n",
    "    for col in base_df.select_dtypes(include=[np.number]).columns[:10]:  # top 10 base features\n",
    "        mi = compute_mi(base_test[col], target_test)\n",
    "        base_mi_total += mi\n",
    "    print(f'Baseline MI (top 10 base features, test set): {base_mi_total:.6f}')\n",
    "except Exception as e:\n",
    "    print(f'WARNING: Could not load base features: {e}')\n",
    "    base_mi_total = 0.1  # fallback: assume small baseline\n",
    "\n",
    "# Compute MI for new features on test set\n",
    "target_test = gold_return_next_schema.reindex(test_dates)\n",
    "feat_test = features_aligned.reindex(test_dates)\n",
    "\n",
    "new_mi_total = 0.0\n",
    "per_feature_mi = {}\n",
    "for col in features_aligned.columns:\n",
    "    mi = compute_mi(feat_test[col], target_test)\n",
    "    new_mi_total += mi\n",
    "    per_feature_mi[col] = float(mi)\n",
    "    print(f'  MI({col}) = {mi:.6f}')\n",
    "\n",
    "print(f'\\nNew features MI total: {new_mi_total:.6f}')\n",
    "print(f'Baseline MI total: {base_mi_total:.6f}')\n",
    "\n",
    "# MI increase percentage\n",
    "if base_mi_total > 0:\n",
    "    mi_increase_pct = (new_mi_total / base_mi_total) * 100\n",
    "else:\n",
    "    mi_increase_pct = 999.0\n",
    "print(f'MI increase: {mi_increase_pct:.2f}% (threshold: > 5%)')\n",
    "gate2_mi_pass = mi_increase_pct > 5.0\n",
    "print(f'Gate 2 MI: {\"PASS\" if gate2_mi_pass else \"FAIL\"}')\n",
    "\n",
    "# VIF check\n",
    "from numpy.linalg import inv\n",
    "feat_clean = feat_test.dropna()\n",
    "if len(feat_clean) > 10:\n",
    "    X = feat_clean.values\n",
    "    corr = np.corrcoef(X.T)\n",
    "    try:\n",
    "        vif_values = np.diag(inv(corr))\n",
    "        max_vif = float(np.max(vif_values))\n",
    "    except:\n",
    "        max_vif = 999.0\n",
    "else:\n",
    "    max_vif = 0.0\n",
    "print(f'Max VIF: {max_vif:.3f} (threshold: < 10)')\n",
    "gate2_vif_pass = max_vif < 10.0\n",
    "\n",
    "# Rolling correlation stability\n",
    "target_full = gold_return_next_schema\n",
    "rolling_corr_stds = []\n",
    "for col in features_aligned.columns:\n",
    "    merged = pd.concat([features_aligned[col], target_full], axis=1).dropna()\n",
    "    if len(merged) > 120:\n",
    "        rc = merged.iloc[:,0].rolling(60).corr(merged.iloc[:,1])\n",
    "        rolling_corr_stds.append(float(rc.std()))\n",
    "max_rolling_corr_std = max(rolling_corr_stds) if rolling_corr_stds else 0.0\n",
    "print(f'Max rolling corr std: {max_rolling_corr_std:.4f} (threshold: < 0.15)')\n",
    "gate2_stability_pass = max_rolling_corr_std < 0.15\n",
    "\n",
    "gate2_pass = gate2_mi_pass and gate2_vif_pass\n",
    "print(f'Gate 2 Overall: {\"PASS\" if gate2_pass else \"FAIL\"} (MI={gate2_mi_pass}, VIF={gate2_vif_pass}, stability={gate2_stability_pass})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: Gate 3 - Ablation Test (5-fold cross-validation)\nprint('=' * 60)\nprint('GATE 3: ABLATION TEST (5-FOLD CV)')\nprint('=' * 60)\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\n\n# Load meta-model input (base features + submodels) from Kaggle dataset\n# meta_model_input.csv is preferred; if missing, construct from base_features_raw.csv + submodel CSVs\nmeta_input_path = os.path.join(SUBMODEL_PATH, 'meta_model_input.csv')\nif os.path.exists(meta_input_path):\n    meta_df = pd.read_csv(meta_input_path, index_col=0, parse_dates=True)\n    print(f'Meta model input loaded: {meta_df.shape}')\nelse:\n    print('meta_model_input.csv not found - constructing from base_features_raw.csv + submodel CSVs')\n    base_path = os.path.join(SUBMODEL_PATH, 'base_features_raw.csv')\n    meta_df = pd.read_csv(base_path, index_col=0, parse_dates=True)\n    print(f'  base_features_raw.csv: {meta_df.shape}')\n    # Load all submodel CSVs except real_rate (the feature being tested)\n    sm_files = sorted([f for f in os.listdir(SUBMODEL_PATH)\n                       if f.endswith('.csv') and f != 'base_features_raw.csv'\n                       and 'real_rate' not in f])\n    for fname in sm_files:\n        try:\n            sm_df = pd.read_csv(os.path.join(SUBMODEL_PATH, fname), index_col=0, parse_dates=True)\n            sm_df.index = pd.to_datetime(sm_df.index)\n            meta_df = meta_df.join(sm_df, how='left')\n            print(f'  {fname}: {sm_df.shape}')\n        except Exception as e:\n            print(f'  WARNING: Could not load {fname}: {e}')\n    print(f'Meta model input (constructed): {meta_df.shape}')\n\nmeta_df.index = pd.to_datetime(meta_df.index)\nprint(f'Columns: {list(meta_df.columns[:10])}...')\n\n# Align target\ntarget_col = 'gold_return_next' if 'gold_return_next' in meta_df.columns else meta_df.columns[0]\nprint(f'Target column: {target_col}')\n\n# Get base features (without target)\nbase_feature_cols = [c for c in meta_df.columns if c != target_col]\nprint(f'Base feature columns: {len(base_feature_cols)}')\n\ndef compute_direction_accuracy(y_true, y_pred):\n    \"\"\"Direction accuracy, excluding exact-zero returns.\"\"\"\n    mask = y_true != 0\n    if mask.sum() == 0:\n        return 0.5\n    return float((np.sign(y_pred[mask]) == np.sign(y_true[mask])).mean())\n\ndef compute_sharpe(y_true, y_pred, cost_bps=5):\n    \"\"\"Sharpe ratio with 5bps transaction cost per trade.\"\"\"\n    position = np.sign(y_pred)\n    strategy_ret = position * y_true\n    # Transaction cost on each position change\n    position_changes = np.abs(np.diff(np.concatenate([[0], position])))\n    costs = position_changes * (cost_bps / 10000.0)\n    net_ret = strategy_ret - costs\n    if net_ret.std() < 1e-10:\n        return 0.0\n    return float(net_ret.mean() / net_ret.std() * np.sqrt(252))\n\n# XGBoost config (simple, no HPO needed for ablation)\nxgb_params = {\n    'max_depth': 3,\n    'learning_rate': 0.05,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_lambda': 1.0,\n    'random_state': 42,\n    'n_jobs': -1,\n    'verbosity': 0\n}\n\n# 5-fold time-series cross-validation\nfolds = []\nn_common = len(common_dates)\nfold_size = n_common // 5\nfor i in range(5):\n    test_start = i * fold_size\n    test_end = (i + 1) * fold_size if i < 4 else n_common\n    train_idx = list(range(0, test_start))\n    test_idx = list(range(test_start, test_end))\n    if len(train_idx) < 50:\n        print(f'  Fold {i+1}: skipped (insufficient training data)')\n        continue\n    folds.append((train_idx, test_idx))\n\nprint(f'Running {len(folds)} folds...')\n\nbase_da_folds = []\nbase_sharpe_folds = []\nbase_mae_folds = []\next_da_folds = []\next_sharpe_folds = []\next_mae_folds = []\n\nfor fold_i, (train_idx, test_idx) in enumerate(folds):\n    fold_train_dates = common_dates[train_idx]\n    fold_test_dates = common_dates[test_idx]\n\n    base_train = meta_df[base_feature_cols].reindex(fold_train_dates)\n    base_test = meta_df[base_feature_cols].reindex(fold_test_dates)\n    y_train = meta_df[target_col].reindex(fold_train_dates)\n    y_test = meta_df[target_col].reindex(fold_test_dates)\n\n    new_train = features_aligned.reindex(fold_train_dates)\n    new_test = features_aligned.reindex(fold_test_dates)\n\n    ext_train = pd.concat([base_train, new_train], axis=1)\n    ext_test = pd.concat([base_test, new_test], axis=1)\n\n    base_train_clean = base_train.join(y_train).dropna()\n    ext_train_clean = ext_train.join(y_train).dropna()\n    base_test_clean = base_test.join(y_test).dropna()\n    ext_test_clean = ext_test.join(y_test).dropna()\n\n    if len(base_train_clean) < 50 or len(base_test_clean) < 10:\n        print(f'  Fold {fold_i+1}: skipped (insufficient data)')\n        continue\n\n    base_model = xgb.XGBRegressor(**xgb_params)\n    base_model.fit(base_train_clean[base_feature_cols], base_train_clean[target_col])\n    base_pred = base_model.predict(base_test_clean[base_feature_cols])\n    y_test_base = base_test_clean[target_col].values\n\n    base_da = compute_direction_accuracy(y_test_base, base_pred)\n    base_sh = compute_sharpe(y_test_base, base_pred)\n    base_mae = mean_absolute_error(y_test_base, base_pred)\n    base_da_folds.append(base_da)\n    base_sharpe_folds.append(base_sh)\n    base_mae_folds.append(base_mae)\n\n    ext_cols = [c for c in ext_train.columns if c != target_col]\n    ext_model = xgb.XGBRegressor(**xgb_params)\n    ext_model.fit(ext_train_clean[ext_cols], ext_train_clean[target_col])\n    ext_pred = ext_model.predict(ext_test_clean[ext_cols])\n    y_test_ext = ext_test_clean[target_col].values\n\n    ext_da = compute_direction_accuracy(y_test_ext, ext_pred)\n    ext_sh = compute_sharpe(y_test_ext, ext_pred)\n    ext_mae = mean_absolute_error(y_test_ext, ext_pred)\n    ext_da_folds.append(ext_da)\n    ext_sharpe_folds.append(ext_sh)\n    ext_mae_folds.append(ext_mae)\n\n    print(f'  Fold {fold_i+1}: Base DA={base_da:.4f}, Ext DA={ext_da:.4f} (delta={ext_da-base_da:+.4f})')\n    print(f'           Base Sharpe={base_sh:.4f}, Ext Sharpe={ext_sh:.4f} (delta={ext_sh-base_sh:+.4f})')\n    print(f'           Base MAE={base_mae:.4f}, Ext MAE={ext_mae:.4f} (delta={ext_mae-base_mae:+.4f})')\n\nbase_da = float(np.mean(base_da_folds))\next_da = float(np.mean(ext_da_folds))\nbase_sharpe = float(np.mean(base_sharpe_folds))\next_sharpe = float(np.mean(ext_sharpe_folds))\nbase_mae = float(np.mean(base_mae_folds))\next_mae = float(np.mean(ext_mae_folds))\n\nda_delta = ext_da - base_da\nsharpe_delta = ext_sharpe - base_sharpe\nmae_delta = ext_mae - base_mae\n\nprint(f'\\n--- Gate 3 Summary ---')\nprint(f'Direction Accuracy: {base_da:.4f} -> {ext_da:.4f} (delta={da_delta:+.4f})')\nprint(f'Sharpe Ratio:       {base_sharpe:.4f} -> {ext_sharpe:.4f} (delta={sharpe_delta:+.4f})')\nprint(f'MAE:                {base_mae:.4f} -> {ext_mae:.4f} (delta={mae_delta:+.4f})')\n\ngate3_da_pass = da_delta >= 0.005\ngate3_sharpe_pass = sharpe_delta >= 0.05\ngate3_mae_pass = mae_delta <= -0.01\ngate3_pass = gate3_da_pass or gate3_sharpe_pass or gate3_mae_pass\nprint(f'Gate 3: DA={gate3_da_pass}, Sharpe={gate3_sharpe_pass}, MAE={gate3_mae_pass}')\nprint(f'Gate 3 Overall: {\"PASS\" if gate3_pass else \"FAIL\"}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save Outputs\n",
    "print('=' * 60)\n",
    "print('SAVING OUTPUTS')\n",
    "print('=' * 60)\n",
    "\n",
    "# 1. Submodel output CSV\n",
    "output = features_aligned.copy()\n",
    "output.index.name = 'date'\n",
    "output.to_csv('submodel_output.csv')\n",
    "print(f'Saved submodel_output.csv: {output.shape}')\n",
    "\n",
    "# 2. Training result JSON\n",
    "result = {\n",
    "    'feature': 'real_rate',\n",
    "    'attempt': 7,\n",
    "    'method': 'deterministic_yield_curve_shape',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'description': 'Real yield curve shape features: slope change z-score, curvature change z-score, level change z-score, slope level z-score. All daily DFII series. No forward-fill.',\n",
    "    'data_sources': ['FRED:DFII5', 'FRED:DFII7', 'FRED:DFII10', 'FRED:DFII20', 'FRED:DFII30'],\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': list(output.columns),\n",
    "    'gate1': {\n",
    "        'pass': gate1_pass,\n",
    "        'overfit_ratio': overfit_ratio,\n",
    "        'checks': gate1_results\n",
    "    },\n",
    "    'gate2': {\n",
    "        'pass': gate2_pass,\n",
    "        'checks': {\n",
    "            'mi': {\n",
    "                'new_total': float(new_mi_total),\n",
    "                'baseline_total': float(base_mi_total),\n",
    "                'increase': float(mi_increase_pct),\n",
    "                'pass': bool(gate2_mi_pass)\n",
    "            },\n",
    "            'vif': {\n",
    "                'max': float(max_vif),\n",
    "                'pass': bool(gate2_vif_pass)\n",
    "            },\n",
    "            'stability': {\n",
    "                'max_rolling_corr_std': float(max_rolling_corr_std),\n",
    "                'pass': bool(gate2_stability_pass)\n",
    "            }\n",
    "        },\n",
    "        'per_feature_mi': per_feature_mi\n",
    "    },\n",
    "    'gate3': {\n",
    "        'pass': gate3_pass,\n",
    "        'baseline': {\n",
    "            'direction_accuracy': base_da,\n",
    "            'sharpe_ratio': base_sharpe,\n",
    "            'mae': base_mae\n",
    "        },\n",
    "        'extended': {\n",
    "            'direction_accuracy': ext_da,\n",
    "            'sharpe_ratio': ext_sharpe,\n",
    "            'mae': ext_mae\n",
    "        },\n",
    "        'checks': {\n",
    "            'direction': {\n",
    "                'delta': float(da_delta),\n",
    "                'threshold': 0.005,\n",
    "                'pass': bool(gate3_da_pass)\n",
    "            },\n",
    "            'sharpe': {\n",
    "                'delta': float(sharpe_delta),\n",
    "                'threshold': 0.05,\n",
    "                'pass': bool(gate3_sharpe_pass)\n",
    "            },\n",
    "            'mae': {\n",
    "                'delta': float(mae_delta),\n",
    "                'threshold': -0.01,\n",
    "                'pass': bool(gate3_mae_pass)\n",
    "            }\n",
    "        },\n",
    "        'fold_results': {\n",
    "            'base_da': base_da_folds,\n",
    "            'ext_da': ext_da_folds,\n",
    "            'base_sharpe': base_sharpe_folds,\n",
    "            'ext_sharpe': ext_sharpe_folds,\n",
    "            'base_mae': base_mae_folds,\n",
    "            'ext_mae': ext_mae_folds\n",
    "        }\n",
    "    },\n",
    "    'feature_stats': {\n",
    "        col: {\n",
    "            'mean': float(output[col].mean()),\n",
    "            'std': float(output[col].std()),\n",
    "            'min': float(output[col].min()),\n",
    "            'max': float(output[col].max()),\n",
    "            'autocorr': float(output[col].autocorr(1)),\n",
    "            'nan_pct': float(output[col].isna().mean() * 100)\n",
    "        }\n",
    "        for col in output.columns\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print('Saved training_result.json')\n",
    "\n",
    "print(f'\\nFinished: {datetime.now().isoformat()}')\n",
    "print('Training complete!')\n",
    "print(f'Gate 1: {\"PASS\" if gate1_pass else \"FAIL\"}')\n",
    "print(f'Gate 2: {\"PASS\" if gate2_pass else \"FAIL\"} (MI +{mi_increase_pct:.1f}%, VIF {max_vif:.2f})')\n",
    "print(f'Gate 3: {\"PASS\" if gate3_pass else \"FAIL\"} (DA {da_delta:+.4f}, Sharpe {sharpe_delta:+.4f}, MAE {mae_delta:+.4f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}