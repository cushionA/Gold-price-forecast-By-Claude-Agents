{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold SubModel: Yield Curve - Attempt 2\n",
    "**Approach**: Deterministic z-score features only (no HMM)\n",
    "**Changes from Attempt 1**:\n",
    "- Removed HMM (collapsed to single state in attempt 1 -> yc_regime_prob was constant)\n",
    "- Added DGS3MO (3-month T-bill, not in base features) for new information\n",
    "- 4 output columns: curvature_z, spread_velocity_z, 10y3m_velocity_z, dgs3mo_velocity_z\n",
    "- 50 Optuna trials (up from 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport sys\n\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pandas-datareader', '-q'])\n\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as pdr\nimport yfinance as yf\nimport optuna\nimport json\nimport os\nimport warnings\nfrom datetime import datetime\nfrom sklearn.metrics import mutual_info_score\n\nwarnings.filterwarnings('ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nnp.random.seed(42)\n\nprint(f'=== Gold SubModel Training: yield_curve attempt 2 ===')\nprint(f'Approach: Deterministic z-score features (no HMM, no API key needed)')\nprint(f'Started: {datetime.now().isoformat()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# No FRED API key needed - using pandas_datareader for direct FRED public access\n# pandas_datareader fetches FRED data over public HTTP (no authentication required)\nprint('Using pandas_datareader for FRED data (no API key required)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_data():\n    \"\"\"\n    Fetch yield curve data from FRED via pandas_datareader (no API key needed)\n    Series: DGS10, DGS2, DGS5, DGS3MO\n    DGS3MO is NEW data not in base_features.\n    \"\"\"\n    start = '2014-01-01'\n    print('Fetching yield curve data from FRED via pandas_datareader...')\n\n    # pandas_datareader fetches FRED data publicly (no API key required)\n    dgs10 = pdr.DataReader('DGS10', 'fred', start=start)['DGS10'] / 100\n    dgs2  = pdr.DataReader('DGS2',  'fred', start=start)['DGS2']  / 100\n    dgs5  = pdr.DataReader('DGS5',  'fred', start=start)['DGS5']  / 100\n    dgs3mo = pdr.DataReader('DGS3MO', 'fred', start=start)['DGS3MO'] / 100\n\n    df = pd.DataFrame({\n        'dgs10':  dgs10,\n        'dgs2':   dgs2,\n        'dgs5':   dgs5,\n        'dgs3mo': dgs3mo,\n    })\n    df.index = pd.to_datetime(df.index)\n    df.index.name = 'date'\n\n    # Forward-fill weekends and holidays (max 5 business days)\n    df = df.ffill(limit=5)\n    df = df.dropna()\n\n    # Derived series\n    df['spread_10y2y'] = df['dgs10'] - df['dgs2']\n    df['spread_10y3m'] = df['dgs10'] - df['dgs3mo']\n    df['curvature']    = df['dgs5'] - 0.5 * (df['dgs2'] + df['dgs10'])\n\n    print(f'Yield data: {len(df)} rows from {df.index.min().date()} to {df.index.max().date()}')\n\n    # Gold target from Yahoo Finance\n    print('Fetching GLD for gold target...')\n    gld = yf.download('GLD', start='2015-01-01', progress=False)\n    if gld.empty:\n        raise ValueError('GLD download returned empty DataFrame')\n    if len(gld) < 100:\n        raise ValueError(f'GLD download too short: {len(gld)} rows')\n\n    if isinstance(gld.columns, pd.MultiIndex):\n        gold_close = gld['Close'].iloc[:, 0]\n    else:\n        gold_close = gld['Close']\n\n    target = pd.DataFrame({'gold_close': gold_close.values}, index=gld.index)\n    target.index = pd.to_datetime(target.index)\n    target.index.name = 'date'\n    target['gold_return_next'] = target['gold_close'].pct_change().shift(-1) * 100\n    target = target.dropna(subset=['gold_return_next'])\n\n    # Align on common dates, restrict to 2015-01-01 onwards\n    common_idx = df.index.intersection(target.index)\n    common_idx = common_idx[common_idx >= pd.Timestamp('2015-01-01')]\n    df = df.loc[common_idx]\n    target = target.loc[common_idx]\n\n    print(f'Aligned: {len(df)} rows from {df.index.min().date()} to {df.index.max().date()}')\n    return df, target"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, change_window, velocity_zscore_window, curvature_zscore_window):\n",
    "    \"\"\"\n",
    "    Generate 4 deterministic yield curve features (no HMM).\n",
    "    All features use z-scores of CHANGES to avoid unit-root autocorrelation issues.\n",
    "\n",
    "    Outputs:\n",
    "      yc_curvature_z        : z-score of daily curvature change (DGS5-based, autocorr ~ -0.15)\n",
    "      yc_spread_velocity_z  : z-score of N-day 10Y-2Y spread change (autocorr 0.62-0.85)\n",
    "      yc_10y3m_velocity_z   : z-score of N-day 10Y-3M spread change (NEW: uses DGS3MO)\n",
    "      yc_dgs3mo_velocity_z  : z-score of N-day DGS3MO change (NEW: Fed policy signal)\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    def zscore_of_change(series, n_change, window):\n",
    "        chg = series.diff(n_change)\n",
    "        mu = chg.rolling(window).mean()\n",
    "        sigma = chg.rolling(window).std()\n",
    "        z = (chg - mu) / sigma\n",
    "        return z.clip(-4, 4).ffill()\n",
    "\n",
    "    # 1. Curvature z-score (curvature = DGS5 - 0.5*(DGS2+DGS10))\n",
    "    #    Use 1-day change; curvature_zscore_window for normalization\n",
    "    result['yc_curvature_z'] = zscore_of_change(df['curvature'], 1, curvature_zscore_window)\n",
    "\n",
    "    # 2. 10Y-2Y spread velocity z-score\n",
    "    result['yc_spread_velocity_z'] = zscore_of_change(df['spread_10y2y'], change_window, velocity_zscore_window)\n",
    "\n",
    "    # 3. 10Y-3M spread velocity z-score (NEW: uses DGS3MO data)\n",
    "    result['yc_10y3m_velocity_z'] = zscore_of_change(df['spread_10y3m'], change_window, velocity_zscore_window)\n",
    "\n",
    "    # 4. DGS3MO velocity z-score (NEW: captures Fed policy shifts)\n",
    "    result['yc_dgs3mo_velocity_z'] = zscore_of_change(df['dgs3mo'], change_window, velocity_zscore_window)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(df, target_df, train_end, val_end):\n",
    "    val_y = target_df['gold_return_next'].iloc[train_end:val_end].values\n",
    "\n",
    "    def discretize(x, bins=20):\n",
    "        valid = ~np.isnan(x)\n",
    "        if valid.sum() < bins:\n",
    "            return None\n",
    "        x_clean = x.copy()\n",
    "        x_clean[~valid] = np.nanmedian(x)\n",
    "        try:\n",
    "            return pd.qcut(x_clean, bins, labels=False, duplicates='drop')\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def objective(trial):\n",
    "        change_window = trial.suggest_categorical('change_window', [3, 5, 10])\n",
    "        velocity_zscore_window = trial.suggest_categorical('velocity_zscore_window', [30, 60, 90, 120])\n",
    "        curvature_zscore_window = trial.suggest_categorical('curvature_zscore_window', [30, 60, 90, 120])\n",
    "\n",
    "        try:\n",
    "            features = generate_features(df, change_window, velocity_zscore_window, curvature_zscore_window)\n",
    "            val_X = features.iloc[train_end:val_end]\n",
    "\n",
    "            mi_sum = 0.0\n",
    "            for col in features.columns:\n",
    "                feat_val = val_X[col].values\n",
    "                mask = ~np.isnan(feat_val) & ~np.isnan(val_y)\n",
    "                if mask.sum() < 50:\n",
    "                    continue\n",
    "                feat_disc = discretize(feat_val[mask])\n",
    "                tgt_disc = discretize(val_y[mask])\n",
    "                if feat_disc is not None and tgt_disc is not None:\n",
    "                    mi_sum += mutual_info_score(feat_disc, tgt_disc)\n",
    "            return mi_sum\n",
    "        except Exception as e:\n",
    "            print(f'Trial failed: {e}')\n",
    "            return 0.0\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fetch Data ===\n",
    "data_df, target_df = fetch_data()\n",
    "\n",
    "n = len(data_df)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "print(f'\\nData split:')\n",
    "print(f'  Train: {train_end} rows ({data_df.index[0].date()} - {data_df.index[train_end-1].date()})')\n",
    "print(f'  Val:   {val_end - train_end} rows ({data_df.index[train_end].date()} - {data_df.index[val_end-1].date()})')\n",
    "print(f'  Test:  {n - val_end} rows ({data_df.index[val_end].date()} - {data_df.index[-1].date()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optuna HPO: 50 trials ===\n",
    "print('Running Optuna HPO (50 trials)...')\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    create_objective(data_df, target_df, train_end, val_end),\n",
    "    n_trials=50,\n",
    "    timeout=600\n",
    ")\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'\\nOptuna complete:')\n",
    "print(f'  Best MI sum: {study.best_value:.4f}')\n",
    "print(f'  Best params: {best_params}')\n",
    "print(f'  Completed trials: {len(study.trials)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate Final Output with Best Params ===\n",
    "print('\\nGenerating final submodel output...')\n",
    "output = generate_features(data_df, **best_params)\n",
    "\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Output columns: {list(output.columns)}')\n",
    "print(f'Date range: {output.index.min().date()} to {output.index.max().date()}')\n",
    "print('\\nOutput summary:')\n",
    "print(output.describe())\n",
    "\n",
    "# Gate 1 checks: std and autocorrelation\n",
    "print('\\nGate 1 checks (training set):')\n",
    "train_output = output.iloc[:train_end]\n",
    "autocorr_results = {}\n",
    "for col in output.columns:\n",
    "    vals = train_output[col].dropna().values\n",
    "    std_val = np.std(vals)\n",
    "    if len(vals) > 1:\n",
    "        ac = np.corrcoef(vals[:-1], vals[1:])[0, 1]\n",
    "    else:\n",
    "        ac = 0.0\n",
    "    autocorr_results[col] = float(ac)\n",
    "    status = 'FAIL' if std_val < 1e-6 else ('WARN' if abs(ac) > 0.95 else 'OK')\n",
    "    print(f'  {col}: std={std_val:.4f}, autocorr={ac:.4f} [{status}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Gate 2 prep: MI on validation set ===\n",
    "def discretize_final(x, bins=20):\n",
    "    valid = ~np.isnan(x)\n",
    "    if valid.sum() < bins:\n",
    "        return None\n",
    "    x_clean = x.copy()\n",
    "    x_clean[~valid] = np.nanmedian(x)\n",
    "    try:\n",
    "        return pd.qcut(x_clean, bins, labels=False, duplicates='drop')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "val_X = output.iloc[train_end:val_end]\n",
    "val_y = target_df['gold_return_next'].iloc[train_end:val_end]\n",
    "\n",
    "mi_results = {}\n",
    "for col in output.columns:\n",
    "    feat_val = val_X[col].values\n",
    "    mask = ~np.isnan(feat_val) & ~np.isnan(val_y.values)\n",
    "    feat_disc = discretize_final(feat_val[mask])\n",
    "    tgt_disc = discretize_final(val_y.values[mask])\n",
    "    if feat_disc is not None and tgt_disc is not None:\n",
    "        mi_results[col] = float(mutual_info_score(feat_disc, tgt_disc))\n",
    "    else:\n",
    "        mi_results[col] = 0.0\n",
    "\n",
    "mi_sum = sum(mi_results.values())\n",
    "print(f'MI results (validation set):')\n",
    "for col, mi in mi_results.items():\n",
    "    print(f'  {col}: {mi:.4f}')\n",
    "print(f'  MI Sum: {mi_sum:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Results ===\n",
    "print('\\nSaving results...')\n",
    "\n",
    "output_with_date = output.reset_index()\n",
    "output_with_date.columns = ['Date'] + list(output.columns)\n",
    "output_with_date.to_csv('submodel_output.csv', index=False)\n",
    "\n",
    "result = {\n",
    "    'feature': 'yield_curve',\n",
    "    'attempt': 2,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_params': best_params,\n",
    "    'metrics': {\n",
    "        'mi_individual': mi_results,\n",
    "        'mi_sum': mi_sum,\n",
    "        'autocorr': autocorr_results,\n",
    "        'optuna_best_value': float(study.best_value),\n",
    "        'optuna_trials_completed': len(study.trials)\n",
    "    },\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': list(output.columns),\n",
    "    'data_info': {\n",
    "        'total_samples': len(data_df),\n",
    "        'train_samples': train_end,\n",
    "        'val_samples': val_end - train_end,\n",
    "        'test_samples': n - val_end,\n",
    "        'date_range_start': str(data_df.index.min().date()),\n",
    "        'date_range_end': str(data_df.index.max().date())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "\n",
    "print(f'=== Training complete! ===')\n",
    "print(f'Finished: {datetime.now().isoformat()}')\n",
    "print(f'Files: submodel_output.csv, training_result.json')\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Columns: {list(output.columns)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}