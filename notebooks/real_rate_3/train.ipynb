{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel: real_rate Attempt 3\n",
    "\n",
    "**Feature**: Multi-Country Real Interest Rate Dynamics  \n",
    "**Architecture**: Compact Transformer Autoencoder with GPU Support  \n",
    "**Generated by**: builder_model agent  \n",
    "**Date**: 2026-02-14\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains a Transformer-based autoencoder on multi-country interest rate data to extract global monetary policy regime context. The model compresses 28 features from 7 countries into 4-6 latent semantic dimensions.\n",
    "\n",
    "**Key Design Principles**:\n",
    "- Self-contained: All data fetching and processing in this notebook\n",
    "- GPU-enabled: Utilizes CUDA if available\n",
    "- Compact architecture: 8-45K parameters to prevent overfitting on ~253 monthly samples\n",
    "- Pre-norm Transformer: More stable training than post-norm\n",
    "- Monthlyâ†’daily forward-fill: Outputs regime indicators at daily frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 1: Imports and GPU Detection ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install fredapi if not available\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "except ImportError:\n",
    "    print(\"Installing fredapi...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"fredapi\"], check=True)\n",
    "    from fredapi import Fred\n",
    "\n",
    "# Install yfinance if not available\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    print(\"Installing yfinance...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"yfinance\"], check=True)\n",
    "    import yfinance as yf\n",
    "\n",
    "# GPU Detection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"=\" * 60)\n",
    "print(\"DEVICE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"\\nImports complete. Random seeds set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: Data Fetching (Self-Contained) ===\n",
    "\n",
    "def fetch_multi_country_features():\n",
    "    \"\"\"\n",
    "    Fetch and process multi-country interest rate features.\n",
    "    Returns: pd.DataFrame with monthly features, DatetimeIndex\n",
    "    \n",
    "    Features (28 total):\n",
    "    - US TIPS (2): level + change\n",
    "    - 6 countries x 3 (18): nominal level + change + lagged CPI\n",
    "    - Cross-country aggregates (4): dispersions + spread\n",
    "    - VIX (1): monthly average\n",
    "    - Gold price (1): for calendar alignment\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA FETCHING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get FRED API key from Kaggle Secrets\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        api_key = secrets.get_secret(\"FRED_API_KEY\")\n",
    "        print(\"[OK] FRED API key loaded from Kaggle Secrets\")\n",
    "    except:\n",
    "        # Fallback to environment variable (for local testing)\n",
    "        api_key = os.environ.get('FRED_API_KEY')\n",
    "        if api_key is None:\n",
    "            raise RuntimeError(\"FRED_API_KEY not found in Kaggle Secrets or environment\")\n",
    "        print(\"[OK] FRED API key loaded from environment\")\n",
    "    \n",
    "    fred = Fred(api_key=api_key)\n",
    "    \n",
    "    # === 1. Fetch US TIPS (daily) ===\n",
    "    print(\"\\n1/9: Fetching US TIPS (DFII10)...\")\n",
    "    us_tips = fred.get_series('DFII10', observation_start='2003-01-01')\n",
    "    print(f\"  -> {len(us_tips)} daily observations\")\n",
    "    \n",
    "    # === 2. Fetch 6 countries nominal yields + CPI ===\n",
    "    countries = {\n",
    "        'germany': ('IRLTLT01DEM156N', 'CPALTT01DEM659N'),\n",
    "        'uk': ('IRLTLT01GBM156N', 'CPALTT01GBM659N'),\n",
    "        'canada': ('IRLTLT01CAM156N', 'CPALTT01CAM659N'),\n",
    "        'switzerland': ('IRLTLT01CHM156N', 'CPALTT01CHM659N'),\n",
    "        'norway': ('IRLTLT01NOM156N', 'CPALTT01NOM659N'),\n",
    "        'sweden': ('IRLTLT01SEM156N', 'CPALTT01SEM659N')\n",
    "    }\n",
    "    \n",
    "    country_data = {}\n",
    "    for i, (country, (nominal_id, cpi_id)) in enumerate(countries.items(), start=2):\n",
    "        print(f\"{i}/9: Fetching {country.upper()} data...\")\n",
    "        try:\n",
    "            nominal = fred.get_series(nominal_id, observation_start='2003-01-01')\n",
    "            cpi = fred.get_series(cpi_id, observation_start='2003-01-01')\n",
    "            country_data[country] = {'nominal': nominal, 'cpi': cpi}\n",
    "            print(f\"  -> {len(nominal)} nominal, {len(cpi)} CPI observations\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Failed to fetch {country}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # === 3. Fetch VIX (daily) ===\n",
    "    print(\"8/9: Fetching VIX (VIXCLS)...\")\n",
    "    vix = fred.get_series('VIXCLS', observation_start='2003-01-01')\n",
    "    print(f\"  -> {len(vix)} daily observations\")\n",
    "    \n",
    "    # === 4. Fetch Gold price for calendar alignment ===\n",
    "    print(\"9/9: Fetching Gold price (GC=F) for calendar...\")\n",
    "    gold = yf.download('GC=F', start='2003-01-01', progress=False)['Close']\n",
    "    print(f\"  -> {len(gold)} daily observations\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # === 5. Resample to monthly (month-start) ===\n",
    "    print(\"\\nStep 1: Resampling to month-start...\")\n",
    "    us_tips_monthly = us_tips.resample('MS').last()\n",
    "    vix_monthly = vix.resample('MS').mean()\n",
    "    gold_monthly = gold.resample('MS').last()\n",
    "    \n",
    "    # === 6. Build feature DataFrame ===\n",
    "    print(\"Step 2: Building feature matrix...\")\n",
    "    df = pd.DataFrame(index=us_tips_monthly.index)\n",
    "    \n",
    "    # US TIPS features\n",
    "    df['us_tips_level'] = us_tips_monthly\n",
    "    df['us_tips_change'] = us_tips_monthly.diff()\n",
    "    \n",
    "    # Country features\n",
    "    for country, data in country_data.items():\n",
    "        nominal = data['nominal']\n",
    "        cpi = data['cpi']\n",
    "        \n",
    "        df[f'{country}_nominal_level'] = nominal\n",
    "        df[f'{country}_nominal_change'] = nominal.diff()\n",
    "        df[f'{country}_cpi_lagged'] = cpi.shift(1)  # 1-month lag\n",
    "    \n",
    "    # Cross-country aggregates\n",
    "    country_list = list(country_data.keys())\n",
    "    level_cols = [f'{c}_nominal_level' for c in country_list]\n",
    "    change_cols = [f'{c}_nominal_change' for c in country_list]\n",
    "    cpi_cols = [f'{c}_cpi_lagged' for c in country_list]\n",
    "    \n",
    "    df['yield_dispersion'] = df[level_cols].std(axis=1)\n",
    "    df['yield_change_dispersion'] = df[change_cols].std(axis=1)\n",
    "    df['mean_cpi_change'] = df[cpi_cols].mean(axis=1)\n",
    "    df['us_vs_global_spread'] = df['us_tips_level'] - df[level_cols].mean(axis=1)\n",
    "    \n",
    "    # VIX\n",
    "    df['vix_monthly'] = vix_monthly\n",
    "    \n",
    "    # Gold (for alignment only, not a feature)\n",
    "    df['gold_price'] = gold_monthly\n",
    "    \n",
    "    # === 7. Handle missing data ===\n",
    "    print(\"Step 3: Cleaning missing data...\")\n",
    "    print(f\"  Before: {df.shape[0]} rows, {df.isna().sum().sum()} NaN values\")\n",
    "    \n",
    "    df = df.ffill(limit=3)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"  After: {df.shape[0]} rows, {df.isna().sum().sum()} NaN values\")\n",
    "    \n",
    "    # === 8. Validation ===\n",
    "    print(\"\\nStep 4: Data validation...\")\n",
    "    \n",
    "    # Feature count check\n",
    "    feature_cols = [c for c in df.columns if c != 'gold_price']\n",
    "    expected_features = 2 + (len(country_data) * 3) + 4 + 1  # 2 US + 18 countries + 4 agg + 1 VIX\n",
    "    actual_features = len(feature_cols)\n",
    "    \n",
    "    print(f\"  Features: {actual_features} (expected {expected_features})\")\n",
    "    if actual_features != expected_features:\n",
    "        print(f\"  WARNING: Feature count mismatch!\")\n",
    "    \n",
    "    # US synthetic vs TIPS correlation check (EXPECTED to be low ~0.49)\n",
    "    us_synthetic = df['germany_nominal_level'] - df['germany_cpi_lagged']  # Use Germany as proxy\n",
    "    corr = us_synthetic.corr(df['us_tips_level'])\n",
    "    print(f\"  Synthetic vs TIPS correlation: {corr:.3f} (expected ~0.49, low is OK)\")\n",
    "    \n",
    "    print(f\"  Date range: {df.index.min()} to {df.index.max()}\")\n",
    "    print(f\"  Total months: {len(df)}\")\n",
    "    \n",
    "    print(\"\\n[OK] Data fetching complete\")\n",
    "    return df, gold\n",
    "\n",
    "# Fetch data\n",
    "data_monthly, gold_calendar = fetch_multi_country_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: Dataset Class ===\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates sliding windows from monthly time series data.\n",
    "    Each sample is a window of shape [window_size, n_features].\n",
    "    \"\"\"\n",
    "    def __init__(self, data_array, window_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_array: np.ndarray of shape [n_months, n_features]\n",
    "            window_size: int, number of months in each window\n",
    "        \"\"\"\n",
    "        self.data = torch.FloatTensor(data_array)\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return window of shape [window_size, n_features]\n",
    "        window = self.data[idx:idx + self.window_size]\n",
    "        return window\n",
    "\n",
    "print(\"[OK] Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: Transformer Autoencoder Model ===\n",
    "\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for monthly time series.\"\"\"\n",
    "    def __init__(self, d_model, max_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiCountryTransformerAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact Transformer Autoencoder for multi-country real rate dynamics.\n",
    "    \n",
    "    Design principles:\n",
    "    - Small model (8-45K params) to prevent overfitting on ~253 samples\n",
    "    - Pre-norm Transformer (more stable training)\n",
    "    - Mean pooling for temporal aggregation\n",
    "    - Tanh on latent output to bound range\n",
    "    \n",
    "    Input: [batch, seq_len, n_features]\n",
    "    Latent: [batch, latent_dim]\n",
    "    Output: [batch, seq_len, n_features] (reconstruction)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=25, d_model=32, nhead=2, num_encoder_layers=2,\n",
    "                 dim_feedforward=64, latent_dim=4, dropout=0.3, max_seq_len=48):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(n_features, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_len, dropout=dropout)\n",
    "        \n",
    "        # Transformer encoder (pre-norm)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm for stability\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_encoder_layers,\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Latent projection with Tanh activation\n",
    "        self.latent_proj = nn.Sequential(\n",
    "            nn.Linear(d_model, latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.latent_expand = nn.Sequential(\n",
    "            nn.Linear(latent_dim, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.pos_decoder = PositionalEncoding(d_model, max_len=max_seq_len, dropout=dropout)\n",
    "        \n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerEncoder(\n",
    "            decoder_layer,\n",
    "            num_layers=max(1, num_encoder_layers - 1),\n",
    "            norm=nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.output_proj = nn.Linear(d_model, n_features)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"x: [batch, seq_len, n_features] -> z: [batch, latent_dim]\"\"\"\n",
    "        h = self.input_proj(x)\n",
    "        h = self.pos_encoder(h)\n",
    "        h = self.transformer_encoder(h)\n",
    "        h = h.mean(dim=1)  # Mean pooling over time\n",
    "        z = self.latent_proj(h)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z, seq_len):\n",
    "        \"\"\"z: [batch, latent_dim] -> reconstruction: [batch, seq_len, n_features]\"\"\"\n",
    "        batch_size = z.size(0)\n",
    "        h = self.latent_expand(z)\n",
    "        h = h.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        h = self.pos_decoder(h)\n",
    "        h = self.transformer_decoder(h)\n",
    "        reconstruction = self.output_proj(h)\n",
    "        return reconstruction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass for training\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        z = self.encode(x)\n",
    "        reconstruction = self.decode(z, seq_len)\n",
    "        return reconstruction, z\n",
    "    \n",
    "    def transform(self, x):\n",
    "        \"\"\"Generate latent features (inference mode)\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            z = self.encode(x)\n",
    "        return z\n",
    "\n",
    "print(\"[OK] Transformer Autoencoder model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: Training Function ===\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, latent = model(batch)\n",
    "        loss = nn.functional.mse_loss(reconstruction, batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Gradient clipping (essential for Transformers)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            reconstruction, latent = model(batch)\n",
    "            loss = nn.functional.mse_loss(reconstruction, batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"Full training loop with early stopping\"\"\"\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=50,\n",
    "        T_mult=2\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(config['max_epochs']):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        val_loss = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_train_loss = train_loss\n",
    "            patience_counter = 0\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config['patience']:\n",
    "                break\n",
    "    \n",
    "    # Restore best weights\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, {\n",
    "        'train_loss': best_train_loss,\n",
    "        'val_loss': best_val_loss,\n",
    "        'overfit_ratio': best_val_loss / (best_train_loss + 1e-10),\n",
    "        'epochs_trained': epoch + 1\n",
    "    }\n",
    "\n",
    "print(\"[OK] Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: Optuna HPO ===\n",
    "\n",
    "def create_dataloaders(features_standardized, window_size, batch_size, train_idx, val_idx, device):\n",
    "    \"\"\"Create train and validation dataloaders\"\"\"\n",
    "    train_data = features_standardized[:train_idx].values\n",
    "    val_data = features_standardized[train_idx:val_idx].values\n",
    "    \n",
    "    train_dataset = SlidingWindowDataset(train_data, window_size)\n",
    "    val_dataset = SlidingWindowDataset(val_data, window_size)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def run_optuna_hpo(features_standardized, train_idx, val_idx, n_features, device, n_trials=30, timeout=3600):\n",
    "    \"\"\"Run Optuna hyperparameter optimization\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Trials: {n_trials}\")\n",
    "    print(f\"Timeout: {timeout}s\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Sample hyperparameters\n",
    "        window_size = trial.suggest_categorical('window_size', [12, 18, 24])\n",
    "        d_model = trial.suggest_categorical('d_model', [24, 32, 48])\n",
    "        nhead = trial.suggest_categorical('nhead', [2, 4])\n",
    "        num_encoder_layers = trial.suggest_categorical('num_encoder_layers', [2, 3])\n",
    "        latent_dim = trial.suggest_categorical('latent_dim', [4, 5, 6])\n",
    "        dropout = trial.suggest_float('dropout', 0.3, 0.5)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 5e-5, 5e-4, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-3, 5e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader, val_loader = create_dataloaders(\n",
    "            features_standardized, window_size, batch_size, train_idx, val_idx, device\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = MultiCountryTransformerAutoencoder(\n",
    "            n_features=n_features,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            dim_feedforward=2 * d_model,\n",
    "            latent_dim=latent_dim,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=window_size\n",
    "        ).to(device)\n",
    "        \n",
    "        config = {\n",
    "            'max_epochs': 200,\n",
    "            'patience': 20,\n",
    "            'learning_rate': learning_rate,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(config['max_epochs']):\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "            val_loss = evaluate(model, val_loader, device)\n",
    "            \n",
    "            # Report to Optuna for pruning\n",
    "            trial.report(val_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss - 1e-6:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= config['patience']:\n",
    "                    break\n",
    "        \n",
    "        return best_val_loss\n",
    "    \n",
    "    # Create study with MedianPruner\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        pruner=MedianPruner(\n",
    "            n_startup_trials=7,\n",
    "            n_warmup_steps=30\n",
    "        ),\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPTUNA RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best value: {study.best_value:.6f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "    print(f\"Completed trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n",
    "    print(f\"Pruned trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "    \n",
    "    return study.best_params, study.best_value, len(study.trials)\n",
    "\n",
    "print(\"[OK] Optuna HPO function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: Main Execution ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MAIN EXECUTION: REAL_RATE ATTEMPT 3\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Started: {datetime.now().isoformat()}\")\n",
    "\n",
    "# === 1. Prepare features (exclude gold_price column) ===\n",
    "feature_cols = [c for c in data_monthly.columns if c != 'gold_price']\n",
    "features = data_monthly[feature_cols].copy()\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "print(f\"\\nFeatures: {n_features} columns\")\n",
    "print(f\"Samples: {len(features)} months\")\n",
    "print(f\"Date range: {features.index.min()} to {features.index.max()}\")\n",
    "\n",
    "# === 2. Time-series split (70/15/15) ===\n",
    "n_samples = len(features)\n",
    "train_idx = int(n_samples * 0.70)\n",
    "val_idx = int(n_samples * 0.85)\n",
    "\n",
    "print(f\"\\nSplit: train={train_idx}, val={val_idx-train_idx}, test={n_samples-val_idx}\")\n",
    "\n",
    "# === 3. Standardization (fit on train only) ===\n",
    "train_mean = features.iloc[:train_idx].mean()\n",
    "train_std = features.iloc[:train_idx].std()\n",
    "\n",
    "features_standardized = (features - train_mean) / (train_std + 1e-8)\n",
    "\n",
    "print(\"\\n[OK] Features standardized (train statistics)\")\n",
    "\n",
    "# === 4. Run Optuna HPO ===\n",
    "best_params, best_value, n_trials_completed = run_optuna_hpo(\n",
    "    features_standardized,\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    n_features,\n",
    "    device,\n",
    "    n_trials=30,\n",
    "    timeout=3600\n",
    ")\n",
    "\n",
    "# === 5. Final training with best params ===\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "window_size = best_params['window_size']\n",
    "batch_size = best_params['batch_size']\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    features_standardized, window_size, batch_size, train_idx, val_idx, device\n",
    ")\n",
    "\n",
    "# Create final model\n",
    "final_model = MultiCountryTransformerAutoencoder(\n",
    "    n_features=n_features,\n",
    "    d_model=best_params['d_model'],\n",
    "    nhead=best_params['nhead'],\n",
    "    num_encoder_layers=best_params['num_encoder_layers'],\n",
    "    dim_feedforward=2 * best_params['d_model'],\n",
    "    latent_dim=best_params['latent_dim'],\n",
    "    dropout=best_params['dropout'],\n",
    "    max_seq_len=window_size\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in final_model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "\n",
    "config = {\n",
    "    'max_epochs': 200,\n",
    "    'patience': 20,\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'weight_decay': best_params['weight_decay']\n",
    "}\n",
    "\n",
    "# Train\n",
    "final_model, metrics = train_model(final_model, train_loader, val_loader, config, device)\n",
    "\n",
    "print(\"\\n[OK] Final model trained\")\n",
    "print(f\"Train loss: {metrics['train_loss']:.6f}\")\n",
    "print(f\"Val loss: {metrics['val_loss']:.6f}\")\n",
    "print(f\"Overfit ratio: {metrics['overfit_ratio']:.3f}\")\n",
    "print(f\"Epochs: {metrics['epochs_trained']}\")\n",
    "\n",
    "# === 6. Generate latent features for ALL data ===\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING SUBMODEL OUTPUT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_model.eval()\n",
    "all_latents = []\n",
    "monthly_dates = []\n",
    "\n",
    "# Create dataset from all standardized features\n",
    "all_dataset = SlidingWindowDataset(features_standardized.values, window_size)\n",
    "all_loader = DataLoader(all_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in all_loader:\n",
    "        batch = batch.to(device)\n",
    "        latent = final_model.encode(batch)\n",
    "        all_latents.append(latent.cpu().numpy())\n",
    "\n",
    "all_latents = np.vstack(all_latents)\n",
    "print(f\"\\nRaw latent shape: {all_latents.shape}\")\n",
    "\n",
    "# Align dates (windows start at index window_size-1)\n",
    "monthly_dates = features.index[window_size-1:window_size-1+len(all_latents)]\n",
    "\n",
    "# === 7. Apply first-difference postprocessing (at monthly level) ===\n",
    "print(\"\\nApplying first-difference postprocessing...\")\n",
    "latent_diff = np.diff(all_latents, axis=0)\n",
    "# Prepend NaN for first window\n",
    "latent_output = np.vstack([np.full((1, all_latents.shape[1]), np.nan), latent_diff])\n",
    "\n",
    "# Create monthly output DataFrame\n",
    "output_monthly = pd.DataFrame(\n",
    "    latent_output,\n",
    "    index=monthly_dates,\n",
    "    columns=[f'real_rate_sem_{i}' for i in range(best_params['latent_dim'])]\n",
    ")\n",
    "\n",
    "print(f\"Monthly output shape: {output_monthly.shape}\")\n",
    "\n",
    "# === 8. Expand to daily using forward-fill ===\n",
    "print(\"\\nExpanding to daily frequency (forward-fill)...\")\n",
    "\n",
    "# Get gold trading calendar for alignment\n",
    "gold_daily_dates = gold_calendar.index\n",
    "date_range = pd.date_range(\n",
    "    output_monthly.index.min(),\n",
    "    output_monthly.index.max(),\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# Reindex to daily and forward-fill\n",
    "output_daily = output_monthly.reindex(date_range).ffill()\n",
    "\n",
    "# Align to gold calendar (keep only trading days)\n",
    "output_daily = output_daily.reindex(gold_daily_dates).ffill()\n",
    "\n",
    "# Drop any remaining NaN rows\n",
    "output_daily = output_daily.dropna()\n",
    "\n",
    "print(f\"Daily output shape: {output_daily.shape}\")\n",
    "print(f\"Date range: {output_daily.index.min()} to {output_daily.index.max()}\")\n",
    "\n",
    "# === 9. Save outputs ===\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING OUTPUTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save submodel output (daily)\n",
    "output_daily.to_csv('submodel_output.csv')\n",
    "print(\"[OK] Saved: submodel_output.csv\")\n",
    "\n",
    "# Save model weights\n",
    "torch.save({\n",
    "    'model_state': final_model.state_dict(),\n",
    "    'config': best_params,\n",
    "    'train_mean': train_mean.to_dict(),\n",
    "    'train_std': train_std.to_dict()\n",
    "}, 'model.pt')\n",
    "print(\"[OK] Saved: model.pt\")\n",
    "\n",
    "# Save training results\n",
    "result = {\n",
    "    'feature': 'real_rate',\n",
    "    'attempt': 3,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'architecture': 'MultiCountryTransformerAutoencoder',\n",
    "    'device': str(device),\n",
    "    'best_params': best_params,\n",
    "    'metrics': metrics,\n",
    "    'model_parameters': n_params,\n",
    "    'optuna_trials_completed': n_trials_completed,\n",
    "    'optuna_best_value': float(best_value),\n",
    "    'output_shape': list(output_daily.shape),\n",
    "    'output_columns': list(output_daily.columns),\n",
    "    'data_info': {\n",
    "        'n_features': n_features,\n",
    "        'n_samples_monthly': len(features),\n",
    "        'train_samples': train_idx,\n",
    "        'val_samples': val_idx - train_idx,\n",
    "        'test_samples': n_samples - val_idx,\n",
    "        'window_size': window_size,\n",
    "        'output_samples_daily': len(output_daily)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print(\"[OK] Saved: training_result.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Finished: {datetime.now().isoformat()}\")\n",
    "print(f\"\\nOutput summary:\")\n",
    "print(f\"  - Submodel output: {output_daily.shape[0]} rows x {output_daily.shape[1]} columns\")\n",
    "print(f\"  - Latent dimensions: {best_params['latent_dim']}\")\n",
    "print(f\"  - Model parameters: {n_params:,}\")\n",
    "print(f\"  - Overfit ratio: {metrics['overfit_ratio']:.3f}\")\n",
    "print(\"\\n[SUCCESS] All files saved to Kaggle output directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
