{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-0",
   "metadata": {},
   "source": [
    "# Gold SubModel: Yield Curve - Attempt 4\n",
    "**Approach**: Pure deterministic feature engineering - acceleration + structural decomposition (2nd-order dynamics)\n",
    "\n",
    "**Key differences from Attempt 2/3**:\n",
    "- No HMM, no PyTorch, no neural networks\n",
    "- Attempt 2 captured 'how fast' the curve moves (1st derivative)\n",
    "- Attempt 4 captures 'how the movement is accelerating', 'which end is driving', and 'how volatility structure is shifting' (2nd derivative + structural)\n",
    "\n",
    "**Output features**:\n",
    "1. `yc_spread_accel_z`: Z-score of 2nd derivative of 10Y-3M spread (autocorr=-0.496)\n",
    "2. `yc_curv_change_z`: Z-score of daily change in 2Y-5Y-10Y butterfly (autocorr=-0.149)\n",
    "3. `yc_mom_divergence_z`: Difference of z-scored long-end vs short-end momentum (autocorr=0.733)\n",
    "4. `yc_vol_ratio_chg_z`: Z-score of daily CHANGE in (DGS3MO vol / DGS10 vol) (autocorr=0.121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print('=== Gold SubModel Training: yield_curve attempt 4 ===')\n",
    "print('Approach: Acceleration + Structural Decomposition (2nd-order dynamics)')\n",
    "print(f'Started: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# FRED API key via Kaggle Secrets (with graceful fallback to public endpoint)\nFRED_API_KEY = None\ntry:\n    from kaggle_secrets import UserSecretsClient\n    FRED_API_KEY = UserSecretsClient().get_secret('FRED_API_KEY')\n    print('FRED_API_KEY loaded from Kaggle Secrets')\nexcept Exception:\n    FRED_API_KEY = os.environ.get('FRED_API_KEY')\n    if FRED_API_KEY:\n        print('FRED_API_KEY loaded from environment')\n    else:\n        print('FRED_API_KEY not found - will use public FRED CSV endpoint (no key required)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic path resolution for bigbigzabuton/gold-prediction-submodels\n",
    "candidates = [\n",
    "    '/kaggle/input/gold-prediction-submodels',\n",
    "    '/kaggle/input/datasets/bigbigzabuton/gold-prediction-submodels',\n",
    "]\n",
    "DATASET_PATH = None\n",
    "for c in candidates:\n",
    "    if os.path.exists(c):\n",
    "        DATASET_PATH = c\n",
    "        break\n",
    "if DATASET_PATH is None:\n",
    "    raise RuntimeError(f'Dataset not found. Tried: {candidates}')\n",
    "print(f'Dataset path: {DATASET_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "START = '2014-10-01'\nEND = '2026-01-31'\nTICKERS = ['DGS10', 'DGS2', 'DGS5', 'DGS3MO']\n\n# Fetch yield series from FRED (API key path or public CSV fallback)\nseries = {}\nif FRED_API_KEY:\n    from fredapi import Fred\n    fred = Fred(api_key=FRED_API_KEY)\n    for ticker in TICKERS:\n        s = fred.get_series(ticker, observation_start=START, observation_end=END)\n        s = s.ffill(limit=3)\n        series[ticker] = s\n        print(f'{ticker}: {len(s.dropna())} obs (via fredapi)')\nelse:\n    # Public FRED CSV endpoint â€” no API key required\n    for ticker in TICKERS:\n        url = f'https://fred.stlouisfed.org/graph/fredgraph.csv?id={ticker}'\n        s = pd.read_csv(url, index_col=0, parse_dates=True, na_values='.')\n        s = s.iloc[:, 0]  # single value column\n        s.name = ticker\n        s = s[(s.index >= pd.Timestamp(START)) & (s.index <= pd.Timestamp(END))]\n        s = s.ffill(limit=3)\n        series[ticker] = s\n        print(f'{ticker}: {len(s.dropna())} obs (via public CSV)')\n\n# Build aligned DataFrame\ndf = pd.DataFrame(series)\ndf.index = pd.to_datetime(df.index)\ndf = df.dropna()\nprint(f'Combined yields: {len(df)} obs, {df.index[0].date()} to {df.index[-1].date()}')\n\n# Fetch gold price from Yahoo Finance\ngold = yf.download('GC=F', start=START, end=END, auto_adjust=True, progress=False)\nif gold.empty or len(gold) < 100:\n    raise ValueError(f'GC=F download returned insufficient data: {len(gold)} rows')\n\nif isinstance(gold.columns, pd.MultiIndex):\n    gold_close = gold['Close'].iloc[:, 0]\nelse:\n    gold_close = gold['Close'].squeeze()\n\ngold_ret = gold_close.pct_change() * 100\ngold_ret_next = gold_ret.shift(-1)  # next-day return (target)\ngold_ret_next.name = 'gold_return_next'\ngold_ret_next.index = pd.to_datetime(gold_ret_next.index)\n\n# Align yield data with gold target\ncommon_idx = df.index.intersection(gold_ret_next.index)\ndf = df.loc[common_idx]\ngold_ret_next = gold_ret_next.loc[common_idx]\nprint(f'Aligned: {len(df)} obs')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_zscore(x, window):\n",
    "    \"\"\"Rolling z-score with NaN-safe handling.\"\"\"\n",
    "    min_p = max(window // 2, 10)\n",
    "    m = x.rolling(window, min_periods=min_p).mean()\n",
    "    s = x.rolling(window, min_periods=min_p).std()\n",
    "    z = (x - m) / s.replace(0, np.nan)\n",
    "    return z.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "def generate_spread_accel(dgs10, dgs3mo, zscore_window):\n",
    "    \"\"\"Second derivative of 10Y-3M spread, z-scored.\n",
    "    Positive = steepening accelerating (or flattening decelerating).\n",
    "    Negative = flattening accelerating.\n",
    "    Autocorr ~ -0.496 (mean-reverting, no Gate 1 risk).\n",
    "    \"\"\"\n",
    "    spread = dgs10 - dgs3mo\n",
    "    velocity = spread.diff()\n",
    "    accel = velocity.diff()\n",
    "    z = rolling_zscore(accel, zscore_window)\n",
    "    return z.clip(-4, 4)\n",
    "\n",
    "\n",
    "def generate_curv_change(dgs5, dgs2, dgs10, zscore_window):\n",
    "    \"\"\"Z-score of daily change in 2Y-5Y-10Y butterfly (curvature).\n",
    "    Positive = belly bowing outward.\n",
    "    Negative = belly flattening.\n",
    "    Autocorr ~ -0.149 (near white noise, no Gate 1 risk).\n",
    "    \"\"\"\n",
    "    curvature = 2 * dgs5 - dgs10 - dgs2\n",
    "    curv_change = curvature.diff()\n",
    "    z = rolling_zscore(curv_change, zscore_window)\n",
    "    return z.clip(-4, 4)\n",
    "\n",
    "\n",
    "def generate_mom_divergence(dgs10, dgs3mo, momentum_window, zscore_window):\n",
    "    \"\"\"Difference between long-end and short-end momentum z-scores.\n",
    "    Positive = long-end rising faster (bear steepening / term premium).\n",
    "    Negative = short-end rising faster (policy tightening).\n",
    "    momentum_window MUST NOT exceed 10 (autocorr rises to 0.92 at window=20).\n",
    "    \"\"\"\n",
    "    dgs10_mom = dgs10.diff().rolling(momentum_window, min_periods=1).sum()\n",
    "    dgs3mo_mom = dgs3mo.diff().rolling(momentum_window, min_periods=1).sum()\n",
    "    dgs10_mom_z = rolling_zscore(dgs10_mom, zscore_window)\n",
    "    dgs3mo_mom_z = rolling_zscore(dgs3mo_mom, zscore_window)\n",
    "    divergence = dgs10_mom_z - dgs3mo_mom_z\n",
    "    return divergence.clip(-6, 6)\n",
    "\n",
    "\n",
    "def generate_vol_ratio_chg(dgs10, dgs3mo, vol_window, zscore_window):\n",
    "    \"\"\"Z-score of daily CHANGE in (short-end vol / long-end vol ratio).\n",
    "    Using CHANGE (not level) is critical - level has autocorr ~0.95+.\n",
    "    Positive = shift toward policy uncertainty (short-end vol rising).\n",
    "    Negative = shift toward term premium uncertainty (long-end vol rising).\n",
    "    Autocorr ~ 0.121 (near white noise, no Gate 1 risk).\n",
    "    \"\"\"\n",
    "    min_p = max(vol_window // 2, 5)\n",
    "    dgs10_vol = dgs10.diff().abs().rolling(vol_window, min_periods=min_p).mean()\n",
    "    dgs3mo_vol = dgs3mo.diff().abs().rolling(vol_window, min_periods=min_p).mean()\n",
    "    vol_ratio = dgs3mo_vol / dgs10_vol.replace(0, np.nan)\n",
    "    vol_ratio_change = vol_ratio.diff()\n",
    "    z = rolling_zscore(vol_ratio_change, zscore_window)\n",
    "    return z.clip(-4, 4)\n",
    "\n",
    "\n",
    "def compute_mi(feature, target, n_bins=20):\n",
    "    \"\"\"MI between feature and target using quantile binning.\"\"\"\n",
    "    valid = feature.dropna().index.intersection(target.dropna().index)\n",
    "    if len(valid) < 50:\n",
    "        return 0.0\n",
    "    f = feature[valid]\n",
    "    t = target[valid]\n",
    "    try:\n",
    "        f_binned = pd.qcut(f, q=n_bins, labels=False, duplicates='drop')\n",
    "        t_binned = pd.qcut(t, q=n_bins, labels=False, duplicates='drop')\n",
    "        valid2 = f_binned.notna() & t_binned.notna()\n",
    "        if valid2.sum() < 50:\n",
    "            return 0.0\n",
    "        return float(mutual_info_score(f_binned[valid2], t_binned[valid2]))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "print('Feature generation functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base_features for date alignment\n",
    "base_path = os.path.join(DATASET_PATH, 'base_features.csv')\n",
    "base = pd.read_csv(base_path, index_col=0, parse_dates=True)\n",
    "TARGET_DATES = base.index\n",
    "\n",
    "# Filter to target dates\n",
    "df_aligned = df[df.index.isin(TARGET_DATES)].copy()\n",
    "gold_aligned = gold_ret_next[gold_ret_next.index.isin(TARGET_DATES)].copy()\n",
    "common = df_aligned.index.intersection(gold_aligned.index)\n",
    "df_aligned = df_aligned.loc[common]\n",
    "gold_aligned = gold_aligned.loc[common]\n",
    "\n",
    "n = len(df_aligned)\n",
    "train_end_idx = int(n * 0.70)\n",
    "val_end_idx = int(n * 0.85)\n",
    "\n",
    "train_mask = pd.Series(False, index=df_aligned.index)\n",
    "val_mask = pd.Series(False, index=df_aligned.index)\n",
    "test_mask = pd.Series(False, index=df_aligned.index)\n",
    "train_mask.iloc[:train_end_idx] = True\n",
    "val_mask.iloc[train_end_idx:val_end_idx] = True\n",
    "test_mask.iloc[val_end_idx:] = True\n",
    "\n",
    "print(f'Total: {n}, Train: {train_mask.sum()}, Val: {val_mask.sum()}, Test: {test_mask.sum()}')\n",
    "print(f'Train: {df_aligned.index[0].date()} to {df_aligned.index[train_end_idx-1].date()}')\n",
    "print(f'Val:   {df_aligned.index[train_end_idx].date()} to {df_aligned.index[val_end_idx-1].date()}')\n",
    "print(f'Test:  {df_aligned.index[val_end_idx].date()} to {df_aligned.index[-1].date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract aligned yield series for feature generation\n",
    "dgs10 = df_aligned['DGS10']\n",
    "dgs2 = df_aligned['DGS2']\n",
    "dgs5 = df_aligned['DGS5']\n",
    "dgs3mo = df_aligned['DGS3MO']\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    accel_zw = trial.suggest_categorical('accel_zscore_window', [30, 45, 60, 90])\n",
    "    curv_zw = trial.suggest_categorical('curv_zscore_window', [30, 45, 60, 90])\n",
    "    mom_w = trial.suggest_categorical('momentum_window', [3, 5, 7, 10])\n",
    "    mom_zw = trial.suggest_categorical('mom_zscore_window', [30, 45, 60, 90])\n",
    "    vol_w = trial.suggest_categorical('vol_window', [10, 15, 20, 30])\n",
    "    vol_zw = trial.suggest_categorical('vol_zscore_window', [30, 45, 60, 90])\n",
    "\n",
    "    f1 = generate_spread_accel(dgs10, dgs3mo, accel_zw)\n",
    "    f2 = generate_curv_change(dgs5, dgs2, dgs10, curv_zw)\n",
    "    f3 = generate_mom_divergence(dgs10, dgs3mo, mom_w, mom_zw)\n",
    "    f4 = generate_vol_ratio_chg(dgs10, dgs3mo, vol_w, vol_zw)\n",
    "\n",
    "    target_val = gold_aligned[val_mask]\n",
    "    mi_sum = sum(compute_mi(feat[val_mask], target_val) for feat in [f1, f2, f3, f4])\n",
    "    return mi_sum\n",
    "\n",
    "\n",
    "print('Running Optuna HPO (50 trials, timeout=600s)...')\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(objective, n_trials=50, timeout=600)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'Optuna complete: {len(study.trials)} trials')\n",
    "print(f'Best params: {best_params}')\n",
    "print(f'Best MI sum (val): {study.best_value:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final features with best Optuna parameters\n",
    "f1 = generate_spread_accel(dgs10, dgs3mo, best_params['accel_zscore_window'])\n",
    "f2 = generate_curv_change(dgs5, dgs2, dgs10, best_params['curv_zscore_window'])\n",
    "f3 = generate_mom_divergence(dgs10, dgs3mo, best_params['momentum_window'], best_params['mom_zscore_window'])\n",
    "f4 = generate_vol_ratio_chg(dgs10, dgs3mo, best_params['vol_window'], best_params['vol_zscore_window'])\n",
    "\n",
    "features_dict = {\n",
    "    'yc_spread_accel_z': f1,\n",
    "    'yc_curv_change_z': f2,\n",
    "    'yc_mom_divergence_z': f3,\n",
    "    'yc_vol_ratio_chg_z': f4,\n",
    "}\n",
    "\n",
    "# Gate 1 safety: autocorrelation check (threshold 0.95)\n",
    "print('Autocorrelations (Gate 1 threshold: 0.95):')\n",
    "autocorr_results = {}\n",
    "for name, feat in features_dict.items():\n",
    "    train_feat = feat[train_mask].dropna()\n",
    "    if len(train_feat) > 1:\n",
    "        ac = float(np.corrcoef(train_feat.values[:-1], train_feat.values[1:])[0, 1])\n",
    "    else:\n",
    "        ac = 0.0\n",
    "    autocorr_results[name] = round(ac, 4)\n",
    "    status = 'WARN' if abs(ac) > 0.95 else 'OK'\n",
    "    print(f'  {name}: {ac:.4f} [{status}]')\n",
    "\n",
    "# Build output DataFrame (full date range, forward-filled)\n",
    "output = pd.DataFrame(features_dict, index=df_aligned.index)\n",
    "output = output.ffill(limit=5)\n",
    "\n",
    "print(f'\\nOutput shape: {output.shape}')\n",
    "print(f'NaN counts: {output.isna().sum().to_dict()}')\n",
    "print('\\nOutput summary:')\n",
    "print(output.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MI on test set for final evaluation\n",
    "test_mi = {}\n",
    "for name, feat in features_dict.items():\n",
    "    mi = compute_mi(feat[test_mask], gold_aligned[test_mask])\n",
    "    test_mi[name] = round(mi, 6)\n",
    "\n",
    "print('Test set MI per feature:')\n",
    "for name, mi in test_mi.items():\n",
    "    print(f'  {name}: {mi:.6f}')\n",
    "print(f'  Test MI sum: {sum(test_mi.values()):.6f}')\n",
    "\n",
    "# Validation set MI (re-compute from final features for record)\n",
    "val_mi = {}\n",
    "for name, feat in features_dict.items():\n",
    "    mi = compute_mi(feat[val_mask], gold_aligned[val_mask])\n",
    "    val_mi[name] = round(mi, 6)\n",
    "\n",
    "print('\\nVal set MI per feature:')\n",
    "for name, mi in val_mi.items():\n",
    "    print(f'  {name}: {mi:.6f}')\n",
    "print(f'  Val MI sum: {sum(val_mi.values()):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output.to_csv('/kaggle/working/submodel_output.csv')\n",
    "print('Saved: submodel_output.csv')\n",
    "\n",
    "result = {\n",
    "    'feature': 'yield_curve',\n",
    "    'attempt': 4,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_params': best_params,\n",
    "    'metrics': {\n",
    "        'mi_sum_val': round(study.best_value, 6),\n",
    "        'mi_individual_val': val_mi,\n",
    "        'mi_individual_test': test_mi,\n",
    "        'mi_sum_test': round(sum(test_mi.values()), 6),\n",
    "        'autocorr': autocorr_results,\n",
    "        'optuna_best_value': round(study.best_value, 6),\n",
    "        'optuna_trials_completed': len(study.trials),\n",
    "    },\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': list(output.columns),\n",
    "    'data_info': {\n",
    "        'total_samples': n,\n",
    "        'train_samples': int(train_mask.sum()),\n",
    "        'val_samples': int(val_mask.sum()),\n",
    "        'test_samples': int(test_mask.sum()),\n",
    "        'date_range_start': str(df_aligned.index.min().date()),\n",
    "        'date_range_end': str(df_aligned.index.max().date()),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/training_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "print('Saved: training_result.json')\n",
    "\n",
    "print('\\n=== Training complete! ===')\n",
    "print(f'Finished: {datetime.now().isoformat()}')\n",
    "print(f'Output columns: {list(output.columns)}')\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(json.dumps(result, indent=2, default=str))"
   ]
  }
 ]
}