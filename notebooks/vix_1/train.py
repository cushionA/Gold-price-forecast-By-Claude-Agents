"""
Gold Prediction SubModel Training - VIX Attempt 1
Self-contained: Data fetch -> Preprocessing -> HMM + Z-Score + Autocorr -> Optuna HPO -> Save results
Generated by builder_model agent
"""

# ============================================================
# 1. IMPORTS
# ============================================================
import numpy as np
import pandas as pd
import json
import os
from datetime import datetime
from sklearn.mixture import GaussianMixture
from sklearn.metrics import mutual_info_score
import optuna

# Set random seeds for reproducibility
np.random.seed(42)

print(f"=== Gold SubModel Training: VIX attempt 1 ===")
print(f"Started: {datetime.now().isoformat()}")

# ============================================================
# 2. DATA FETCHING
# ============================================================
def fetch_data():
    """
    Fetch VIX data from FRED or Yahoo Finance as fallback.
    Returns DataFrame with columns: date, vix, vix_log_change
    """
    print("\n=== Data Fetching ===")

    # Try FRED first
    try:
        from kaggle_secrets import UserSecretsClient
        secrets = UserSecretsClient()
        FRED_API_KEY = secrets.get_secret("FRED_API_KEY")

        from fredapi import Fred
        fred = Fred(api_key=FRED_API_KEY)

        # Fetch VIXCLS (start from 2014-10-01 for warmup)
        vix_series = fred.get_series('VIXCLS', observation_start='2014-10-01')

        df = pd.DataFrame({
            'date': vix_series.index,
            'vix': vix_series.values
        })
        df['date'] = pd.to_datetime(df['date'])

        print(f"Fetched {len(df)} rows from FRED (VIXCLS)")

    except Exception as e:
        print(f"FRED fetch failed: {e}")
        print("Falling back to Yahoo Finance...")

        import yfinance as yf

        # Fetch ^VIX from Yahoo Finance
        vix_df = yf.download('^VIX', start='2014-10-01', progress=False)

        # Handle both single-ticker (Series) and multi-ticker (DataFrame with MultiIndex) formats
        if 'Close' in vix_df.columns:
            close_series = vix_df['Close']
        elif isinstance(vix_df.columns, pd.MultiIndex):
            # MultiIndex case: ('Close', '^VIX')
            close_series = vix_df[('Close', '^VIX')]
        else:
            # Fallback: assume single column
            close_series = vix_df.iloc[:, 0]

        # Ensure it's a Series
        if isinstance(close_series, pd.DataFrame):
            close_series = close_series.iloc[:, 0]

        df = pd.DataFrame({
            'date': vix_df.index,
            'vix': close_series.values
        })
        df['date'] = pd.to_datetime(df['date'])
        df = df.reset_index(drop=True)

        print(f"Fetched {len(df)} rows from Yahoo Finance (^VIX)")

    # Remove any NaN values
    df = df.dropna(subset=['vix'])

    # Forward-fill gaps up to 3 days (pandas 2.x compatible)
    df = df.set_index('date').asfreq('D').fillna(method='ffill', limit=3).reset_index()
    df = df.dropna(subset=['vix'])

    # Compute log-changes
    df['vix_log_change'] = np.log(df['vix']).diff()

    # Drop first row (NaN log_change)
    df = df.dropna(subset=['vix_log_change']).reset_index(drop=True)

    print(f"Final data: {len(df)} rows, date range: {df['date'].min()} to {df['date'].max()}")
    print(f"VIX range: {df['vix'].min():.2f} to {df['vix'].max():.2f}")
    print(f"Log-change std: {df['vix_log_change'].std():.4f}")

    return df

# ============================================================
# 3. FEATURE GENERATION FUNCTIONS
# ============================================================

def generate_regime_feature(vix_log_changes, n_components, covariance_type, n_init, train_size):
    """
    Fit Gaussian Mixture Model on train portion and return P(highest-variance component) for full data.
    Using GMM as a simpler alternative to HMM (no hmmlearn dependency).
    """
    X_train = vix_log_changes[:train_size].reshape(-1, 1)
    X_full = vix_log_changes.reshape(-1, 1)

    # Fit GMM on training data
    gmm = GaussianMixture(
        n_components=n_components,
        covariance_type=covariance_type,
        max_iter=100,
        tol=1e-4,
        random_state=42,
        n_init=n_init
    )

    gmm.fit(X_train)

    # Predict probabilities for full data
    probs = gmm.predict_proba(X_full)

    # Identify highest-variance component
    if covariance_type == 'full':
        state_vars = [float(cov[0, 0]) for cov in gmm.covariances_]
    elif covariance_type == 'diag':
        state_vars = [float(cov[0]) for cov in gmm.covariances_]
    else:  # 'spherical' or 'tied'
        state_vars = [float(cov) for cov in gmm.covariances_]

    high_var_component = np.argmax(state_vars)

    return probs[:, high_var_component]

def generate_zscore_feature(vix_levels, window):
    """
    Rolling z-score: (VIX - rolling_mean) / rolling_std
    Clipped to [-4, 4] for stability.
    """
    s = pd.Series(vix_levels)
    rolling_mean = s.rolling(window, min_periods=window).mean()
    rolling_std = s.rolling(window, min_periods=window).std()
    z = (s - rolling_mean) / rolling_std
    z = z.clip(-4, 4)
    return z.values

def generate_persistence_feature(vix_log_changes, window):
    """
    Rolling lag-1 autocorrelation of log-VIX changes.
    High = persistent (sustained regime), Low = transient (spike reverting).
    """
    s = pd.Series(vix_log_changes)

    def autocorr_lag1(x):
        if len(x) < 2:
            return np.nan
        x_series = pd.Series(x)
        return x_series.autocorr(lag=1)

    acorr = s.rolling(window, min_periods=window).apply(autocorr_lag1, raw=True)
    return acorr.values

# ============================================================
# 4. OPTUNA OBJECTIVE
# ============================================================

def objective(trial, vix_log_changes, vix_levels, target, train_size, val_mask):
    """
    Maximize sum of mutual information between 3 output columns and gold_return on validation set.
    """
    # Suggest hyperparameters
    n_components = trial.suggest_categorical('hmm_n_components', [2, 3])
    covariance_type = trial.suggest_categorical('hmm_covariance_type', ['full', 'diag'])
    n_init = trial.suggest_categorical('hmm_n_init', [3, 5, 10])
    zscore_window = trial.suggest_categorical('zscore_window', [40, 60, 90])
    autocorr_window = trial.suggest_categorical('autocorr_window', [15, 20, 30])

    try:
        # Generate features
        regime = generate_regime_feature(
            vix_log_changes, n_components, covariance_type, n_init, train_size
        )
        zscore = generate_zscore_feature(vix_levels, zscore_window)
        persistence = generate_persistence_feature(vix_log_changes, autocorr_window)

        # Extract validation period
        regime_val = regime[val_mask]
        zscore_val = zscore[val_mask]
        persist_val = persistence[val_mask]
        target_val = target[val_mask]

        # Helper function to discretize continuous variables
        def discretize(x, bins=20):
            valid = ~np.isnan(x)
            if valid.sum() < bins:
                return None
            x_clean = x.copy()
            x_clean[~valid] = np.nanmedian(x)
            try:
                return pd.qcut(x_clean, bins, labels=False, duplicates='drop')
            except:
                return pd.cut(x_clean, bins, labels=False, duplicates='drop')

        # Compute MI for each feature
        mi_sum = 0.0

        for feat_val, feat_name in [(regime_val, 'regime'), (zscore_val, 'zscore'), (persist_val, 'persistence')]:
            # Create joint valid mask
            mask = ~np.isnan(feat_val) & ~np.isnan(target_val)

            if mask.sum() > 50:
                feat_disc = discretize(feat_val[mask], bins=20)
                tgt_disc = discretize(target_val[mask], bins=20)

                if feat_disc is not None and tgt_disc is not None:
                    mi = mutual_info_score(feat_disc, tgt_disc)
                    mi_sum += mi

        return mi_sum

    except Exception as e:
        print(f"Trial failed: {e}")
        return 0.0

# ============================================================
# 5. MAIN EXECUTION
# ============================================================

# Fetch data
data_df = fetch_data()

# Load target variable
print("\n=== Loading Target Variable ===")
try:
    target_df = pd.read_csv('/kaggle/input/gold-price-target/target.csv')
    target_df['date'] = pd.to_datetime(target_df['date'])

    # Handle both column name possibilities
    if 'gold_return_next' in target_df.columns:
        target_col = 'gold_return_next'
    elif 'gold_return' in target_df.columns:
        target_col = 'gold_return'
    else:
        raise ValueError(f"Target file must contain 'gold_return_next' or 'gold_return' column. Found: {target_df.columns.tolist()}")

    print(f"Target loaded: {len(target_df)} rows, using column '{target_col}'")

except Exception as e:
    print(f"Error loading target: {e}")
    raise

# Merge data with target
merged_df = pd.merge(data_df, target_df[['date', target_col]], on='date', how='inner')
print(f"Merged data: {len(merged_df)} rows")

# Extract arrays for processing
vix_log_changes = merged_df['vix_log_change'].values
vix_levels = merged_df['vix'].values
target = merged_df[target_col].values
dates = merged_df['date'].values

# Data split: train/val/test = 70/15/15 (time-series order)
n_total = len(merged_df)
n_train = int(n_total * 0.70)
n_val = int(n_total * 0.15)
n_test = n_total - n_train - n_val

train_mask = np.zeros(n_total, dtype=bool)
val_mask = np.zeros(n_total, dtype=bool)
test_mask = np.zeros(n_total, dtype=bool)

train_mask[:n_train] = True
val_mask[n_train:n_train+n_val] = True
test_mask[n_train+n_val:] = True

print(f"\nData split: train={n_train}, val={n_val}, test={n_test}")

# ============================================================
# 6. OPTUNA HPO
# ============================================================

print("\n=== Running Optuna HPO ===")

study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler(seed=42)
)

study.optimize(
    lambda trial: objective(trial, vix_log_changes, vix_levels, target, n_train, val_mask),
    n_trials=30,
    timeout=300,
    show_progress_bar=False
)

print(f"\nOptuna completed: {len(study.trials)} trials")
print(f"Best value: {study.best_value:.6f}")
print(f"Best params: {study.best_params}")

# ============================================================
# 7. GENERATE FINAL OUTPUT WITH BEST PARAMS
# ============================================================

print("\n=== Generating Final Output ===")

best_params = study.best_params

# Generate features with best params
regime = generate_regime_feature(
    vix_log_changes,
    best_params['hmm_n_components'],
    best_params['hmm_covariance_type'],
    best_params['hmm_n_init'],
    n_train
)

zscore = generate_zscore_feature(
    vix_levels,
    best_params['zscore_window']
)

persistence = generate_persistence_feature(
    vix_log_changes,
    best_params['autocorr_window']
)

# Create output DataFrame
output_df = pd.DataFrame({
    'date': dates,
    'vix_regime_probability': regime,
    'vix_mean_reversion_z': zscore,
    'vix_persistence': persistence
})

# Forward-fill NaN values (from warmup period)
output_df['vix_regime_probability'] = output_df['vix_regime_probability'].ffill()
output_df['vix_mean_reversion_z'] = output_df['vix_mean_reversion_z'].ffill()
output_df['vix_persistence'] = output_df['vix_persistence'].ffill()

print(f"Output shape: {output_df.shape}")
print(f"Output columns: {list(output_df.columns)}")
print(f"\nOutput summary:")
print(output_df.describe())

# Check for remaining NaN
nan_counts = output_df.isna().sum()
print(f"\nNaN counts after forward-fill:")
print(nan_counts)

# ============================================================
# 8. COMPUTE METRICS FOR TRAINING_RESULT.JSON
# ============================================================

print("\n=== Computing Metrics ===")

# Calculate MI on test set for final metrics
def discretize_for_mi(x, bins=20):
    valid = ~np.isnan(x)
    if valid.sum() < bins:
        return None
    x_clean = x.copy()
    x_clean[~valid] = np.nanmedian(x)
    try:
        return pd.qcut(x_clean, bins, labels=False, duplicates='drop')
    except:
        return pd.cut(x_clean, bins, labels=False, duplicates='drop')

mi_test = {}
for col in ['vix_regime_probability', 'vix_mean_reversion_z', 'vix_persistence']:
    feat_test = output_df.loc[test_mask, col].values
    target_test = target[test_mask]

    mask = ~np.isnan(feat_test) & ~np.isnan(target_test)

    if mask.sum() > 50:
        feat_disc = discretize_for_mi(feat_test[mask])
        tgt_disc = discretize_for_mi(target_test[mask])

        if feat_disc is not None and tgt_disc is not None:
            mi = mutual_info_score(feat_disc, tgt_disc)
            mi_test[col] = float(mi)
        else:
            mi_test[col] = 0.0
    else:
        mi_test[col] = 0.0

mi_total_test = sum(mi_test.values())

print(f"MI on test set:")
for col, mi in mi_test.items():
    print(f"  {col}: {mi:.6f}")
print(f"  Total: {mi_total_test:.6f}")

# Compute autocorrelation for each feature
autocorr_metrics = {}
for col in ['vix_regime_probability', 'vix_mean_reversion_z', 'vix_persistence']:
    series = output_df[col].dropna()
    if len(series) > 1:
        autocorr = series.autocorr(lag=1)
        autocorr_metrics[col] = float(autocorr) if not np.isnan(autocorr) else 0.0
    else:
        autocorr_metrics[col] = 0.0

print(f"\nAutocorrelation (lag 1):")
for col, ac in autocorr_metrics.items():
    print(f"  {col}: {ac:.4f}")

# ============================================================
# 9. SAVE RESULTS
# ============================================================

print("\n=== Saving Results ===")

# Save submodel output
output_df.to_csv("submodel_output.csv", index=False)
print("Saved: submodel_output.csv")

# Save training result
result = {
    "feature": "vix",
    "attempt": 1,
    "timestamp": datetime.now().isoformat(),
    "best_params": best_params,
    "metrics": {
        "mi_test": mi_test,
        "mi_total_test": mi_total_test,
        "autocorrelation": autocorr_metrics,
        "optuna_best_value": float(study.best_value),
        "optuna_trials_completed": len(study.trials)
    },
    "output_shape": list(output_df.shape),
    "output_columns": list(output_df.columns),
    "data_info": {
        "train_samples": n_train,
        "val_samples": n_val,
        "test_samples": n_test,
        "full_samples": n_total,
        "date_range": {
            "start": str(dates[0]),
            "end": str(dates[-1])
        }
    }
}

with open("training_result.json", "w") as f:
    json.dump(result, f, indent=2)

print("Saved: training_result.json")

print(f"\n=== Training Complete! ===")
print(f"Finished: {datetime.now().isoformat()}")
print(f"Best MI (validation): {study.best_value:.6f}")
print(f"Best MI (test): {mi_total_test:.6f}")
print(f"Output: {output_df.shape[0]} rows Ã— {output_df.shape[1]} columns")
