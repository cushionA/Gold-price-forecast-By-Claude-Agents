{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-0",
   "metadata": {},
   "source": [
    "# Gold SubModel: Yield Curve - Attempt 5\n",
    "**Approach**: Cross-Tenor Correlation Dynamics\n",
    "\n",
    "**Key differences from previous attempts**:\n",
    "- No ML model, no PyTorch, no HMM\n",
    "- Captures the STRUCTURE of co-movement between yield tenors\n",
    "- Takes CHANGE in rolling correlation (not level) to avoid autocorrelation\n",
    "\n",
    "**Output features**:\n",
    "1. `yc_corr_long_short_z`: Z-scored daily CHANGE in rolling corr(DGS10 changes, DGS3MO changes)\n",
    "2. `yc_corr_long_mid_z`: Z-scored daily CHANGE in rolling corr(DGS10 changes, DGS2 changes)\n",
    "3. `yc_corr_1y10y_z`: Z-scored daily CHANGE in rolling corr(DGS1 changes, DGS10 changes)\n",
    "\n",
    "**Expected Gate 1**: autocorr max 0.053 (far below 0.95 threshold)\n",
    "**Expected Gate 2**: VIF max 1.52 internal, 1.55 combined with att2. Max corr with att2 = 0.073."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install fredapi if not available\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'fredapi', '-q'])\n",
    "    from fredapi import Fred\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from numpy.linalg import inv as np_inv\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "FEATURE_NAME = 'yield_curve'\n",
    "ATTEMPT = 5\n",
    "OUTPUT_COLUMNS = ['yc_corr_long_short_z', 'yc_corr_long_mid_z', 'yc_corr_1y10y_z']\n",
    "CLIP_RANGE = (-4, 4)\n",
    "\n",
    "print(f'=== Gold SubModel Training: {FEATURE_NAME} attempt {ATTEMPT} ===')\n",
    "print('Approach: Cross-Tenor Correlation Dynamics')\n",
    "print(f'Started: {datetime.now().isoformat()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRED API key via Kaggle Secrets (with fallback)\n",
    "FRED_API_KEY = None\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    FRED_API_KEY = UserSecretsClient().get_secret('FRED_API_KEY')\n",
    "    print('FRED_API_KEY loaded from Kaggle Secrets')\n",
    "except Exception:\n",
    "    FRED_API_KEY = os.environ.get('FRED_API_KEY')\n",
    "    if FRED_API_KEY:\n",
    "        print('FRED_API_KEY loaded from environment')\n",
    "    else:\n",
    "        raise RuntimeError('FRED_API_KEY not found in Kaggle Secrets or environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic path resolution for bigbigzabuton/gold-prediction-submodels\n",
    "# Strategy: check BOTH path AND file existence, then list files for debugging.\n",
    "import glob as _glob\n",
    "\n",
    "PROBE_FILES = ['base_features.csv', 'base_features_raw.csv', 'vix.csv']\n",
    "candidates = [\n",
    "    '/kaggle/input/gold-prediction-submodels',\n",
    "    '/kaggle/input/datasets/bigbigzabuton/gold-prediction-submodels',\n",
    "]\n",
    "\n",
    "DATASET_PATH = None\n",
    "for c in candidates:\n",
    "    if os.path.isdir(c):\n",
    "        files_in_dir = os.listdir(c)\n",
    "        if any(f in files_in_dir for f in PROBE_FILES):\n",
    "            DATASET_PATH = c\n",
    "            print(f'Dataset found: {DATASET_PATH}')\n",
    "            print(f'  Files ({len(files_in_dir)}): {sorted(files_in_dir)[:10]}')\n",
    "            break\n",
    "        else:\n",
    "            print(f'Dir exists but missing probe files: {c} -> {files_in_dir[:5]}')\n",
    "\n",
    "if DATASET_PATH is None:\n",
    "    found = _glob.glob('/kaggle/input/*gold*') + _glob.glob('/kaggle/input/datasets/*/*gold*')\n",
    "    raise RuntimeError(\n",
    "        f'Dataset not found.\\n'\n",
    "        f'Tried: {candidates}\\n'\n",
    "        f'Glob /kaggle/input/*gold*: {found}\\n'\n",
    "        f'All /kaggle/input/: {os.listdir(\"/kaggle/input\") if os.path.exists(\"/kaggle/input\") else \"N/A\"}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch yield data from FRED\n",
    "# DGS10, DGS2, DGS3MO, DGS1 from 2014-06-01\n",
    "# Buffer for rolling window warmup: corr_window(90) + zscore_window(90) + buffer = ~200 trading days\n",
    "START = '2014-06-01'\n",
    "TICKERS_FRED = ['DGS10', 'DGS2', 'DGS3MO', 'DGS1']\n",
    "\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "series = {}\n",
    "for ticker in TICKERS_FRED:\n",
    "    s = fred.get_series(ticker, observation_start=START)\n",
    "    s = s.ffill(limit=3)\n",
    "    series[ticker] = s\n",
    "    print(f'{ticker}: {len(s.dropna())} obs, {s.dropna().index[0].date()} to {s.dropna().index[-1].date()}')\n",
    "\n",
    "# Build aligned DataFrame of yield series\n",
    "yields_df = pd.DataFrame({\n",
    "    'dgs10': series['DGS10'],\n",
    "    'dgs2': series['DGS2'],\n",
    "    'dgs3mo': series['DGS3MO'],\n",
    "    'dgs1': series['DGS1'],\n",
    "})\n",
    "yields_df.index = pd.to_datetime(yields_df.index)\n",
    "yields_df = yields_df.dropna()\n",
    "print(f'Combined yields: {len(yields_df)} obs, {yields_df.index[0].date()} to {yields_df.index[-1].date()}')\n",
    "print(f'Sample yields (last 3 rows):')\n",
    "print(yields_df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch gold price from Yahoo Finance for target computation\n",
    "gold = yf.download('GC=F', start=START, auto_adjust=True, progress=False)\n",
    "\n",
    "if gold.empty or len(gold) < 100:\n",
    "    raise ValueError(f'GC=F download returned insufficient data: {len(gold)} rows')\n",
    "\n",
    "if isinstance(gold.columns, pd.MultiIndex):\n",
    "    gold_close = gold['Close'].iloc[:, 0]\n",
    "else:\n",
    "    gold_close = gold['Close'].squeeze()\n",
    "\n",
    "gold_ret = gold_close.pct_change() * 100\n",
    "gold_ret_next = gold_ret.shift(-1)  # next-day return (target)\n",
    "gold_ret_next.name = 'gold_return_next'\n",
    "gold_ret_next.index = pd.to_datetime(gold_ret_next.index).tz_localize(None)\n",
    "print(f'GC=F: {len(gold_ret_next.dropna())} obs, {gold_ret_next.dropna().index[0].date()} to {gold_ret_next.dropna().index[-1].date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base_features for date alignment (try both filename variants)\n",
    "base_path = None\n",
    "for fname in ['base_features.csv', 'base_features_raw.csv']:\n",
    "    candidate = os.path.join(DATASET_PATH, fname)\n",
    "    if os.path.exists(candidate):\n",
    "        base_path = candidate\n",
    "        print(f'Using base features file: {fname}')\n",
    "        break\n",
    "\n",
    "if base_path is None:\n",
    "    files_in_dir = os.listdir(DATASET_PATH)\n",
    "    raise FileNotFoundError(\n",
    "        f'base_features[_raw].csv not found in {DATASET_PATH}.\\n'\n",
    "        f'Available files: {sorted(files_in_dir)}'\n",
    "    )\n",
    "\n",
    "base_features = pd.read_csv(base_path, index_col=0, parse_dates=True)\n",
    "base_features.index = pd.to_datetime(base_features.index)\n",
    "TARGET_DATES = base_features.index\n",
    "print(f'Base features: {len(base_features)} rows, {TARGET_DATES[0].date()} to {TARGET_DATES[-1].date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split: train/val/test = 70/15/15 on base_features dates\n",
    "# Align yields and gold target to base_features dates\n",
    "common_dates = yields_df.index.intersection(TARGET_DATES)\n",
    "common_dates = common_dates.intersection(gold_ret_next.dropna().index)\n",
    "common_dates = common_dates.sort_values()\n",
    "\n",
    "n = len(common_dates)\n",
    "train_end_idx = int(n * 0.70)\n",
    "val_end_idx = int(n * 0.85)\n",
    "\n",
    "train_dates = common_dates[:train_end_idx]\n",
    "val_dates = common_dates[train_end_idx:val_end_idx]\n",
    "test_dates = common_dates[val_end_idx:]\n",
    "\n",
    "print(f'Total aligned: {n}')\n",
    "print(f'Train: {len(train_dates)} ({train_dates[0].date()} to {train_dates[-1].date()})')\n",
    "print(f'Val:   {len(val_dates)} ({val_dates[0].date()} to {val_dates[-1].date()})')\n",
    "print(f'Test:  {len(test_dates)} ({test_dates[0].date()} to {test_dates[-1].date()})')\n",
    "\n",
    "# Create masks on yields_df index (not just common_dates)\n",
    "val_mask = yields_df.index.isin(val_dates)\n",
    "test_mask = yields_df.index.isin(test_dates)\n",
    "\n",
    "# Aligned gold target\n",
    "gold_aligned = gold_ret_next.reindex(yields_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation functions\n",
    "\n",
    "def rolling_zscore(x, window):\n",
    "    \"\"\"Rolling z-score with NaN-safe handling.\"\"\"\n",
    "    min_p = max(window // 2, 10)\n",
    "    m = x.rolling(window, min_periods=min_p).mean()\n",
    "    s = x.rolling(window, min_periods=min_p).std()\n",
    "    z = (x - m) / s.replace(0, np.nan)\n",
    "    return z.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "def compute_corr_change_z(series_a_chg, series_b_chg, corr_window, zscore_window):\n",
    "    \"\"\"\n",
    "    Compute the z-scored daily change in rolling correlation\n",
    "    between two yield change series.\n",
    "\n",
    "    Steps:\n",
    "    1. Rolling correlation between daily yield changes\n",
    "    2. First difference of correlation (avoids level autocorrelation ~0.98)\n",
    "    3. Z-score over rolling window\n",
    "    4. Clip to [-4, 4]\n",
    "\n",
    "    Critical: use CHANGE not level. Level autocorr ~0.98; change autocorr ~0.05.\n",
    "    \"\"\"\n",
    "    min_p = max(corr_window // 2, 15)\n",
    "    corr = series_a_chg.rolling(corr_window, min_periods=min_p).corr(series_b_chg)\n",
    "    corr_chg = corr.diff()\n",
    "    z = rolling_zscore(corr_chg, zscore_window)\n",
    "    return z.clip(*CLIP_RANGE)\n",
    "\n",
    "\n",
    "def generate_all_features(yields_df, corr_window, zscore_window):\n",
    "    \"\"\"Generate all 3 cross-tenor correlation change z-scores.\"\"\"\n",
    "    dgs10_chg = yields_df['dgs10'].diff()\n",
    "    dgs2_chg = yields_df['dgs2'].diff()\n",
    "    dgs3mo_chg = yields_df['dgs3mo'].diff()\n",
    "    dgs1_chg = yields_df['dgs1'].diff()\n",
    "\n",
    "    features = pd.DataFrame(index=yields_df.index)\n",
    "\n",
    "    # Feature 1: 10Y vs 3M correlation change (long-end vs short-end)\n",
    "    # Positive = curve synchronizing. Negative = curve fragmenting.\n",
    "    features['yc_corr_long_short_z'] = compute_corr_change_z(\n",
    "        dgs10_chg, dgs3mo_chg, corr_window, zscore_window\n",
    "    )\n",
    "\n",
    "    # Feature 2: 10Y vs 2Y correlation change (long-end vs mid-range)\n",
    "    # Positive = 10Y and 2Y moving in sync. Negative = term premium shift.\n",
    "    features['yc_corr_long_mid_z'] = compute_corr_change_z(\n",
    "        dgs10_chg, dgs2_chg, corr_window, zscore_window\n",
    "    )\n",
    "\n",
    "    # Feature 3: 1Y vs 10Y correlation change (policy-end vs long-end)\n",
    "    # 1Y = policy expectations. 10Y = term premium. Decorrelation = policy/market disconnect.\n",
    "    features['yc_corr_1y10y_z'] = compute_corr_change_z(\n",
    "        dgs1_chg, dgs10_chg, corr_window, zscore_window\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def compute_mi(feature, target, n_bins=20):\n",
    "    \"\"\"MI between feature and target using quantile binning.\"\"\"\n",
    "    valid = feature.dropna().index.intersection(target.dropna().index)\n",
    "    if len(valid) < 50:\n",
    "        return 0.0\n",
    "    f = feature[valid]\n",
    "    t = target[valid]\n",
    "    try:\n",
    "        f_binned = pd.qcut(f, q=n_bins, labels=False, duplicates='drop')\n",
    "        t_binned = pd.qcut(t, q=n_bins, labels=False, duplicates='drop')\n",
    "        valid2 = f_binned.notna() & t_binned.notna()\n",
    "        if valid2.sum() < 50:\n",
    "            return 0.0\n",
    "        return float(mutual_info_score(f_binned[valid2], t_binned[valid2]))\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "print('Feature generation functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna HPO: maximize sum of MI between 3 features and gold_return_next on validation set\n",
    "# Search space: corr_window in {30, 45, 60, 90}, zscore_window in {30, 45, 60, 90}\n",
    "# Total unique combinations: 4 x 4 = 16. 30 trials provides complete coverage.\n",
    "\n",
    "def objective(trial):\n",
    "    corr_window = trial.suggest_categorical('corr_window', [30, 45, 60, 90])\n",
    "    zscore_window = trial.suggest_categorical('zscore_window', [30, 45, 60, 90])\n",
    "\n",
    "    features = generate_all_features(yields_df, corr_window, zscore_window)\n",
    "\n",
    "    # Compute MI on validation set only\n",
    "    target_val = gold_aligned[val_mask]\n",
    "    mi_sum = 0.0\n",
    "    for col in OUTPUT_COLUMNS:\n",
    "        feat_val = features[col][val_mask]\n",
    "        mi_sum += compute_mi(feat_val, target_val)\n",
    "\n",
    "    return mi_sum\n",
    "\n",
    "\n",
    "print('Running Optuna HPO (30 trials, timeout=300s, TPE sampler)...')\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "study.optimize(objective, n_trials=30, timeout=300)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "n_completed = len(study.trials)\n",
    "print(f'Optuna complete: {n_completed} trials')\n",
    "print(f'Best params: {best_params}')\n",
    "print(f'Best MI sum (val): {best_value:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final features with best Optuna parameters\n",
    "print(f'Generating final features with corr_window={best_params[\"corr_window\"]}, zscore_window={best_params[\"zscore_window\"]}...')\n",
    "final_features = generate_all_features(\n",
    "    yields_df,\n",
    "    corr_window=best_params['corr_window'],\n",
    "    zscore_window=best_params['zscore_window']\n",
    ")\n",
    "\n",
    "print(f'Final features shape: {final_features.shape}')\n",
    "print(f'NaN counts before alignment:')\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    print(f'  {col}: {final_features[col].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks: autocorrelation, internal correlation, VIF\n",
    "print('=== Quality Checks ===')\n",
    "\n",
    "# Autocorrelation check (Gate 1 threshold: 0.95)\n",
    "autocorr_results = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    series_clean = final_features[col].dropna()\n",
    "    if len(series_clean) > 1:\n",
    "        ac = float(series_clean.autocorr(lag=1))\n",
    "    else:\n",
    "        ac = 0.0\n",
    "    autocorr_results[col] = round(ac, 6)\n",
    "    status = 'FAIL' if abs(ac) > 0.95 else 'PASS'\n",
    "    print(f'Autocorr {col}: {ac:.6f} [{status}]')\n",
    "\n",
    "# NaN check\n",
    "print('\\nNaN per column:')\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    nan_count = final_features[col].isna().sum()\n",
    "    nan_pct = nan_count / len(final_features) * 100\n",
    "    print(f'  {col}: {nan_count} ({nan_pct:.1f}%)')\n",
    "\n",
    "# Internal correlation check\n",
    "clean_features = final_features[OUTPUT_COLUMNS].dropna()\n",
    "corr_matrix = clean_features.corr()\n",
    "print(f'\\nInternal correlation matrix:')\n",
    "print(corr_matrix.round(4).to_string())\n",
    "\n",
    "# Upper triangle values for max internal corr\n",
    "upper_tri_vals = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
    "max_internal_corr = float(np.abs(upper_tri_vals).max())\n",
    "print(f'\\nMax internal |correlation|: {max_internal_corr:.4f}')\n",
    "\n",
    "# VIF check\n",
    "print('\\nVIF values:')\n",
    "X = clean_features.values\n",
    "cm = np.corrcoef(X.T)\n",
    "try:\n",
    "    inv_cm = np_inv(cm)\n",
    "    vif_values = np.diag(inv_cm)\n",
    "    for col, v in zip(OUTPUT_COLUMNS, vif_values):\n",
    "        print(f'  VIF {col}: {v:.4f}')\n",
    "    max_vif = float(np.max(vif_values))\n",
    "    print(f'  Max VIF: {max_vif:.4f}')\n",
    "except Exception as e:\n",
    "    print(f'  VIF calculation failed: {e}')\n",
    "    max_vif = None\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f'\\nDescriptive statistics:')\n",
    "print(final_features[OUTPUT_COLUMNS].describe().round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI on validation and test sets\n",
    "val_mi = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    mi = compute_mi(final_features[col][val_mask], gold_aligned[val_mask])\n",
    "    val_mi[col] = round(mi, 6)\n",
    "\n",
    "test_mi = {}\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    mi = compute_mi(final_features[col][test_mask], gold_aligned[test_mask])\n",
    "    test_mi[col] = round(mi, 6)\n",
    "\n",
    "print('MI on validation set:')\n",
    "for col, mi in val_mi.items():\n",
    "    print(f'  {col}: {mi:.6f}')\n",
    "print(f'  Val MI sum: {sum(val_mi.values()):.6f}')\n",
    "\n",
    "print('\\nMI on test set:')\n",
    "for col, mi in test_mi.items():\n",
    "    print(f'  {col}: {mi:.6f}')\n",
    "print(f'  Test MI sum: {sum(test_mi.values()):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align output to base_features date range and save\n",
    "output = final_features[OUTPUT_COLUMNS].reindex(base_features.index)\n",
    "output.index.name = 'Date'\n",
    "\n",
    "# Forward-fill up to 3 days for minor gaps (FRED data has occasional missing trading days)\n",
    "output = output.ffill(limit=3)\n",
    "\n",
    "# Drop rows that are entirely NaN (warmup period before rolling windows are populated)\n",
    "output = output.dropna(how='all')\n",
    "\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Date range: {output.index[0].date()} to {output.index[-1].date()}')\n",
    "print(f'NaN per column after alignment and ffill:')\n",
    "for col in OUTPUT_COLUMNS:\n",
    "    nan_count = output[col].isna().sum()\n",
    "    print(f'  {col}: {nan_count}')\n",
    "\n",
    "# Save submodel output\n",
    "output.to_csv('/kaggle/working/submodel_output.csv')\n",
    "print('\\nSaved: /kaggle/working/submodel_output.csv')\n",
    "print(output.tail(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training result JSON\n",
    "# overfit_ratio: set to 1.0 (deterministic, no train/val model; evaluator uses for Gate 1)\n",
    "# autocorr: dict of {column_name: autocorr_value} for all 3 features\n",
    "# mi_sum_val: best Optuna value\n",
    "# optuna_trials_completed: number of completed trials\n",
    "\n",
    "result = {\n",
    "    'feature': FEATURE_NAME,\n",
    "    'attempt': ATTEMPT,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'approach': 'Cross-Tenor Correlation Dynamics',\n",
    "    'description': (\n",
    "        'Z-scored daily changes in rolling cross-tenor yield correlations. '\n",
    "        '3 features: 10Y-3M corr change, 10Y-2Y corr change, 1Y-10Y corr change. '\n",
    "        'CHANGE not level avoids autocorrelation (level ~0.98, change ~0.05).'\n",
    "    ),\n",
    "    'best_params': best_params,\n",
    "    'metrics': {\n",
    "        'overfit_ratio': 1.0,\n",
    "        'mi_sum_val': round(best_value, 6),\n",
    "        'mi_individual_val': val_mi,\n",
    "        'mi_individual_test': test_mi,\n",
    "        'mi_sum_test': round(sum(test_mi.values()), 6),\n",
    "        'autocorr': autocorr_results,\n",
    "        'max_internal_corr': round(max_internal_corr, 6),\n",
    "        'optuna_trials_completed': n_completed,\n",
    "        'optuna_best_value': round(best_value, 6),\n",
    "        'output_nan_counts': {col: int(output[col].isna().sum()) for col in OUTPUT_COLUMNS},\n",
    "    },\n",
    "    'output_shape': list(output.shape),\n",
    "    'output_columns': OUTPUT_COLUMNS,\n",
    "    'data_info': {\n",
    "        'total_aligned': n,\n",
    "        'train_samples': len(train_dates),\n",
    "        'val_samples': len(val_dates),\n",
    "        'test_samples': len(test_dates),\n",
    "        'date_range_start': str(output.index.min().date()),\n",
    "        'date_range_end': str(output.index.max().date()),\n",
    "        'fred_tickers': TICKERS_FRED,\n",
    "        'gold_ticker': 'GC=F',\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/training_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "\n",
    "print('Saved: /kaggle/working/training_result.json')\n",
    "print(f'\\n=== Training complete! ===')\n",
    "print(f'Finished: {datetime.now().isoformat()}')\n",
    "print(f'Output columns: {OUTPUT_COLUMNS}')\n",
    "print(f'Best params: corr_window={best_params[\"corr_window\"]}, zscore_window={best_params[\"zscore_window\"]}')\n",
    "print(f'Best MI sum (val): {best_value:.6f}')\n",
    "print(f'Autocorrelations: {autocorr_results}')\n",
    "print(json.dumps(result, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
