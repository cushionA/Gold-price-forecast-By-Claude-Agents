{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel Training - Temporal Context Transformer\n",
    "## Attempt 1\n",
    "\n",
    "**Generated by**: builder_model agent  \n",
    "**Architecture**: Asymmetric Transformer Autoencoder with Masked Reconstruction  \n",
    "**Input**: 14 features (5 base + 9 submodel outputs)  \n",
    "**Output**: 1 column (temporal_context_score, 0-1)  \n",
    "**Target params**: ~6,200 (3K-10K range)  \n",
    "**Window size**: 5-20 days (Optuna search)  \n",
    "\n",
    "Self-contained: Data fetch → Preprocessing → Training → Evaluation → Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"=== Gold SubModel Training: temporal_context attempt 1 ===\")\n",
    "print(f\"Started: {datetime.now().isoformat()}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Data Fetching (API-based, self-contained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_data():\n    \"\"\"\n    Fetch and prepare data for temporal context transformer.\n    Returns: (train_df, val_df, test_df, full_df, scaler)\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"DATA FETCHING\")\n    print(\"=\"*80)\n    \n    # ===== 1. Import data APIs =====\n    import yfinance as yf\n    try:\n        from fredapi import Fred\n    except ImportError:\n        import subprocess\n        print(\"Installing fredapi...\")\n        subprocess.run([\"pip\", \"install\", \"fredapi\"], check=True)\n        from fredapi import Fred\n    \n    # Get FRED API key from environment variable\n    FRED_API_KEY = \"3ffb68facdf6321e180e380c00e909c8\"\n    fred = Fred(api_key=FRED_API_KEY)\n    \n    # ===== 2. Fetch base features (FRED + Yahoo) =====\n    print(\"\\n[1/5] Fetching base features from FRED and Yahoo...\")\n    \n    # Real interest rate (10Y TIPS)\n    real_rate = fred.get_series('DFII10', observation_start='2015-01-01')\n    real_rate_df = pd.DataFrame({'real_rate_real_rate': real_rate})\n    print(f\"   Real rate: {len(real_rate_df)} rows\")\n    \n    # DXY (Dollar Index) - using FRED for stability\n    dxy = fred.get_series('DTWEXBGS', observation_start='2015-01-01')\n    dxy_df = pd.DataFrame({'dxy_dxy': dxy})\n    print(f\"   DXY: {len(dxy_df)} rows\")\n    \n    # VIX\n    vix = fred.get_series('VIXCLS', observation_start='2015-01-01')\n    vix_df = pd.DataFrame({'vix_vix': vix})\n    print(f\"   VIX: {len(vix_df)} rows\")\n    \n    # Yield spread (10Y - 2Y)\n    dgs10 = fred.get_series('DGS10', observation_start='2015-01-01')\n    dgs2 = fred.get_series('DGS2', observation_start='2015-01-01')\n    yield_spread = dgs10 - dgs2\n    yield_spread_df = pd.DataFrame({'yield_curve_yield_spread': yield_spread})\n    print(f\"   Yield spread: {len(yield_spread_df)} rows\")\n    \n    # Inflation expectation (10Y Breakeven)\n    inflation_exp = fred.get_series('T10YIE', observation_start='2015-01-01')\n    inflation_exp_df = pd.DataFrame({'inflation_expectation_inflation_expectation': inflation_exp})\n    print(f\"   Inflation expectation: {len(inflation_exp_df)} rows\")\n    \n    # Merge base features\n    base_df = real_rate_df.join(dxy_df, how='outer')\n    base_df = base_df.join(vix_df, how='outer')\n    base_df = base_df.join(yield_spread_df, how='outer')\n    base_df = base_df.join(inflation_exp_df, how='outer')\n    base_df = base_df.sort_index()\n    \n    # Forward fill and backward fill (max 5 days)\n    base_df = base_df.ffill(limit=5)\n    base_df = base_df.bfill(limit=5)\n    base_df = base_df.dropna()\n    \n    print(f\"   Base features merged: {len(base_df)} rows, {base_df.shape[1]} columns\")\n    \n    # ===== 3. Transform base features =====\n    print(\"\\n[2/5] Transforming base features (diff)...\")\n    \n    base_features = pd.DataFrame(index=base_df.index)\n    base_features['real_rate_change'] = base_df['real_rate_real_rate'].diff()\n    base_features['dxy_change'] = base_df['dxy_dxy'].diff()\n    base_features['vix'] = base_df['vix_vix']  # No transformation (already stationary)\n    base_features['yield_spread_change'] = base_df['yield_curve_yield_spread'].diff()\n    base_features['inflation_exp_change'] = base_df['inflation_expectation_inflation_expectation'].diff()\n    \n    print(f\"   Base features transformed: {list(base_features.columns)}\")\n    \n    # ===== 4. Load submodel outputs from Kaggle Dataset =====\n    print(\"\\n[3/5] Loading submodel outputs from Kaggle Dataset...\")\n    \n    # Note: In Kaggle environment, the dataset is mounted at /kaggle/input/\n    # For local testing, adjust the path\n    try:\n        # Kaggle environment\n        submodel_path = \"/kaggle/input/gold-prediction-submodels/\"\n    except:\n        # Fallback for local testing\n        submodel_path = \"../data/submodel_outputs/\"\n    \n    # VIX submodel (2 columns)\n    vix_sub = pd.read_csv(submodel_path + \"vix.csv\")\n    vix_sub['date'] = pd.to_datetime(vix_sub['date'])\n    vix_sub = vix_sub.set_index('date').sort_index()\n    vix_features = vix_sub[['vix_regime_probability', 'vix_mean_reversion_z']].copy()\n    print(f\"   VIX: {len(vix_features)} rows\")\n    \n    # Technical submodel (3 columns) - handle timezone\n    tech_sub = pd.read_csv(submodel_path + \"technical.csv\")\n    tech_sub['date'] = tech_sub['date'].str[:10]  # Extract YYYY-MM-DD\n    tech_sub['date'] = pd.to_datetime(tech_sub['date'])\n    tech_sub = tech_sub.set_index('date').sort_index()\n    tech_features = tech_sub[['tech_trend_regime_prob', 'tech_mean_reversion_z', 'tech_volatility_regime']].copy()\n    print(f\"   Technical: {len(tech_features)} rows\")\n    \n    # Cross-asset submodel (2 columns)\n    xasset_sub = pd.read_csv(submodel_path + \"cross_asset.csv\")\n    xasset_sub['Date'] = pd.to_datetime(xasset_sub['Date'])\n    xasset_sub = xasset_sub.set_index('Date').sort_index()\n    xasset_features = xasset_sub[['xasset_regime_prob', 'xasset_divergence']].copy()\n    print(f\"   Cross-asset: {len(xasset_features)} rows\")\n    \n    # ETF flow submodel (1 column)\n    etf_sub = pd.read_csv(submodel_path + \"etf_flow.csv\")\n    etf_sub['Date'] = pd.to_datetime(etf_sub['Date'])\n    etf_sub = etf_sub.set_index('Date').sort_index()\n    etf_features = etf_sub[['etf_regime_prob']].copy()\n    print(f\"   ETF flow: {len(etf_features)} rows\")\n    \n    # Options market submodel (1 column) - handle timezone\n    options_sub = pd.read_csv(submodel_path + \"options_market.csv\")\n    options_sub['Date'] = options_sub['Date'].str[:10]  # Extract YYYY-MM-DD\n    options_sub['Date'] = pd.to_datetime(options_sub['Date'])\n    options_sub = options_sub.set_index('Date').sort_index()\n    options_features = options_sub[['options_risk_regime_prob']].copy()\n    print(f\"   Options: {len(options_features)} rows\")\n    \n    # ===== 5. Merge all features =====\n    print(\"\\n[4/5] Merging all features...\")\n    \n    merged_df = base_features.copy()\n    merged_df = merged_df.join(vix_features, how='inner')\n    merged_df = merged_df.join(tech_features, how='inner')\n    merged_df = merged_df.join(xasset_features, how='inner')\n    merged_df = merged_df.join(etf_features, how='inner')\n    merged_df = merged_df.join(options_features, how='inner')\n    \n    # Handle NaN values\n    merged_df = merged_df.ffill(limit=5)\n    merged_df = merged_df.bfill()\n    merged_df = merged_df.dropna()\n    \n    # Remove infinite values\n    inf_mask = np.isinf(merged_df.values).any(axis=1)\n    if inf_mask.any():\n        merged_df = merged_df[~inf_mask]\n    \n    print(f\"   Final merged: {len(merged_df)} rows, {merged_df.shape[1]} columns\")\n    print(f\"   Date range: {merged_df.index.min()} to {merged_df.index.max()}\")\n    \n    # Verify we have exactly 14 columns\n    assert merged_df.shape[1] == 14, f\"Expected 14 columns, got {merged_df.shape[1]}\"\n    \n    # ===== 6. Time-series split (70/15/15) =====\n    print(\"\\n[5/5] Splitting data (70/15/15)...\")\n    \n    n = len(merged_df)\n    train_end = int(n * 0.70)\n    val_end = int(n * 0.85)\n    \n    train_df = merged_df.iloc[:train_end].copy()\n    val_df = merged_df.iloc[train_end:val_end].copy()\n    test_df = merged_df.iloc[val_end:].copy()\n    \n    print(f\"   Train: {len(train_df)} rows ({train_df.index.min()} to {train_df.index.max()})\")\n    print(f\"   Val:   {len(val_df)} rows ({val_df.index.min()} to {val_df.index.max()})\")\n    print(f\"   Test:  {len(test_df)} rows ({test_df.index.min()} to {test_df.index.max()})\")\n    \n    # ===== 7. Standardization =====\n    print(\"\\nStandardizing features...\")\n    \n    scaler = StandardScaler()\n    train_scaled = scaler.fit_transform(train_df)\n    val_scaled = scaler.transform(val_df)\n    test_scaled = scaler.transform(test_df)\n    full_scaled = scaler.transform(merged_df)\n    \n    # Convert back to DataFrame\n    train_df = pd.DataFrame(train_scaled, index=train_df.index, columns=train_df.columns)\n    val_df = pd.DataFrame(val_scaled, index=val_df.index, columns=val_df.columns)\n    test_df = pd.DataFrame(test_scaled, index=test_df.index, columns=test_df.columns)\n    full_df = pd.DataFrame(full_scaled, index=merged_df.index, columns=merged_df.columns)\n    \n    print(\"   [OK] Features standardized using train set statistics\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"DATA FETCHING COMPLETE\")\n    print(\"=\"*80)\n    \n    return train_df, val_df, test_df, full_df, scaler\n\n# Fetch data\ntrain_data, val_data, test_data, full_data, scaler = fetch_data()\nprint(f\"\\nData ready: train={len(train_data)}, val={len(val_data)}, test={len(test_data)}, full={len(full_data)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Windowing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size):\n",
    "    \"\"\"\n",
    "    Create sliding windows from time-series data.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame (N, 14) with date index\n",
    "        window_size: int, number of time steps per window\n",
    "    \n",
    "    Returns:\n",
    "        windows: tensor (N-W+1, W, 14)\n",
    "        dates: list of dates for each window (end date)\n",
    "    \"\"\"\n",
    "    values = data.values\n",
    "    n_samples = len(values)\n",
    "    \n",
    "    if n_samples < window_size:\n",
    "        raise ValueError(f\"Not enough samples ({n_samples}) for window size {window_size}\")\n",
    "    \n",
    "    windows = []\n",
    "    dates = []\n",
    "    \n",
    "    for i in range(window_size - 1, n_samples):\n",
    "        window = values[i - window_size + 1:i + 1]\n",
    "        windows.append(window)\n",
    "        dates.append(data.index[i])\n",
    "    \n",
    "    windows = np.array(windows)\n",
    "    windows_tensor = torch.FloatTensor(windows)\n",
    "    \n",
    "    return windows_tensor, dates\n",
    "\n",
    "print(\"Windowing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for windowed time-series data.\n",
    "    \"\"\"\n",
    "    def __init__(self, windows):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            windows: tensor (N, W, 14)\n",
    "        \"\"\"\n",
    "        self.windows = windows\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.windows[idx]\n",
    "\n",
    "print(\"WindowDataset class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Model Definition - Temporal Context Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalContextTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Asymmetric Transformer Autoencoder for temporal context extraction.\n",
    "    \n",
    "    Architecture:\n",
    "      Input (batch, seq, 14)\n",
    "        -> Input Projection (14 -> d_model)\n",
    "        -> Learned Positional Encoding\n",
    "        -> TransformerEncoder (L layers, H heads)\n",
    "        -> Mean Pool over time\n",
    "        -> Bottleneck Linear (d_model -> 1)\n",
    "        -> Sigmoid -> context_score (0-1)\n",
    "      \n",
    "      Reconstruction branch (training only):\n",
    "        -> Bottleneck (1) -> Expand (d_model)\n",
    "        -> Repeat to seq_len\n",
    "        -> Output Projection (d_model -> 14)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=14, d_model=24, n_heads=2, n_layers=1,\n",
    "                 ffn_ratio=2, dropout=0.2, max_seq_len=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Learned positional encoding\n",
    "        self.pos_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * ffn_ratio,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-Norm for stability\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Lightweight Decoder (reconstruction branch)\n",
    "        self.decoder_expand = nn.Linear(1, d_model)\n",
    "        self.decoder_output = nn.Linear(d_model, input_dim)\n",
    "        \n",
    "        # Dropout for input\n",
    "        self.input_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_dim)\n",
    "        Returns: context_score (batch, 1), pooled (batch, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection\n",
    "        h = self.input_proj(x)  # (batch, seq, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        h = h + self.pos_encoding(positions).unsqueeze(0)\n",
    "        \n",
    "        # Apply input dropout\n",
    "        h = self.input_dropout(h)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoded = self.encoder(h)  # (batch, seq, d_model)\n",
    "        \n",
    "        # Mean pool over time\n",
    "        pooled = encoded.mean(dim=1)  # (batch, d_model)\n",
    "        \n",
    "        # Bottleneck -> context score\n",
    "        context_score = self.bottleneck(pooled)  # (batch, 1)\n",
    "        \n",
    "        return context_score, pooled\n",
    "    \n",
    "    def decode(self, context_score, seq_len):\n",
    "        \"\"\"\n",
    "        Reconstruct from bottleneck for masked reconstruction loss.\n",
    "        context_score: (batch, 1)\n",
    "        Returns: (batch, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        # Expand bottleneck\n",
    "        expanded = self.decoder_expand(context_score)  # (batch, d_model)\n",
    "        \n",
    "        # Repeat to sequence length\n",
    "        expanded = expanded.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, seq, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        reconstructed = self.decoder_output(expanded)  # (batch, seq, input_dim)\n",
    "        \n",
    "        return reconstructed\n",
    "    \n",
    "    def forward(self, x, mask_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Forward pass with masked reconstruction.\n",
    "        x: (batch, seq_len, input_dim)\n",
    "        Returns: context_score, reconstructed, mask\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        \n",
    "        # Create random mask (mask time steps, not features)\n",
    "        mask = torch.rand(batch_size, seq_len, device=x.device) < mask_ratio\n",
    "        # Ensure at least 1 step is masked and 1 is unmasked\n",
    "        mask[:, 0] = False  # Keep first step\n",
    "        if seq_len > 2:\n",
    "            mask[:, -1] = True   # Always mask last step\n",
    "        \n",
    "        # Apply mask (zero out masked positions)\n",
    "        x_masked = x.clone()\n",
    "        x_masked[mask] = 0.0\n",
    "        \n",
    "        # Encode\n",
    "        context_score, pooled = self.encode(x_masked)\n",
    "        \n",
    "        # Decode (reconstruct)\n",
    "        reconstructed = self.decode(context_score, seq_len)\n",
    "        \n",
    "        return context_score, reconstructed, mask\n",
    "    \n",
    "    def extract(self, x):\n",
    "        \"\"\"\n",
    "        Extract context score for inference (no masking).\n",
    "        x: (batch, seq_len, input_dim)\n",
    "        Returns: context_score (batch, 1)\n",
    "        \"\"\"\n",
    "        context_score, _ = self.encode(x)\n",
    "        return context_score\n",
    "\n",
    "# Test model instantiation\n",
    "test_model = TemporalContextTransformer()\n",
    "n_params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"\\nModel defined. Test model parameters: {n_params:,}\")\n",
    "print(f\"Expected range: 3,000-10,000 params\")\n",
    "del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Loss Function and Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_reconstruction_loss(original, reconstructed, mask):\n",
    "    \"\"\"\n",
    "    MSE loss computed ONLY on masked time steps.\n",
    "    \n",
    "    original: (batch, seq_len, 14)\n",
    "    reconstructed: (batch, seq_len, 14)\n",
    "    mask: (batch, seq_len) -- True where masked\n",
    "    \"\"\"\n",
    "    # Expand mask to feature dimension\n",
    "    mask_expanded = mask.unsqueeze(-1).expand_as(original)  # (batch, seq, 14)\n",
    "    \n",
    "    # Compute MSE only on masked positions\n",
    "    diff = (original - reconstructed) ** 2\n",
    "    masked_diff = diff[mask_expanded]\n",
    "    \n",
    "    if masked_diff.numel() == 0:\n",
    "        return torch.tensor(0.0, requires_grad=True, device=original.device)\n",
    "    \n",
    "    return masked_diff.mean()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping.\n",
    "    \n",
    "    Returns: model, metrics_dict\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config.get('weight_decay', 0.01),\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=20, T_mult=2\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    max_epochs = config.get('max_epochs', 200)\n",
    "    patience = config.get('patience', 10)\n",
    "    mask_ratio = config.get('mask_ratio', 0.2)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            context_score, reconstructed, mask = model(batch, mask_ratio=mask_ratio)\n",
    "            loss = masked_reconstruction_loss(batch, reconstructed, mask)\n",
    "            \n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                context_score, reconstructed, mask = model(batch, mask_ratio=mask_ratio)\n",
    "                loss = masked_reconstruction_loss(batch, reconstructed, mask)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_train_loss = train_loss\n",
    "            patience_counter = 0\n",
    "            best_state = {k: v.clone().cpu() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"   Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{max_epochs}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n",
    "    \n",
    "    # Restore best weights\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    overfit_ratio = best_val_loss / (best_train_loss + 1e-10)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_loss': best_train_loss,\n",
    "        'val_loss': best_val_loss,\n",
    "        'overfit_ratio': overfit_ratio,\n",
    "        'epochs_trained': epoch + 1\n",
    "    }\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Optuna HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_hpo(train_data, val_data, n_trials=30, timeout=1800):\n    \"\"\"\n    Run hyperparameter optimization using Optuna.\n    \n    Returns: best_params, best_value, n_completed_trials\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"HYPERPARAMETER OPTIMIZATION (OPTUNA)\")\n    print(\"=\"*80)\n    \n    def objective(trial):\n        # Sample hyperparameters\n        window_size = trial.suggest_categorical('window_size', [5, 10, 15, 20])\n\n        # Flatten parameter space: (d_model, n_heads) combinations\n        model_config = trial.suggest_categorical('model_config', [\n            (16, 2),\n            (24, 2),\n            (24, 4),\n            (32, 2),\n            (32, 4)\n        ])\n        d_model, n_heads = model_config\n\n        n_layers = trial.suggest_int('n_layers', 1, 2)\n        ffn_ratio = trial.suggest_categorical('ffn_ratio', [2, 3])\n        dropout = trial.suggest_float('dropout', 0.1, 0.3)\n        mask_ratio = trial.suggest_float('mask_ratio', 0.15, 0.30)\n        learning_rate = trial.suggest_float('learning_rate', 1e-4, 3e-3, log=True)\n        weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n        patience = trial.suggest_categorical('patience', [7, 10, 15])\n        \n        config = {\n            'window_size': window_size,\n            'd_model': d_model,\n            'n_heads': n_heads,\n            'n_layers': n_layers,\n            'ffn_ratio': ffn_ratio,\n            'dropout': dropout,\n            'mask_ratio': mask_ratio,\n            'learning_rate': learning_rate,\n            'weight_decay': weight_decay,\n            'patience': patience,\n            'max_epochs': 200,\n            'batch_size': 64\n        }\n        \n        # Create windows\n        try:\n            train_windows, _ = create_windows(train_data, window_size)\n            val_windows, _ = create_windows(val_data, window_size)\n        except ValueError as e:\n            print(f\"   Trial failed: {e}\")\n            return float('inf')\n        \n        # Create dataloaders\n        train_dataset = WindowDataset(train_windows)\n        val_dataset = WindowDataset(val_windows)\n        \n        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=False)\n        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n        \n        # Build model\n        model = TemporalContextTransformer(\n            input_dim=14,\n            d_model=d_model,\n            n_heads=n_heads,\n            n_layers=n_layers,\n            ffn_ratio=ffn_ratio,\n            dropout=dropout,\n            max_seq_len=20\n        )\n        \n        # Train\n        model, metrics = train_model(model, train_loader, val_loader, config)\n        \n        val_loss = metrics['val_loss']\n        overfit_ratio = metrics['overfit_ratio']\n        \n        # Pruning\n        trial.report(val_loss, step=metrics['epochs_trained'])\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n        \n        # Penalize overfitting\n        penalty = max(0, overfit_ratio - 1.5) * 0.1\n        \n        return val_loss + penalty\n    \n    # Run optimization\n    study = optuna.create_study(\n        direction='minimize',\n        sampler=TPESampler(seed=SEED),\n        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n    )\n    \n    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"OPTUNA RESULTS\")\n    print(\"=\"*80)\n    print(f\"Number of completed trials: {len(study.trials)}\")\n    print(f\"Best trial value: {study.best_value:.6f}\")\n    print(f\"Best parameters:\")\n    for key, value in study.best_params.items():\n        print(f\"   {key}: {value}\")\n    \n    return study.best_params, study.best_value, len(study.trials)\n\nprint(\"HPO function defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Run HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter optimization\n",
    "best_params, best_value, n_completed = run_hpo(\n",
    "    train_data, val_data,\n",
    "    n_trials=30,\n",
    "    timeout=1800  # 30 minutes\n",
    ")\n",
    "\n",
    "print(f\"\\n[OK] HPO complete. Best validation loss: {best_value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Final Training with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"FINAL TRAINING WITH BEST PARAMETERS\")\nprint(\"=\"*80)\n\n# Extract best hyperparameters\nwindow_size = best_params['window_size']\nd_model, n_heads = best_params['model_config']\nn_layers = best_params['n_layers']\nffn_ratio = best_params['ffn_ratio']\ndropout = best_params['dropout']\nmask_ratio = best_params['mask_ratio']\nlearning_rate = best_params['learning_rate']\nweight_decay = best_params['weight_decay']\npatience = best_params['patience']\n\n# Create windows for train+val combined (for final training)\ntrain_val_data = pd.concat([train_data, val_data])\ntrain_val_windows, train_val_dates = create_windows(train_val_data, window_size)\n\n# Create windows for validation (to monitor training)\nval_windows, val_dates = create_windows(val_data, window_size)\n\n# Create dataloaders\ntrain_val_dataset = WindowDataset(train_val_windows)\nval_dataset = WindowDataset(val_windows)\n\ntrain_val_loader = DataLoader(train_val_dataset, batch_size=64, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n# Build final model\nfinal_model = TemporalContextTransformer(\n    input_dim=14,\n    d_model=d_model,\n    n_heads=n_heads,\n    n_layers=n_layers,\n    ffn_ratio=ffn_ratio,\n    dropout=dropout,\n    max_seq_len=20\n)\n\n# Count parameters\nn_params = sum(p.numel() for p in final_model.parameters())\nprint(f\"\\nFinal model parameters: {n_params:,}\")\nprint(f\"Target range: 3,000-10,000\")\nprint(f\"Status: {'✓ OK' if 3000 <= n_params <= 10000 else '⚠ WARNING'}\")\n\n# Train final model\nfinal_config = {\n    'learning_rate': learning_rate,\n    'weight_decay': weight_decay,\n    'patience': patience,\n    'mask_ratio': mask_ratio,\n    'max_epochs': 200,\n    'batch_size': 64\n}\n\nfinal_model, final_metrics = train_model(final_model, train_val_loader, val_loader, final_config)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL TRAINING METRICS\")\nprint(\"=\"*80)\nfor key, value in final_metrics.items():\n    print(f\"{key}: {value}\")\n\n# Move model to CPU for inference\nfinal_model = final_model.cpu()\nfinal_model.eval()\n\nprint(\"\\n[OK] Final training complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Generate Submodel Output for Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING SUBMODEL OUTPUT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create windows for full dataset\n",
    "full_windows, full_dates = create_windows(full_data, window_size)\n",
    "\n",
    "print(f\"Full windows: {full_windows.shape}\")\n",
    "print(f\"Date range: {full_dates[0]} to {full_dates[-1]}\")\n",
    "\n",
    "# Extract context scores\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    context_scores = []\n",
    "    batch_size = 256\n",
    "    \n",
    "    for i in range(0, len(full_windows), batch_size):\n",
    "        batch = full_windows[i:i+batch_size]\n",
    "        scores = final_model.extract(batch)\n",
    "        context_scores.append(scores.cpu().numpy())\n",
    "    \n",
    "    context_scores = np.concatenate(context_scores, axis=0).flatten()\n",
    "\n",
    "# Create output DataFrame\n",
    "output = pd.DataFrame({\n",
    "    'date': full_dates,\n",
    "    'temporal_context_score': context_scores\n",
    "})\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Output columns: {list(output.columns)}\")\n",
    "print(f\"\\nOutput statistics:\")\n",
    "print(output['temporal_context_score'].describe())\n",
    "\n",
    "# Check for issues\n",
    "n_nan = output['temporal_context_score'].isna().sum()\n",
    "n_inf = np.isinf(output['temporal_context_score']).sum()\n",
    "is_constant = output['temporal_context_score'].std() < 1e-10\n",
    "\n",
    "print(f\"\\nQuality checks:\")\n",
    "print(f\"   NaN values: {n_nan} {'✓ OK' if n_nan == 0 else '✗ FAIL'}\")\n",
    "print(f\"   Inf values: {n_inf} {'✓ OK' if n_inf == 0 else '✗ FAIL'}\")\n",
    "print(f\"   Constant output: {'✗ FAIL' if is_constant else '✓ OK'}\")\n",
    "print(f\"   Range [0,1]: {'✓ OK' if output['temporal_context_score'].min() >= 0 and output['temporal_context_score'].max() <= 1 else '✗ FAIL'}\")\n",
    "\n",
    "print(\"\\n[OK] Submodel output generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save submodel output CSV\n",
    "output.to_csv(\"submodel_output.csv\", index=False)\n",
    "print(\"[OK] Saved: submodel_output.csv\")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state': final_model.state_dict(),\n",
    "    'config': {\n",
    "        'd_model': d_model,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layers': n_layers,\n",
    "        'ffn_ratio': ffn_ratio,\n",
    "        'dropout': dropout,\n",
    "        'window_size': window_size\n",
    "    }\n",
    "}, \"model.pt\")\n",
    "print(\"[OK] Saved: model.pt\")\n",
    "\n",
    "# Save training result JSON\n",
    "result = {\n",
    "    \"feature\": \"temporal_context\",\n",
    "    \"attempt\": 1,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"best_params\": best_params,\n",
    "    \"metrics\": final_metrics,\n",
    "    \"optuna_trials_completed\": n_completed,\n",
    "    \"optuna_best_value\": best_value,\n",
    "    \"model_param_count\": n_params,\n",
    "    \"output_shape\": list(output.shape),\n",
    "    \"output_columns\": list(output.columns),\n",
    "    \"output_statistics\": {\n",
    "        \"mean\": float(output['temporal_context_score'].mean()),\n",
    "        \"std\": float(output['temporal_context_score'].std()),\n",
    "        \"min\": float(output['temporal_context_score'].min()),\n",
    "        \"max\": float(output['temporal_context_score'].max()),\n",
    "        \"median\": float(output['temporal_context_score'].median())\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"train_samples\": len(train_data),\n",
    "        \"val_samples\": len(val_data),\n",
    "        \"test_samples\": len(test_data),\n",
    "        \"full_samples\": len(full_data),\n",
    "        \"window_size\": window_size,\n",
    "        \"windowed_samples\": len(output)\n",
    "    },\n",
    "    \"quality_checks\": {\n",
    "        \"nan_count\": int(n_nan),\n",
    "        \"inf_count\": int(n_inf),\n",
    "        \"is_constant\": bool(is_constant),\n",
    "        \"in_range_0_1\": bool(output['temporal_context_score'].min() >= 0 and output['temporal_context_score'].max() <= 1)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"training_result.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "\n",
    "print(\"[OK] Saved: training_result.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Finished: {datetime.now().isoformat()}\")\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  1. submodel_output.csv ({len(output)} rows)\")\n",
    "print(f\"  2. model.pt ({n_params:,} parameters)\")\n",
    "print(f\"  3. training_result.json\")\n",
    "print(\"\\nReady for evaluator.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}