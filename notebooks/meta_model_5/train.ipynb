{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Meta-Model Training - Attempt 5\n",
    "\n",
    "**Architecture:** Single XGBoost with reg:squarederror\n",
    "\n",
    "**Key Changes from Attempt 2:**\n",
    "- Add options_risk_regime_prob as 23rd feature (22 → 23 features)\n",
    "- Rebalance Optuna weights: HCDA 10% → 20%, Sharpe 50% → 40%\n",
    "- Widen HP search space: max_depth [2,5], n_estimators [100,1000]\n",
    "- Increase Optuna trials: 80 → 100\n",
    "- Remove confidence_threshold from Optuna (fixed at 80th percentile)\n",
    "- Fallback: Also evaluate Attempt 2 best_params on 23 features\n",
    "\n",
    "**Goal:** Close HCDA gap (55.26% → 60%+) through base model improvement\n",
    "\n",
    "**Design:** `docs/design/meta_model_attempt_5.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"Started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = [\n",
    "    # Base features (5)\n",
    "    'real_rate_change',\n",
    "    'dxy_change',\n",
    "    'vix',\n",
    "    'yield_spread_change',\n",
    "    'inflation_exp_change',\n",
    "    # VIX submodel (3)\n",
    "    'vix_regime_probability',\n",
    "    'vix_mean_reversion_z',\n",
    "    'vix_persistence',\n",
    "    # Technical submodel (3)\n",
    "    'tech_trend_regime_prob',\n",
    "    'tech_mean_reversion_z',\n",
    "    'tech_volatility_regime',\n",
    "    # Cross-asset submodel (3)\n",
    "    'xasset_regime_prob',\n",
    "    'xasset_recession_signal',\n",
    "    'xasset_divergence',\n",
    "    # Yield curve submodel (2)\n",
    "    'yc_spread_velocity_z',\n",
    "    'yc_curvature_z',\n",
    "    # ETF flow submodel (3)\n",
    "    'etf_regime_prob',\n",
    "    'etf_capital_intensity',\n",
    "    'etf_pv_divergence',\n",
    "    # Inflation expectation submodel (3)\n",
    "    'ie_regime_prob',\n",
    "    'ie_anchoring_z',\n",
    "    'ie_gold_sensitivity_z',\n",
    "    # Options market submodel (1) -- NEW IN ATTEMPT 5\n",
    "    'options_risk_regime_prob',\n",
    "]\n",
    "\n",
    "TARGET = 'gold_return_next'\n",
    "\n",
    "assert len(FEATURE_COLUMNS) == 23, f\"Expected 23 features, got {len(FEATURE_COLUMNS)}\"\n",
    "print(f\"Features defined: {len(FEATURE_COLUMNS)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading (Self-Contained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SELF-CONTAINED DATA LOADING\n",
    "# ============================================================\n",
    "# Fetches raw data via APIs and recreates the exact dataset\n",
    "# with 23 features (22 from Attempt 2 + options_risk_regime_prob)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FETCHING RAW DATA VIA APIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === 1. Install and import required libraries ===\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"yfinance\"], check=True)\n",
    "    import yfinance as yf\n",
    "\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"fredapi\"], check=True)\n",
    "    from fredapi import Fred\n",
    "\n",
    "# === 2. FRED API key from Kaggle Secrets ===\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    FRED_API_KEY = secrets.get_secret(\"FRED_API_KEY\")\n",
    "    print(\"✓ FRED API key loaded from Kaggle Secrets\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load FRED_API_KEY from Kaggle Secrets: {e}\")\n",
    "    raise\n",
    "\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "# === 3. Fetch base features (FRED + Yahoo Finance) ===\n",
    "print(\"\\nFetching base features...\")\n",
    "\n",
    "# Gold price (target)\n",
    "gold = yf.download(\"GC=F\", start=\"2014-01-01\", end=\"2025-02-15\", progress=False)\n",
    "gold_ret = gold['Close'].pct_change() * 100  # Convert to %\n",
    "gold_ret_next = gold_ret.shift(-1)  # Next-day return\n",
    "gold_df = pd.DataFrame({'gold_return_next': gold_ret_next.squeeze()}, index=gold.index)\n",
    "\n",
    "# Real rate (10Y TIPS)\n",
    "real_rate = fred.get_series('DFII10', observation_start='2014-01-01', observation_end='2025-02-15')\n",
    "real_rate_df = pd.DataFrame({'real_rate_real_rate': real_rate.squeeze()}, index=real_rate.index)\n",
    "\n",
    "# DXY (Dollar Index)\n",
    "dxy = yf.download(\"DX-Y.NYB\", start=\"2014-01-01\", end=\"2025-02-15\", progress=False)\n",
    "dxy_df = pd.DataFrame({'dxy_dxy': dxy['Close'].squeeze()}, index=dxy.index)\n",
    "\n",
    "# VIX\n",
    "vix = fred.get_series('VIXCLS', observation_start='2014-01-01', observation_end='2025-02-15')\n",
    "vix_df = pd.DataFrame({'vix_vix': vix.squeeze()}, index=vix.index)\n",
    "\n",
    "# Yield curve (10Y - 2Y spread)\n",
    "dgs10 = fred.get_series('DGS10', observation_start='2014-01-01', observation_end='2025-02-15')\n",
    "dgs2 = fred.get_series('DGS2', observation_start='2014-01-01', observation_end='2025-02-15')\n",
    "yield_spread = dgs10 - dgs2\n",
    "yield_df = pd.DataFrame({'yield_curve_yield_spread': yield_spread.squeeze()}, index=dgs10.index)\n",
    "\n",
    "# Inflation expectation (10Y Breakeven)\n",
    "inf_exp = fred.get_series('T10YIE', observation_start='2014-01-01', observation_end='2025-02-15')\n",
    "inf_df = pd.DataFrame({'inflation_expectation_inflation_expectation': inf_exp.squeeze()}, index=inf_exp.index)\n",
    "\n",
    "print(f\"  Gold: {len(gold_df)} rows\")\n",
    "print(f\"  Real rate: {len(real_rate_df)} rows\")\n",
    "print(f\"  DXY: {len(dxy_df)} rows\")\n",
    "print(f\"  VIX: {len(vix_df)} rows\")\n",
    "print(f\"  Yield spread: {len(yield_df)} rows\")\n",
    "print(f\"  Inflation exp: {len(inf_df)} rows\")\n",
    "\n",
    "# === 4. Merge base features ===\n",
    "base_df = gold_df.join([real_rate_df, dxy_df, vix_df, yield_df, inf_df], how='inner')\n",
    "base_df.index = pd.to_datetime(base_df.index).strftime('%Y-%m-%d')\n",
    "base_df.index.name = 'Date'\n",
    "print(f\"\\nBase features merged: {len(base_df)} rows\")\n",
    "\n",
    "# === 5. Load submodel outputs ===\n",
    "print(\"\\nLoading submodel outputs...\")\n",
    "\n",
    "submodel_files = {\n",
    "    'vix': {\n",
    "        'columns': ['vix_regime_probability', 'vix_mean_reversion_z', 'vix_persistence'],\n",
    "        'date_col': 'date',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'technical': {\n",
    "        'columns': ['tech_trend_regime_prob', 'tech_mean_reversion_z', 'tech_volatility_regime'],\n",
    "        'date_col': 'date',\n",
    "        'tz_aware': True,  # CRITICAL: timezone-aware dates\n",
    "    },\n",
    "    'cross_asset': {\n",
    "        'columns': ['xasset_regime_prob', 'xasset_recession_signal', 'xasset_divergence'],\n",
    "        'date_col': 'Date',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'yield_curve': {\n",
    "        'columns': ['yc_spread_velocity_z', 'yc_curvature_z'],\n",
    "        'date_col': 'index',  # Special: rename from 'index'\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'etf_flow': {\n",
    "        'columns': ['etf_regime_prob', 'etf_capital_intensity', 'etf_pv_divergence'],\n",
    "        'date_col': 'Date',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'inflation_expectation': {\n",
    "        'columns': ['ie_regime_prob', 'ie_anchoring_z', 'ie_gold_sensitivity_z'],\n",
    "        'date_col': 'Unnamed: 0',  # Special: rename from 'Unnamed: 0'\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'options_market': {  # NEW IN ATTEMPT 5\n",
    "        'columns': ['options_risk_regime_prob'],\n",
    "        'date_col': 'Date',\n",
    "        'tz_aware': True,  # CRITICAL: timezone-aware dates (same as technical.csv)\n",
    "    },\n",
    "}\n",
    "\n",
    "submodel_dfs = {}\n",
    "for feature, spec in submodel_files.items():\n",
    "    try:\n",
    "        # Try loading from gold-prediction-complete dataset\n",
    "        df = pd.read_csv(f'../input/gold-prediction-complete/{feature}.csv')\n",
    "        \n",
    "        # Normalize date column\n",
    "        date_col = spec['date_col']\n",
    "        if spec['tz_aware']:\n",
    "            # CRITICAL: timezone-aware dates require utc=True\n",
    "            df['Date'] = pd.to_datetime(df[date_col], utc=True).dt.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            if date_col == 'index':\n",
    "                df['Date'] = pd.to_datetime(df.index).dt.strftime('%Y-%m-%d')\n",
    "            elif date_col == 'Unnamed: 0':\n",
    "                df['Date'] = pd.to_datetime(df['Unnamed: 0']).dt.strftime('%Y-%m-%d')\n",
    "            else:\n",
    "                df['Date'] = pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        df = df[['Date'] + spec['columns']]\n",
    "        df = df.set_index('Date')\n",
    "        submodel_dfs[feature] = df\n",
    "        print(f\"  {feature}: {len(df)} rows (tz_aware={spec['tz_aware']})\")\n",
    "    except Exception as e:\n",
    "        # Create placeholder (all zeros) - will be imputed later\n",
    "        print(f\"  {feature}: NOT FOUND ({e}), using placeholder\")\n",
    "        placeholder = pd.DataFrame(\n",
    "            np.zeros((len(base_df), len(spec['columns']))),\n",
    "            columns=spec['columns'],\n",
    "            index=base_df.index\n",
    "        )\n",
    "        submodel_dfs[feature] = placeholder\n",
    "\n",
    "# === 6. Merge all features ===\n",
    "all_dfs = [base_df] + list(submodel_dfs.values())\n",
    "merged_df = pd.concat(all_dfs, axis=1, join='inner')\n",
    "print(f\"\\nAll features merged: {len(merged_df)} rows, {merged_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation and NaN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Apply transformations (stationary conversion) ===\n",
    "print(\"\\nApplying transformations...\")\n",
    "\n",
    "# Create final feature DataFrame\n",
    "final_df = pd.DataFrame(index=merged_df.index)\n",
    "final_df['gold_return_next'] = merged_df['gold_return_next']\n",
    "\n",
    "# Base features (4 diff, 1 level)\n",
    "final_df['real_rate_change'] = merged_df['real_rate_real_rate'].diff()\n",
    "final_df['dxy_change'] = merged_df['dxy_dxy'].diff()\n",
    "final_df['vix'] = merged_df['vix_vix']  # Level (stationary)\n",
    "final_df['yield_spread_change'] = merged_df['yield_curve_yield_spread'].diff()\n",
    "final_df['inflation_exp_change'] = merged_df['inflation_expectation_inflation_expectation'].diff()\n",
    "\n",
    "# Submodel features (copy as-is)\n",
    "for feature, spec in submodel_files.items():\n",
    "    for col in spec['columns']:\n",
    "        final_df[col] = merged_df[col]\n",
    "\n",
    "print(f\"  Features after transformation: {final_df.shape[1]} columns\")\n",
    "\n",
    "# === 8. NaN Imputation (domain-specific) ===\n",
    "print(\"\\nApplying NaN imputation...\")\n",
    "\n",
    "nan_before = final_df.isna().sum().sum()\n",
    "print(f\"  NaN before imputation: {nan_before}\")\n",
    "\n",
    "# Regime probability columns → 0.5 (maximum uncertainty)\n",
    "regime_cols = ['vix_regime_probability', 'tech_trend_regime_prob', \n",
    "               'xasset_regime_prob', 'etf_regime_prob', 'ie_regime_prob',\n",
    "               'options_risk_regime_prob']  # NEW: include options_market\n",
    "for col in regime_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(0.5)\n",
    "\n",
    "# Z-score columns → 0.0 (at mean)\n",
    "z_cols = ['vix_mean_reversion_z', 'tech_mean_reversion_z', \n",
    "          'yc_spread_velocity_z', 'yc_curvature_z',\n",
    "          'etf_capital_intensity', 'etf_pv_divergence',\n",
    "          'ie_anchoring_z', 'ie_gold_sensitivity_z']\n",
    "for col in z_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(0.0)\n",
    "\n",
    "# Divergence/signal columns → 0.0 (neutral)\n",
    "div_cols = ['xasset_recession_signal', 'xasset_divergence']\n",
    "for col in div_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(0.0)\n",
    "\n",
    "# Continuous state columns → median\n",
    "cont_cols = ['tech_volatility_regime', 'vix_persistence']\n",
    "for col in cont_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].median())\n",
    "\n",
    "# Drop rows with NaN in target or base features (critical rows)\n",
    "final_df = final_df.dropna(subset=['gold_return_next', 'real_rate_change', 'dxy_change', \n",
    "                                     'vix', 'yield_spread_change', 'inflation_exp_change'])\n",
    "\n",
    "nan_after = final_df.isna().sum().sum()\n",
    "print(f\"  NaN after imputation: {nan_after}\")\n",
    "print(f\"  Final dataset: {len(final_df)} rows\")\n",
    "\n",
    "# === 9. Verify feature set ===\n",
    "assert all(col in final_df.columns for col in FEATURE_COLUMNS), \"Missing features after merge!\"\n",
    "assert TARGET in final_df.columns, \"Target not found!\"\n",
    "print(f\"\\n✓ All {len(FEATURE_COLUMNS)} features present\")\n",
    "print(f\"✓ Dataset shape: {final_df.shape}\")\n",
    "print(f\"✓ Date range: {final_df.index.min()} to {final_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val/Test Split (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. Train/Val/Test Split (70/15/15, time-series order) ===\n",
    "n_total = len(final_df)\n",
    "n_train = int(n_total * 0.70)\n",
    "n_val = int(n_total * 0.15)\n",
    "\n",
    "train_df = final_df.iloc[:n_train].copy()\n",
    "val_df = final_df.iloc[n_train:n_train+n_val].copy()\n",
    "test_df = final_df.iloc[n_train+n_val:].copy()\n",
    "\n",
    "print(f\"\\n✓ Data split complete:\")\n",
    "print(f\"  Train: {len(train_df)} rows ({len(train_df)/n_total*100:.1f}%) - {train_df.index.min()} to {train_df.index.max()}\")\n",
    "print(f\"  Val:   {len(val_df)} rows ({len(val_df)/n_total*100:.1f}%) - {val_df.index.min()} to {val_df.index.max()}\")\n",
    "print(f\"  Test:  {len(test_df)} rows ({len(test_df)/n_total*100:.1f}%) - {test_df.index.min()} to {test_df.index.max()}\")\n",
    "print(f\"  Total: {n_total} rows\")\n",
    "print(f\"  Samples per feature: {n_train / len(FEATURE_COLUMNS):.1f}:1 (train)\")\n",
    "\n",
    "# Verify no data leakage\n",
    "assert train_df.index.max() < val_df.index.min(), \"Train-val overlap detected!\"\n",
    "assert val_df.index.max() < test_df.index.min(), \"Val-test overlap detected!\"\n",
    "print(f\"\\n✓ No time-series leakage detected\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare arrays for training\n",
    "X_train = train_df[FEATURE_COLUMNS].values\n",
    "y_train = train_df[TARGET].values\n",
    "\n",
    "X_val = val_df[FEATURE_COLUMNS].values\n",
    "y_val = val_df[TARGET].values\n",
    "\n",
    "X_test = test_df[FEATURE_COLUMNS].values\n",
    "y_test = test_df[TARGET].values\n",
    "\n",
    "# Store dates for output\n",
    "dates_train = train_df.index\n",
    "dates_val = val_df.index\n",
    "dates_test = test_df.index\n",
    "\n",
    "print(f\"\\nArray shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_direction_accuracy(y_true, y_pred):\n",
    "    \"\"\"Direction accuracy, excluding zeros.\"\"\"\n",
    "    mask = (y_true != 0) & (y_pred != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return (np.sign(y_pred[mask]) == np.sign(y_true[mask])).mean()\n",
    "\n",
    "def compute_mae(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Error.\"\"\"\n",
    "    return np.abs(y_pred - y_true).mean()\n",
    "\n",
    "def compute_sharpe_trade_cost(y_true, y_pred, cost_bps=5.0):\n",
    "    \"\"\"Sharpe ratio with position-change cost (5bps per change).\"\"\"\n",
    "    positions = np.sign(y_pred)\n",
    "    \n",
    "    # Strategy returns (position * actual return)\n",
    "    strategy_returns = positions * y_true / 100.0  # Convert % to decimal\n",
    "    \n",
    "    # Position changes\n",
    "    position_changes = np.abs(np.diff(positions, prepend=0))\n",
    "    trade_costs = position_changes * (cost_bps / 10000.0)  # 5bps = 0.0005\n",
    "    \n",
    "    # Net returns\n",
    "    net_returns = strategy_returns - trade_costs\n",
    "    \n",
    "    # Annualized Sharpe (252 trading days)\n",
    "    if len(net_returns) < 2 or net_returns.std() == 0:\n",
    "        return 0.0\n",
    "    return (net_returns.mean() / net_returns.std()) * np.sqrt(252)\n",
    "\n",
    "def compute_hcda(y_true, y_pred, threshold_percentile=80):\n",
    "    \"\"\"High-confidence direction accuracy (top 20% by |prediction|).\"\"\"\n",
    "    threshold = np.percentile(np.abs(y_pred), threshold_percentile)\n",
    "    hc_mask = np.abs(y_pred) > threshold\n",
    "    \n",
    "    if hc_mask.sum() == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    coverage = hc_mask.sum() / len(y_pred)\n",
    "    hc_pred = y_pred[hc_mask]\n",
    "    hc_actual = y_true[hc_mask]\n",
    "    \n",
    "    mask = (hc_actual != 0) & (hc_pred != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0, coverage\n",
    "    \n",
    "    da = (np.sign(hc_pred[mask]) == np.sign(hc_actual[mask])).mean()\n",
    "    return da, coverage\n",
    "\n",
    "print(\"Metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna HPO (100 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_objective(trial):\n",
    "    \"\"\"Optuna objective function with Attempt 5 specifications.\"\"\"\n",
    "    \n",
    "    # === Sample hyperparameters (8 parameters) ===\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.7),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'rmse',\n",
    "        'verbosity': 0,\n",
    "        'seed': 42 + trial.number,\n",
    "    }\n",
    "    \n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    \n",
    "    # === Train model ===\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=n_estimators, early_stopping_rounds=50)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    # === Predictions ===\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    \n",
    "    # === Compute metrics ===\n",
    "    train_da = compute_direction_accuracy(y_train, train_pred)\n",
    "    val_da = compute_direction_accuracy(y_val, val_pred)\n",
    "    val_mae = compute_mae(y_val, val_pred)\n",
    "    val_sharpe = compute_sharpe_trade_cost(y_val, val_pred)\n",
    "    val_hc_da, val_hc_coverage = compute_hcda(y_val, val_pred, threshold_percentile=80)\n",
    "    \n",
    "    # === Overfitting penalty ===\n",
    "    da_gap = (train_da - val_da) * 100  # In percentage points\n",
    "    overfit_penalty = max(0.0, (da_gap - 10.0) / 30.0)  # 0 if gap<=10pp, up to 1.0 if gap=40pp\n",
    "    \n",
    "    # === Normalize metrics to [0, 1] ===\n",
    "    sharpe_norm = np.clip((val_sharpe + 3.0) / 6.0, 0.0, 1.0)   # [-3, +3] -> [0, 1]\n",
    "    da_norm = np.clip((val_da * 100 - 40.0) / 30.0, 0.0, 1.0)   # [40%, 70%] -> [0, 1]\n",
    "    mae_norm = np.clip((1.0 - val_mae) / 0.5, 0.0, 1.0)         # [0.5%, 1.0%] -> [0, 1]\n",
    "    hc_da_norm = np.clip((val_hc_da * 100 - 40.0) / 30.0, 0.0, 1.0)  # [40%, 70%] -> [0, 1]\n",
    "    \n",
    "    # === Weighted composite (ATTEMPT 5 WEIGHTS) ===\n",
    "    objective = (\n",
    "        0.40 * sharpe_norm +     # Reduced from 0.50 (Attempt 2)\n",
    "        0.30 * da_norm +         # Unchanged\n",
    "        0.10 * mae_norm +        # Unchanged\n",
    "        0.20 * hc_da_norm        # Increased from 0.10 (Attempt 2)\n",
    "    ) - 0.30 * overfit_penalty   # Same penalty as Attempt 2\n",
    "    \n",
    "    # === Log trial details ===\n",
    "    trial.set_user_attr('val_da', float(val_da))\n",
    "    trial.set_user_attr('val_mae', float(val_mae))\n",
    "    trial.set_user_attr('val_sharpe', float(val_sharpe))\n",
    "    trial.set_user_attr('val_hc_da', float(val_hc_da))\n",
    "    trial.set_user_attr('val_hc_coverage', float(val_hc_coverage))\n",
    "    trial.set_user_attr('train_da', float(train_da))\n",
    "    trial.set_user_attr('da_gap_pp', float(da_gap))\n",
    "    trial.set_user_attr('n_estimators_used',\n",
    "                         int(model.best_iteration + 1) if hasattr(model, 'best_iteration')\n",
    "                         and model.best_iteration is not None else n_estimators)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "print(\"Optuna objective function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING OPTUNA HPO (100 trials, 2-hour timeout)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    optuna_objective,\n",
    "    n_trials=100,\n",
    "    timeout=7200,  # 2 hours\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOptuna optimization complete\")\n",
    "print(f\"  Trials completed: {len(study.trials)}\")\n",
    "print(f\"  Best value: {study.best_value:.4f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest trial validation metrics:\")\n",
    "print(f\"  DA:     {best_trial.user_attrs['val_da']*100:.2f}%\")\n",
    "print(f\"  HCDA:   {best_trial.user_attrs['val_hc_da']*100:.2f}%\")\n",
    "print(f\"  MAE:    {best_trial.user_attrs['val_mae']:.4f}%\")\n",
    "print(f\"  Sharpe: {best_trial.user_attrs['val_sharpe']:.2f}\")\n",
    "print(f\"  DA gap: {best_trial.user_attrs['da_gap_pp']:.2f}pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallback: Attempt 2 Best Params on 23 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FALLBACK: Testing Attempt 2 Best Params on 23 Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Attempt 2 best params (from evaluation JSON)\n",
    "FALLBACK_PARAMS = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 14,\n",
    "    'reg_lambda': 4.76,\n",
    "    'reg_alpha': 3.65,\n",
    "    'subsample': 0.478,\n",
    "    'colsample_bytree': 0.371,\n",
    "    'learning_rate': 0.025,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'verbosity': 0,\n",
    "    'seed': 42,\n",
    "}\n",
    "FALLBACK_N_ESTIMATORS = 247\n",
    "\n",
    "print(\"Training fallback model...\")\n",
    "fallback_model = xgb.XGBRegressor(**FALLBACK_PARAMS, n_estimators=FALLBACK_N_ESTIMATORS, early_stopping_rounds=50)\n",
    "fallback_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "# Predictions\n",
    "fallback_train_pred = fallback_model.predict(X_train)\n",
    "fallback_val_pred = fallback_model.predict(X_val)\n",
    "\n",
    "# Metrics\n",
    "fallback_train_da = compute_direction_accuracy(y_train, fallback_train_pred)\n",
    "fallback_val_da = compute_direction_accuracy(y_val, fallback_val_pred)\n",
    "fallback_val_mae = compute_mae(y_val, fallback_val_pred)\n",
    "fallback_val_sharpe = compute_sharpe_trade_cost(y_val, fallback_val_pred)\n",
    "fallback_val_hc_da, _ = compute_hcda(y_val, fallback_val_pred, threshold_percentile=80)\n",
    "fallback_da_gap = (fallback_train_da - fallback_val_da) * 100\n",
    "\n",
    "# Composite objective (same formula as Optuna)\n",
    "sharpe_norm_fb = np.clip((fallback_val_sharpe + 3.0) / 6.0, 0.0, 1.0)\n",
    "da_norm_fb = np.clip((fallback_val_da * 100 - 40.0) / 30.0, 0.0, 1.0)\n",
    "mae_norm_fb = np.clip((1.0 - fallback_val_mae) / 0.5, 0.0, 1.0)\n",
    "hc_da_norm_fb = np.clip((fallback_val_hc_da * 100 - 40.0) / 30.0, 0.0, 1.0)\n",
    "overfit_penalty_fb = max(0.0, (fallback_da_gap - 10.0) / 30.0)\n",
    "fallback_objective = (\n",
    "    0.40 * sharpe_norm_fb + 0.30 * da_norm_fb + 0.10 * mae_norm_fb + 0.20 * hc_da_norm_fb\n",
    ") - 0.30 * overfit_penalty_fb\n",
    "\n",
    "print(f\"\\nFallback validation metrics:\")\n",
    "print(f\"  DA:     {fallback_val_da*100:.2f}%\")\n",
    "print(f\"  HCDA:   {fallback_val_hc_da*100:.2f}%\")\n",
    "print(f\"  MAE:    {fallback_val_mae:.4f}%\")\n",
    "print(f\"  Sharpe: {fallback_val_sharpe:.2f}\")\n",
    "print(f\"  DA gap: {fallback_da_gap:.2f}pp\")\n",
    "print(f\"  Composite objective: {fallback_objective:.4f}\")\n",
    "\n",
    "print(f\"\\nOptuna best vs Fallback:\")\n",
    "print(f\"  Optuna objective: {study.best_value:.4f}\")\n",
    "print(f\"  Fallback objective: {fallback_objective:.4f}\")\n",
    "\n",
    "# Select configuration\n",
    "if study.best_value >= fallback_objective:\n",
    "    print(\"\\n✓ Using Optuna best configuration\")\n",
    "    selected_config = 'optuna'\n",
    "    selected_params = study.best_params\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        max_depth=selected_params['max_depth'],\n",
    "        min_child_weight=selected_params['min_child_weight'],\n",
    "        subsample=selected_params['subsample'],\n",
    "        colsample_bytree=selected_params['colsample_bytree'],\n",
    "        reg_lambda=selected_params['reg_lambda'],\n",
    "        reg_alpha=selected_params['reg_alpha'],\n",
    "        learning_rate=selected_params['learning_rate'],\n",
    "        tree_method='hist',\n",
    "        eval_metric='rmse',\n",
    "        verbosity=0,\n",
    "        seed=42,\n",
    "        n_estimators=selected_params['n_estimators'],\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n✓ Using Attempt 2 fallback configuration\")\n",
    "    selected_config = 'fallback'\n",
    "    selected_params = FALLBACK_PARAMS.copy()\n",
    "    selected_params['n_estimators'] = FALLBACK_N_ESTIMATORS\n",
    "    final_model = xgb.XGBRegressor(**FALLBACK_PARAMS, n_estimators=FALLBACK_N_ESTIMATORS, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TRAINING FINAL MODEL ({selected_config.upper()} CONFIG)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "# Generate predictions\n",
    "pred_train = final_model.predict(X_train)\n",
    "pred_val = final_model.predict(X_val)\n",
    "pred_test = final_model.predict(X_test)\n",
    "\n",
    "# Combine all predictions for full dataset\n",
    "pred_full = np.concatenate([pred_train, pred_val, pred_test])\n",
    "dates_full = pd.Index(list(dates_train) + list(dates_val) + list(dates_test))\n",
    "y_full = np.concatenate([y_train, y_val, y_test])\n",
    "\n",
    "print(\"\\nPredictions generated:\")\n",
    "print(f\"  Train: mean={pred_train.mean():.4f}, std={pred_train.std():.4f}\")\n",
    "print(f\"  Val:   mean={pred_val.mean():.4f}, std={pred_val.std():.4f}\")\n",
    "print(f\"  Test:  mean={pred_test.mean():.4f}, std={pred_test.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on All Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute metrics for all splits\n",
    "metrics_all = {}\n",
    "for split_name, y_true, y_pred in [\n",
    "    ('train', y_train, pred_train),\n",
    "    ('val', y_val, pred_val),\n",
    "    ('test', y_test, pred_test),\n",
    "]:\n",
    "    da = compute_direction_accuracy(y_true, y_pred)\n",
    "    mae = compute_mae(y_true, y_pred)\n",
    "    sharpe = compute_sharpe_trade_cost(y_true, y_pred)\n",
    "    hc_da, hc_coverage = compute_hcda(y_true, y_pred, threshold_percentile=80)\n",
    "    \n",
    "    metrics_all[split_name] = {\n",
    "        'direction_accuracy': float(da),\n",
    "        'high_confidence_da': float(hc_da),\n",
    "        'high_confidence_coverage': float(hc_coverage),\n",
    "        'mae': float(mae),\n",
    "        'sharpe_ratio': float(sharpe),\n",
    "    }\n",
    "\n",
    "# Print metrics\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    m = metrics_all[split_name]\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    print(f\"  DA:     {m['direction_accuracy']*100:.2f}%\")\n",
    "    print(f\"  HCDA:   {m['high_confidence_da']*100:.2f}% (coverage: {m['high_confidence_coverage']*100:.1f}%)\")\n",
    "    print(f\"  MAE:    {m['mae']:.4f}%\")\n",
    "    print(f\"  Sharpe: {m['sharpe_ratio']:.2f}\")\n",
    "\n",
    "# Overfitting analysis\n",
    "train_test_da_gap = (metrics_all['train']['direction_accuracy'] - metrics_all['test']['direction_accuracy']) * 100\n",
    "print(f\"\\nOVERFITTING:\")\n",
    "print(f\"  Train-Test DA gap: {train_test_da_gap:.2f}pp (target: <10pp)\")\n",
    "\n",
    "# Target evaluation\n",
    "test_m = metrics_all['test']\n",
    "targets_met = [\n",
    "    test_m['direction_accuracy'] > 0.56,\n",
    "    test_m['high_confidence_da'] > 0.60,\n",
    "    test_m['mae'] < 0.0075,\n",
    "    test_m['sharpe_ratio'] > 0.8,\n",
    "]\n",
    "\n",
    "print(f\"\\nTARGET STATUS:\")\n",
    "print(f\"  DA > 56%:     {'✓' if targets_met[0] else '✗'} ({test_m['direction_accuracy']*100:.2f}%)\")\n",
    "print(f\"  HCDA > 60%:   {'✓' if targets_met[1] else '✗'} ({test_m['high_confidence_da']*100:.2f}%)\")\n",
    "print(f\"  MAE < 0.75%:  {'✓' if targets_met[2] else '✗'} ({test_m['mae']:.4f}%)\")\n",
    "print(f\"  Sharpe > 0.8: {'✓' if targets_met[3] else '✗'} ({test_m['sharpe_ratio']:.2f})\")\n",
    "print(f\"\\nTargets passed: {sum(targets_met)}/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. HCDA at multiple thresholds\n",
    "print(\"\\nHCDA at different confidence thresholds (test set):\")\n",
    "for pct in [70, 75, 80, 85, 90]:\n",
    "    hc_da, hc_cov = compute_hcda(y_test, pred_test, threshold_percentile=pct)\n",
    "    n_samples = int(len(y_test) * hc_cov)\n",
    "    print(f\"  Top {100-pct}% (N={n_samples}): {hc_da*100:.2f}%\")\n",
    "\n",
    "# 2. Feature importance\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'feature': FEATURE_COLUMNS,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importance:\")\n",
    "for i, row in feature_ranking.head(10).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Find options_risk_regime_prob rank\n",
    "options_rank = (feature_ranking.reset_index(drop=True).reset_index()\n",
    "                .loc[feature_ranking['feature'] == 'options_risk_regime_prob', 'index'].values[0] + 1)\n",
    "options_importance = feature_ranking.loc[feature_ranking['feature'] == 'options_risk_regime_prob', 'importance'].values[0]\n",
    "print(f\"\\noptions_risk_regime_prob: Rank {options_rank}/23, Importance {options_importance:.4f}\")\n",
    "\n",
    "# 3. Prediction distribution\n",
    "print(\"\\nPrediction distribution (test set):\")\n",
    "print(f\"  Mean:     {pred_test.mean():.4f}%\")\n",
    "print(f\"  Std:      {pred_test.std():.4f}%\")\n",
    "print(f\"  Min:      {pred_test.min():.4f}%\")\n",
    "print(f\"  Max:      {pred_test.max():.4f}%\")\n",
    "print(f\"  Positive: {(pred_test > 0).sum() / len(pred_test) * 100:.1f}%\")\n",
    "\n",
    "# 4. Comparison with baselines\n",
    "naive_always_up_da = (y_test > 0).sum() / len(y_test)\n",
    "print(f\"\\nNaive Baseline:\")\n",
    "print(f\"  Always-up DA: {naive_always_up_da*100:.2f}%\")\n",
    "print(f\"  Model vs naive: {(test_m['direction_accuracy'] - naive_always_up_da)*100:+.2f}pp\")\n",
    "\n",
    "# 5. Attempt 2 comparison\n",
    "print(\"\\nVs Attempt 2:\")\n",
    "print(f\"  DA:     {test_m['direction_accuracy']*100:.2f}% (Attempt 2: 57.26%, delta: {(test_m['direction_accuracy']-0.5726)*100:+.2f}pp)\")\n",
    "print(f\"  HCDA:   {test_m['high_confidence_da']*100:.2f}% (Attempt 2: 55.26%, delta: {(test_m['high_confidence_da']-0.5526)*100:+.2f}pp)\")\n",
    "print(f\"  MAE:    {test_m['mae']:.4f}% (Attempt 2: 0.6877%, delta: {(test_m['mae']-0.6877)*100:+.2f}pp)\")\n",
    "print(f\"  Sharpe: {test_m['sharpe_ratio']:.2f} (Attempt 2: 1.58, delta: {test_m['sharpe_ratio']-1.5835:+.2f})\")\n",
    "\n",
    "# 6. Quarterly breakdown\n",
    "test_df_with_pred = test_df.copy()\n",
    "test_df_with_pred['prediction'] = pred_test\n",
    "test_df_with_pred['quarter'] = pd.to_datetime(test_df_with_pred.index).to_period('Q')\n",
    "\n",
    "print(\"\\nQuarterly Performance (test set):\")\n",
    "for quarter in test_df_with_pred['quarter'].unique():\n",
    "    qtr_data = test_df_with_pred[test_df_with_pred['quarter'] == quarter]\n",
    "    qtr_da = compute_direction_accuracy(qtr_data[TARGET].values, qtr_data['prediction'].values)\n",
    "    qtr_mae = compute_mae(qtr_data[TARGET].values, qtr_data['prediction'].values)\n",
    "    qtr_sharpe = compute_sharpe_trade_cost(qtr_data[TARGET].values, qtr_data['prediction'].values)\n",
    "    print(f\"  {quarter}: DA={qtr_da*100:.1f}%, MAE={qtr_mae:.3f}%, Sharpe={qtr_sharpe:.2f}, N={len(qtr_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. predictions.csv (full dataset)\n",
    "split_labels = ['train'] * len(dates_train) + ['val'] * len(dates_val) + ['test'] * len(dates_test)\n",
    "predictions_df = pd.DataFrame({\n",
    "    'date': dates_full,\n",
    "    'split': split_labels,\n",
    "    'actual': y_full,\n",
    "    'prediction': pred_full,\n",
    "    'direction_correct': (np.sign(pred_full) == np.sign(y_full)).astype(int),\n",
    "    'abs_prediction': np.abs(pred_full),\n",
    "})\n",
    "\n",
    "# Add high_confidence flag (80th percentile)\n",
    "threshold_80 = np.percentile(np.abs(pred_full), 80)\n",
    "predictions_df['high_confidence'] = (predictions_df['abs_prediction'] > threshold_80).astype(int)\n",
    "\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"✓ Saved predictions.csv\")\n",
    "\n",
    "# 2. test_predictions.csv (test set only)\n",
    "test_predictions_df = predictions_df[predictions_df['split'] == 'test'].copy()\n",
    "test_predictions_df.to_csv('test_predictions.csv', index=False)\n",
    "print(\"✓ Saved test_predictions.csv\")\n",
    "\n",
    "# 3. submodel_output.csv (for pipeline compatibility)\n",
    "predictions_df.to_csv('submodel_output.csv', index=False)\n",
    "print(\"✓ Saved submodel_output.csv\")\n",
    "\n",
    "# 4. model.json (XGBoost model)\n",
    "final_model.save_model('model.json')\n",
    "print(\"✓ Saved model.json\")\n",
    "\n",
    "# 5. training_result.json\n",
    "training_result = {\n",
    "    'feature': 'meta_model',\n",
    "    'attempt': 5,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'architecture': 'XGBoost reg:squarederror',\n",
    "    'phase': '3_meta_model',\n",
    "    \n",
    "    'model_config': {\n",
    "        'n_features': 23,\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'samples_per_feature_ratio': len(X_train) / 23,\n",
    "        'selected_configuration': selected_config,\n",
    "        'optuna_trials_completed': len(study.trials),\n",
    "        'best_params': selected_params,\n",
    "    },\n",
    "    \n",
    "    'optuna_search': {\n",
    "        'n_trials': len(study.trials),\n",
    "        'best_value': float(study.best_value),\n",
    "        'best_trial_number': study.best_trial.number,\n",
    "        'top_5_trials': [\n",
    "            {\n",
    "                'number': t.number,\n",
    "                'value': float(t.value),\n",
    "                'params': t.params,\n",
    "                'val_da': float(t.user_attrs['val_da']),\n",
    "                'val_hc_da': float(t.user_attrs['val_hc_da']),\n",
    "            }\n",
    "            for t in sorted(study.trials, key=lambda x: x.value, reverse=True)[:5]\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    'fallback_comparison': {\n",
    "        'fallback_objective': float(fallback_objective),\n",
    "        'optuna_objective': float(study.best_value),\n",
    "        'selected': selected_config,\n",
    "        'fallback_metrics': {\n",
    "            'da': float(fallback_val_da),\n",
    "            'hcda': float(fallback_val_hc_da),\n",
    "            'mae': float(fallback_val_mae),\n",
    "            'sharpe': float(fallback_val_sharpe),\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    'metrics': metrics_all,\n",
    "    \n",
    "    'target_evaluation': {\n",
    "        'direction_accuracy': {\n",
    "            'target': '> 56.0%',\n",
    "            'actual': f\"{test_m['direction_accuracy']*100:.2f}%\",\n",
    "            'gap': f\"{(test_m['direction_accuracy'] - 0.56)*100:+.2f}pp\",\n",
    "            'passed': bool(targets_met[0]),\n",
    "        },\n",
    "        'high_confidence_da': {\n",
    "            'target': '> 60.0%',\n",
    "            'actual': f\"{test_m['high_confidence_da']*100:.2f}%\",\n",
    "            'gap': f\"{(test_m['high_confidence_da'] - 0.60)*100:+.2f}pp\",\n",
    "            'passed': bool(targets_met[1]),\n",
    "        },\n",
    "        'mae': {\n",
    "            'target': '< 0.75%',\n",
    "            'actual': f\"{test_m['mae']:.4f}%\",\n",
    "            'gap': f\"{(0.0075 - test_m['mae']):.4f}%\",\n",
    "            'passed': bool(targets_met[2]),\n",
    "        },\n",
    "        'sharpe_ratio': {\n",
    "            'target': '> 0.80',\n",
    "            'actual': f\"{test_m['sharpe_ratio']:.2f}\",\n",
    "            'gap': f\"{(test_m['sharpe_ratio'] - 0.8):+.2f}\",\n",
    "            'passed': bool(targets_met[3]),\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    'targets_passed': sum(targets_met),\n",
    "    'targets_total': 4,\n",
    "    'overall_passed': all(targets_met),\n",
    "    \n",
    "    'overfitting_analysis': {\n",
    "        'train_test_da_gap_pp': float(train_test_da_gap),\n",
    "        'target_gap_pp': 10.0,\n",
    "        'overfitting_check': 'PASS' if train_test_da_gap < 10 else 'FAIL',\n",
    "    },\n",
    "    \n",
    "    'feature_importance': {\n",
    "        'top_10': feature_ranking.head(10).to_dict('records'),\n",
    "        'options_risk_regime_prob_rank': int(options_rank),\n",
    "        'options_risk_regime_prob_importance': float(options_importance),\n",
    "    },\n",
    "    \n",
    "    'vs_attempt_2': {\n",
    "        'da_delta_pp': float((test_m['direction_accuracy'] - 0.5726) * 100),\n",
    "        'hcda_delta_pp': float((test_m['high_confidence_da'] - 0.5526) * 100),\n",
    "        'mae_delta': float(test_m['mae'] - 0.6877),\n",
    "        'sharpe_delta': float(test_m['sharpe_ratio'] - 1.5835),\n",
    "    },\n",
    "    \n",
    "    'vs_naive': {\n",
    "        'naive_always_up_da': f\"{naive_always_up_da*100:.2f}%\",\n",
    "        'model_vs_naive_pp': float((test_m['direction_accuracy'] - naive_always_up_da) * 100),\n",
    "    },\n",
    "    \n",
    "    'prediction_characteristics': {\n",
    "        'mean': float(pred_test.mean()),\n",
    "        'std': float(pred_test.std()),\n",
    "        'min': float(pred_test.min()),\n",
    "        'max': float(pred_test.max()),\n",
    "        'positive_pct': float((pred_test > 0).sum() / len(pred_test) * 100),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(training_result, f, indent=2, default=str)\n",
    "print(\"✓ Saved training_result.json\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Finished: {datetime.now().isoformat()}\")\n",
    "print(f\"\\nFinal Status:\")\n",
    "print(f\"  Configuration: {selected_config.upper()}\")\n",
    "print(f\"  Targets passed: {sum(targets_met)}/4\")\n",
    "if all(targets_met):\n",
    "    print(f\"  ✓✓✓ ALL TARGETS MET ✓✓✓\")\n",
    "else:\n",
    "    failed = [t for t, m in zip(['DA', 'HCDA', 'MAE', 'Sharpe'], targets_met) if not m]\n",
    "    print(f\"  Improvements needed on: {failed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
