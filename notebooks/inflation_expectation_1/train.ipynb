{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e025b4",
   "metadata": {},
   "source": [
    "# Inflation Expectation SubModel Training - Attempt 1\n",
    "\n",
    "Self-contained training script with embedded FRED_API_KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dcc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gold Prediction SubModel Training - Inflation Expectation Attempt 1\n",
    "Self-contained: Data fetch -> Preprocessing -> Training -> Evaluation -> Save results\n",
    "\n",
    "Architecture: 2D HMM on [ie_change, ie_vol_5d]\n",
    "              + IE anchoring z-score (vol z-scored)\n",
    "              + IE-gold sensitivity z-score (5d corr z-scored)\n",
    "\n",
    "Output: 3 features\n",
    "  - ie_regime_prob: Probability of high-variance IE regime (HMM)\n",
    "  - ie_anchoring_z: IE change volatility z-score vs baseline\n",
    "  - ie_gold_sensitivity_z: 5d IE-gold correlation z-scored vs baseline\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# === 1. Data Fetching ===\n",
    "def fetch_and_preprocess():\n",
    "    \"\"\"Self-contained. Fetches T10YIE from FRED and GC=F from Yahoo Finance.\n",
    "    Returns: (train_df, val_df, test_df, full_df)\n",
    "    \"\"\"\n",
    "    # --- FRED API ---\n",
    "    try:\n",
    "        from fredapi import Fred\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"fredapi\"], check=True)\n",
    "        from fredapi import Fred\n",
    "\n",
    "    # --- Yahoo Finance ---\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"yfinance\"], check=True)\n",
    "        import yfinance as yf\n",
    "\n",
    "    # FRED API key (embedded for Kaggle execution)\n",
    "    api_key = \"3ffb68facdf6321e180e380c00e909c8\"\n",
    "\n",
    "    fred = Fred(api_key=api_key)\n",
    "\n",
    "    # Fetch T10YIE (10-Year Breakeven Inflation Rate)\n",
    "    # Start 2014-06-01 for warmup buffer (120d baseline + additional margin)\n",
    "    print(\"Fetching T10YIE from FRED...\")\n",
    "    t10yie = fred.get_series('T10YIE', observation_start='2014-06-01')\n",
    "\n",
    "    if len(t10yie) < 1000:\n",
    "        raise RuntimeError(f\"Insufficient T10YIE data: only {len(t10yie)} observations\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({'T10YIE': t10yie})\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Fetch GC=F (Gold Futures) for gold returns\n",
    "    print(\"Fetching GC=F from Yahoo Finance...\")\n",
    "    gc = yf.download('GC=F', start='2014-06-01', progress=False)\n",
    "\n",
    "    if gc.empty:\n",
    "        raise RuntimeError(\"Failed to fetch GC=F data from Yahoo Finance\")\n",
    "\n",
    "    # Flatten MultiIndex columns if present\n",
    "    if isinstance(gc.columns, pd.MultiIndex):\n",
    "        gc.columns = gc.columns.get_level_values(0)\n",
    "\n",
    "    gc_close = gc['Close']\n",
    "    gc_close.index = pd.to_datetime(gc_close.index)\n",
    "\n",
    "    # Align dates (inner join on common trading days)\n",
    "    # FRED data includes weekends/holidays with same values, Yahoo only has trading days\n",
    "    df = df.join(gc_close.rename('gc_close'), how='inner')\n",
    "\n",
    "    # Forward-fill gaps up to 3 trading days\n",
    "    df = df.ffill(limit=3)\n",
    "\n",
    "    # Drop any remaining NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # === Compute Derived Features ===\n",
    "\n",
    "    # 1. IE daily change (basis for all features)\n",
    "    df['ie_change'] = df['T10YIE'].diff()\n",
    "\n",
    "    # 2. Gold returns (current-day, not next-day - for sensitivity feature)\n",
    "    df['gold_return'] = df['gc_close'].pct_change()\n",
    "\n",
    "    # 3. IE volatility windows (for HMM input and anchoring feature)\n",
    "    df['ie_vol_5d'] = df['ie_change'].rolling(5).std()\n",
    "    df['ie_vol_10d'] = df['ie_change'].rolling(10).std()\n",
    "    df['ie_vol_20d'] = df['ie_change'].rolling(20).std()\n",
    "\n",
    "    # Drop rows with NaN from rolling operations\n",
    "    df = df.dropna()\n",
    "\n",
    "    # === Basic Validation ===\n",
    "\n",
    "    # Check row count\n",
    "    if len(df) < 2000:\n",
    "        raise RuntimeError(f\"Insufficient data: only {len(df)} rows after preprocessing\")\n",
    "\n",
    "    # Check T10YIE range (breakeven rates are percentages, typically 0-5%)\n",
    "    if not (0 <= df['T10YIE'].min() <= df['T10YIE'].max() <= 5):\n",
    "        raise RuntimeError(\n",
    "            f\"T10YIE out of expected range [0, 5]: \"\n",
    "            f\"{df['T10YIE'].min():.3f} to {df['T10YIE'].max():.3f}\"\n",
    "        )\n",
    "\n",
    "    # Check for extreme outliers in ie_change (typical daily change is 0.01-0.05)\n",
    "    extreme_changes = df['ie_change'].abs() > 0.5\n",
    "    if extreme_changes.any():\n",
    "        print(f\"Warning: {extreme_changes.sum()} extreme ie_change values (|value| > 0.5)\")\n",
    "\n",
    "    # Check ie_vol_5d for excessive zeros (some zeros are OK if IE is stable)\n",
    "    zero_vol_pct = (df['ie_vol_5d'] == 0).sum() / len(df)\n",
    "    if zero_vol_pct > 0.10:  # More than 10% zero volatility is suspicious\n",
    "        print(f\"Warning: {zero_vol_pct*100:.1f}% of ie_vol_5d values are zero\")\n",
    "\n",
    "    # Check for negative volatility (should never happen)\n",
    "    if (df['ie_vol_5d'] < 0).any():\n",
    "        raise RuntimeError(\"Invalid data: ie_vol_5d contains negative values\")\n",
    "\n",
    "    # Check for excessive gaps (no more than 5% of data should be forward-filled)\n",
    "    date_diffs = df.index.to_series().diff().dt.days\n",
    "    large_gaps = (date_diffs > 5).sum()\n",
    "    if large_gaps > len(df) * 0.05:\n",
    "        print(f\"Warning: {large_gaps} date gaps > 5 days detected\")\n",
    "\n",
    "    # === Split into train/val/test (70/15/15, time-series order) ===\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.70)\n",
    "    val_end = int(n * 0.85)\n",
    "\n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"\\nData fetched successfully:\")\n",
    "    print(f\"  Total rows: {len(df)}\")\n",
    "    print(f\"  Date range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "    print(f\"  Train: {len(train_df)} rows\")\n",
    "    print(f\"  Val: {len(val_df)} rows\")\n",
    "    print(f\"  Test: {len(test_df)} rows\")\n",
    "    print(f\"\\nT10YIE statistics:\")\n",
    "    print(f\"  Range: {df['T10YIE'].min():.3f}% to {df['T10YIE'].max():.3f}%\")\n",
    "    print(f\"  Mean: {df['T10YIE'].mean():.3f}%\")\n",
    "    print(f\"  Std: {df['T10YIE'].std():.3f}%\")\n",
    "    print(f\"\\nie_change statistics:\")\n",
    "    print(f\"  Mean: {df['ie_change'].mean():.6f}\")\n",
    "    print(f\"  Std: {df['ie_change'].std():.6f}\")\n",
    "    print(f\"  Range: {df['ie_change'].min():.6f} to {df['ie_change'].max():.6f}\")\n",
    "    print(f\"\\nie_vol_5d statistics:\")\n",
    "    print(f\"  Mean: {df['ie_vol_5d'].mean():.6f}\")\n",
    "    print(f\"  Std: {df['ie_vol_5d'].std():.6f}\")\n",
    "    print(f\"  Range: {df['ie_vol_5d'].min():.6f} to {df['ie_vol_5d'].max():.6f}\")\n",
    "\n",
    "    return train_df, val_df, test_df, df\n",
    "\n",
    "\n",
    "# === 2. HMM Component ===\n",
    "def train_hmm_regime(train_df, n_components, covariance_type, n_init):\n",
    "    \"\"\"Train 2D HMM on [ie_change, ie_vol_5d] to detect IE regimes.\n",
    "\n",
    "    Returns: fitted HMM model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from hmmlearn import hmm as hmmlearn_hmm\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"hmmlearn\"], check=True)\n",
    "        from hmmlearn import hmm as hmmlearn_hmm\n",
    "\n",
    "    # Prepare 2D input [ie_change, ie_vol_5d]\n",
    "    X_train = train_df[['ie_change', 'ie_vol_5d']].values\n",
    "\n",
    "    # Remove any remaining NaN or inf\n",
    "    valid_mask = np.isfinite(X_train).all(axis=1)\n",
    "    X_train_clean = X_train[valid_mask]\n",
    "\n",
    "    if len(X_train_clean) < 100:\n",
    "        raise RuntimeError(f\"Insufficient valid data for HMM training: {len(X_train_clean)} rows\")\n",
    "\n",
    "    # Train GaussianHMM with multiple restarts\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for restart in range(n_init):\n",
    "        model = hmmlearn_hmm.GaussianHMM(\n",
    "            n_components=n_components,\n",
    "            covariance_type=covariance_type,\n",
    "            n_iter=100,\n",
    "            tol=1e-4,\n",
    "            random_state=42 + restart,\n",
    "            init_params='stmc'\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            model.fit(X_train_clean)\n",
    "            score = model.score(X_train_clean)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"HMM restart {restart} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    if best_model is None:\n",
    "        raise RuntimeError(\"All HMM training attempts failed\")\n",
    "\n",
    "    print(f\"HMM training complete: {n_components} components, covariance={covariance_type}, best log-likelihood={best_score:.2f}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def compute_regime_probabilities(hmm_model, df):\n",
    "    \"\"\"Compute regime probabilities for all data.\n",
    "\n",
    "    Returns: ndarray with regime probabilities\n",
    "    \"\"\"\n",
    "    X = df[['ie_change', 'ie_vol_5d']].values\n",
    "\n",
    "    # Handle NaN/inf\n",
    "    valid_mask = np.isfinite(X).all(axis=1)\n",
    "\n",
    "    # Initialize output\n",
    "    probs = np.full((len(df), hmm_model.n_components), np.nan)\n",
    "\n",
    "    if valid_mask.sum() > 0:\n",
    "        probs[valid_mask] = hmm_model.predict_proba(X[valid_mask])\n",
    "\n",
    "    # Forward-fill NaN values\n",
    "    probs_df = pd.DataFrame(probs, index=df.index)\n",
    "    probs_df = probs_df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    return probs_df.values\n",
    "\n",
    "\n",
    "def identify_high_variance_state(hmm_model):\n",
    "    \"\"\"Identify which HMM state has highest variance in IE_change dimension.\n",
    "\n",
    "    Returns: state index\n",
    "    \"\"\"\n",
    "    state_vars = []\n",
    "\n",
    "    for i in range(hmm_model.n_components):\n",
    "        if hmm_model.covariance_type == 'full':\n",
    "            # Extract variance of first dimension (ie_change) from full covariance matrix\n",
    "            state_vars.append(float(hmm_model.covars_[i][0, 0]))\n",
    "        elif hmm_model.covariance_type == 'diag':\n",
    "            # Extract first diagonal element\n",
    "            state_vars.append(float(hmm_model.covars_[i][0]))\n",
    "\n",
    "    # High-variance state = state with highest variance in IE_change dimension\n",
    "    high_var_state = np.argmax(state_vars)\n",
    "\n",
    "    print(f\"State variances (ie_change dimension): {state_vars}\")\n",
    "    print(f\"High-variance state identified: {high_var_state} (variance: {state_vars[high_var_state]:.6f})\")\n",
    "\n",
    "    return high_var_state\n",
    "\n",
    "\n",
    "# === 3. Deterministic Components ===\n",
    "def compute_anchoring_z(df, vol_window, baseline_window):\n",
    "    \"\"\"Compute IE anchoring z-score: (vol_short - rolling_mean) / rolling_std\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with ie_change column\n",
    "        vol_window: Window for short-term volatility (5, 10, or 20 days)\n",
    "        baseline_window: Window for baseline statistics (60 or 120 days)\n",
    "\n",
    "    Returns: Series of z-scores\n",
    "    \"\"\"\n",
    "    # Select appropriate vol column based on window\n",
    "    vol_col = f'ie_vol_{vol_window}d'\n",
    "\n",
    "    if vol_col not in df.columns:\n",
    "        # Compute on the fly if not pre-computed\n",
    "        vol_short = df['ie_change'].rolling(vol_window).std()\n",
    "    else:\n",
    "        vol_short = df[vol_col]\n",
    "\n",
    "    # Rolling mean and std of volatility\n",
    "    rolling_mean = vol_short.rolling(baseline_window).mean()\n",
    "    rolling_std = vol_short.rolling(baseline_window).std()\n",
    "\n",
    "    # Z-score\n",
    "    z_score = (vol_short - rolling_mean) / rolling_std\n",
    "\n",
    "    # Clip to [-4, 4] for stability\n",
    "    z_score = z_score.clip(-4, 4)\n",
    "\n",
    "    # Replace inf/nan with 0\n",
    "    z_score = z_score.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    return z_score\n",
    "\n",
    "\n",
    "def compute_sensitivity_z(df, corr_window=5, baseline_window=60):\n",
    "    \"\"\"Compute IE-gold sensitivity z-score: 5d rolling correlation z-scored.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with ie_change and gold_return columns\n",
    "        corr_window: Window for rolling correlation (fixed at 5)\n",
    "        baseline_window: Window for baseline statistics (40, 60, or 90 days)\n",
    "\n",
    "    Returns: Series of z-scores\n",
    "    \"\"\"\n",
    "    # Rolling 5-day correlation between IE changes and gold returns\n",
    "    rolling_corr = df['ie_change'].rolling(corr_window).corr(df['gold_return'])\n",
    "\n",
    "    # Rolling baseline mean and std\n",
    "    corr_mean = rolling_corr.rolling(baseline_window).mean()\n",
    "    corr_std = rolling_corr.rolling(baseline_window).std()\n",
    "\n",
    "    # Z-score\n",
    "    z_score = (rolling_corr - corr_mean) / corr_std\n",
    "\n",
    "    # Clip to [-4, 4] for stability\n",
    "    z_score = z_score.clip(-4, 4)\n",
    "\n",
    "    # Replace inf/nan with 0\n",
    "    z_score = z_score.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    return z_score\n",
    "\n",
    "\n",
    "# === 4. Hyperparameter Optimization ===\n",
    "def run_hpo(train_df, val_df, n_trials=30, timeout=300):\n",
    "    \"\"\"Run Optuna HPO for all hyperparameters.\n",
    "\n",
    "    Returns: best_params dict, best_value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import optuna\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"optuna\"], check=True)\n",
    "        import optuna\n",
    "\n",
    "    try:\n",
    "        from sklearn.feature_selection import mutual_info_regression\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"scikit-learn\"], check=True)\n",
    "        from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "    # Prepare validation target (next-day gold return)\n",
    "    val_target = val_df['gold_return'].shift(-1).dropna()\n",
    "\n",
    "    def objective(trial):\n",
    "        # Sample hyperparameters\n",
    "        n_components = trial.suggest_categorical('hmm_n_components', [2, 3])\n",
    "        covariance_type = trial.suggest_categorical('hmm_covariance_type', ['full', 'diag'])\n",
    "        n_init = trial.suggest_categorical('hmm_n_init', [3, 5, 10])\n",
    "        anchoring_vol_window = trial.suggest_categorical('anchoring_vol_window', [5, 10, 20])\n",
    "        anchoring_baseline_window = trial.suggest_categorical('anchoring_baseline_window', [60, 120])\n",
    "        sensitivity_baseline_window = trial.suggest_categorical('sensitivity_baseline_window', [40, 60, 90])\n",
    "\n",
    "        try:\n",
    "            # Train HMM\n",
    "            hmm_model = train_hmm_regime(train_df, n_components, covariance_type, n_init)\n",
    "            high_var_state = identify_high_variance_state(hmm_model)\n",
    "\n",
    "            # Generate features for validation set\n",
    "            regime_probs = compute_regime_probabilities(hmm_model, val_df)\n",
    "            regime_prob = regime_probs[:, high_var_state]\n",
    "\n",
    "            anchoring_z = compute_anchoring_z(val_df, anchoring_vol_window, anchoring_baseline_window).values\n",
    "            sensitivity_z = compute_sensitivity_z(val_df, 5, sensitivity_baseline_window).values\n",
    "\n",
    "            # Stack features\n",
    "            X_val = np.column_stack([regime_prob, anchoring_z, sensitivity_z])\n",
    "\n",
    "            # Align with target (drop last row since target is shifted)\n",
    "            X_val_aligned = X_val[:-1]\n",
    "\n",
    "            if len(X_val_aligned) != len(val_target):\n",
    "                return -np.inf\n",
    "\n",
    "            # Check for NaN/constant features\n",
    "            if np.any(np.isnan(X_val_aligned)) or np.any(np.std(X_val_aligned, axis=0) < 1e-6):\n",
    "                return -np.inf\n",
    "\n",
    "            # Compute mutual information for each feature\n",
    "            mi_scores = mutual_info_regression(X_val_aligned, val_target, random_state=42)\n",
    "            mi_total = mi_scores.sum()\n",
    "\n",
    "            if np.isnan(mi_total):\n",
    "                return -np.inf\n",
    "\n",
    "            return mi_total\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return -np.inf\n",
    "\n",
    "    # Run optimization\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n",
    "\n",
    "    print(f\"\\nOptuna optimization complete:\")\n",
    "    print(f\"  Best MI sum: {study.best_value:.6f}\")\n",
    "    print(f\"  Best params: {study.best_params}\")\n",
    "    print(f\"  Trials completed: {len(study.trials)}\")\n",
    "\n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "\n",
    "# === 5. Feature Generation ===\n",
    "def generate_features(train_df, val_df, test_df, full_df, params):\n",
    "    \"\"\"Generate final 3-feature output using best hyperparameters.\n",
    "\n",
    "    Returns: DataFrame with 3 columns\n",
    "    \"\"\"\n",
    "    # Train HMM on full training data\n",
    "    hmm_model = train_hmm_regime(\n",
    "        train_df,\n",
    "        params['hmm_n_components'],\n",
    "        params['hmm_covariance_type'],\n",
    "        params['hmm_n_init']\n",
    "    )\n",
    "    high_var_state = identify_high_variance_state(hmm_model)\n",
    "\n",
    "    # Generate regime probabilities for all data\n",
    "    regime_probs = compute_regime_probabilities(hmm_model, full_df)\n",
    "    regime_prob = regime_probs[:, high_var_state]\n",
    "\n",
    "    # Generate deterministic features\n",
    "    anchoring_z = compute_anchoring_z(\n",
    "        full_df,\n",
    "        params['anchoring_vol_window'],\n",
    "        params['anchoring_baseline_window']\n",
    "    ).values\n",
    "\n",
    "    sensitivity_z = compute_sensitivity_z(\n",
    "        full_df,\n",
    "        5,  # corr_window is fixed at 5\n",
    "        params['sensitivity_baseline_window']\n",
    "    ).values\n",
    "\n",
    "    # Create output DataFrame\n",
    "    output = pd.DataFrame({\n",
    "        'ie_regime_prob': regime_prob,\n",
    "        'ie_anchoring_z': anchoring_z,\n",
    "        'ie_gold_sensitivity_z': sensitivity_z\n",
    "    }, index=full_df.index)\n",
    "\n",
    "    print(f\"\\nFeature generation complete:\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Columns: {list(output.columns)}\")\n",
    "    print(f\"  Date range: {output.index.min().date()} to {output.index.max().date()}\")\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    for col in output.columns:\n",
    "        print(f\"  {col}:\")\n",
    "        print(f\"    Mean: {output[col].mean():.4f}, Std: {output[col].std():.4f}\")\n",
    "        print(f\"    Range: [{output[col].min():.4f}, {output[col].max():.4f}]\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# === 6. Evaluation Metrics ===\n",
    "def compute_metrics(train_df, val_df, test_df, full_df, output, params):\n",
    "    \"\"\"Compute evaluation metrics for Gate 1 & 2.\n",
    "\n",
    "    Returns: metrics dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.feature_selection import mutual_info_regression\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.run([\"pip\", \"install\", \"scikit-learn\"], check=True)\n",
    "        from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "    # Prepare target (next-day gold return)\n",
    "    target = full_df['gold_return'].shift(-1).dropna()\n",
    "    X_aligned = output.iloc[:-1].values  # Drop last row to align with shifted target\n",
    "\n",
    "    # 1. Mutual Information (individual and sum)\n",
    "    mi_scores = mutual_info_regression(X_aligned, target, random_state=42)\n",
    "    mi_individual = dict(zip(output.columns, mi_scores))\n",
    "    mi_sum = mi_scores.sum()\n",
    "\n",
    "    # 2. Autocorrelation (for Gate 1)\n",
    "    autocorr = {}\n",
    "    for col in output.columns:\n",
    "        series = output[col].dropna()\n",
    "        if len(series) > 1:\n",
    "            autocorr[col] = series.autocorr(lag=1)\n",
    "        else:\n",
    "            autocorr[col] = np.nan\n",
    "\n",
    "    # 3. Check for constant/NaN features\n",
    "    has_nan = output.isnull().any().to_dict()\n",
    "    is_constant = (output.std() < 1e-6).to_dict()\n",
    "\n",
    "    metrics = {\n",
    "        'mi_individual': mi_individual,\n",
    "        'mi_sum': mi_sum,\n",
    "        'autocorr': autocorr,\n",
    "        'has_nan': has_nan,\n",
    "        'is_constant': is_constant,\n",
    "        'output_stats': {\n",
    "            'mean': output.mean().to_dict(),\n",
    "            'std': output.std().to_dict(),\n",
    "            'min': output.min().to_dict(),\n",
    "            'max': output.max().to_dict()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"\\nMetrics computed:\")\n",
    "    print(f\"  MI individual: {mi_individual}\")\n",
    "    print(f\"  MI sum: {mi_sum:.6f}\")\n",
    "    print(f\"  Autocorrelation: {autocorr}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# === 7. Main Execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Inflation Expectation SubModel Training - Attempt 1\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Fetch data\n",
    "    print(\"\\n[1/5] Fetching and preprocessing data...\")\n",
    "    train_df, val_df, test_df, full_df = fetch_and_preprocess()\n",
    "\n",
    "    # Step 2: Hyperparameter optimization\n",
    "    print(\"\\n[2/5] Running Optuna hyperparameter optimization...\")\n",
    "    best_params, best_mi = run_hpo(train_df, val_df, n_trials=30, timeout=300)\n",
    "\n",
    "    # Step 3: Generate features with best params\n",
    "    print(\"\\n[3/5] Generating features with best hyperparameters...\")\n",
    "    output = generate_features(train_df, val_df, test_df, full_df, best_params)\n",
    "\n",
    "    # Step 4: Compute metrics\n",
    "    print(\"\\n[4/5] Computing evaluation metrics...\")\n",
    "    metrics = compute_metrics(train_df, val_df, test_df, full_df, output, best_params)\n",
    "\n",
    "    # Step 5: Save results\n",
    "    print(\"\\n[5/5] Saving results...\")\n",
    "\n",
    "    # Save output CSV\n",
    "    output.to_csv('submodel_output.csv')\n",
    "    print(f\"  Saved: submodel_output.csv ({output.shape[0]} rows, {output.shape[1]} columns)\")\n",
    "\n",
    "    # Save training result JSON\n",
    "    result = {\n",
    "        'feature': 'inflation_expectation',\n",
    "        'attempt': 1,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'best_params': best_params,\n",
    "        'metrics': {\n",
    "            'mi_individual': metrics['mi_individual'],\n",
    "            'mi_sum': metrics['mi_sum'],\n",
    "            'autocorr': metrics['autocorr'],\n",
    "            'has_nan': metrics['has_nan'],\n",
    "            'is_constant': metrics['is_constant']\n",
    "        },\n",
    "        'optuna_best_value': best_mi,\n",
    "        'output_shape': list(output.shape),\n",
    "        'output_columns': list(output.columns),\n",
    "        'data_info': {\n",
    "            'train_samples': len(train_df),\n",
    "            'val_samples': len(val_df),\n",
    "            'test_samples': len(test_df),\n",
    "            'full_samples': len(full_df)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('training_result.json', 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    print(f\"  Saved: training_result.json\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Training complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nFinal MI sum: {metrics['mi_sum']:.6f}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Columns: {list(output.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
