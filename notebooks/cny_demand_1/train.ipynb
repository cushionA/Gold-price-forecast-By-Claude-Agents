{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Prediction SubModel Training - CNY Demand Proxy Attempt 1\n",
    "\n",
    "**Self-contained**: Data fetch → Preprocessing → HMM + Z-Scores → Optuna HPO → Save results\n",
    "\n",
    "**Architecture**: 2D HMM [CNY_return, CNY_vol] + momentum z-score + volatility regime z-score\n",
    "\n",
    "**Feature**: cny_demand | **Attempt**: 1\n",
    "\n",
    "Generated by builder_model agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install hmmlearn (not pre-installed on Kaggle)\n",
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', '-q', 'hmmlearn'])\n",
    "\n",
    "print(\"Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import optuna\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Training started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Fetching and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_preprocess():\n",
    "    \"\"\"\n",
    "    Fetch and preprocess CNY demand proxy data.\n",
    "    Self-contained: No external file dependencies.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df, full_df)\n",
    "               Each df contains: cny_return, cny_vol_5d, gold_return\n",
    "    \"\"\"\n",
    "    # === 1. Fetch CNY=X from Yahoo Finance ===\n",
    "    # Start date: 2014-06-01 (buffer for 120-day warmup before 2015-01-30)\n",
    "    print(\"Fetching CNY=X (CNY/USD exchange rate)...\")\n",
    "    cny_data = yf.download('CNY=X', start='2014-06-01', progress=False)\n",
    "    \n",
    "    if cny_data.empty:\n",
    "        raise RuntimeError(\"Failed to fetch CNY=X data from Yahoo Finance\")\n",
    "    \n",
    "    # Extract Close price\n",
    "    if isinstance(cny_data.columns, pd.MultiIndex):\n",
    "        cny_close = cny_data['Close']['CNY=X'].copy()\n",
    "    else:\n",
    "        cny_close = cny_data['Close'].copy()\n",
    "    \n",
    "    cny_close = cny_close.dropna()\n",
    "    print(f\"CNY=X data fetched: {len(cny_close)} rows from {cny_close.index[0]} to {cny_close.index[-1]}\")\n",
    "    \n",
    "    # Validate CNY/USD range (reasonable onshore rate)\n",
    "    if cny_close.min() < 5.5 or cny_close.max() > 8.0:\n",
    "        print(f\"WARNING: CNY/USD range [{cny_close.min():.2f}, {cny_close.max():.2f}] outside expected [5.5, 8.0]\")\n",
    "    \n",
    "    # === 2. Fetch GC=F for gold returns ===\n",
    "    print(\"Fetching GC=F (Gold Futures) for return computation...\")\n",
    "    gc_data = yf.download('GC=F', start='2014-06-01', progress=False)\n",
    "    \n",
    "    if gc_data.empty:\n",
    "        raise RuntimeError(\"Failed to fetch GC=F data from Yahoo Finance\")\n",
    "    \n",
    "    # Extract Close price\n",
    "    if isinstance(gc_data.columns, pd.MultiIndex):\n",
    "        gc_close = gc_data['Close']['GC=F'].copy()\n",
    "    else:\n",
    "        gc_close = gc_data['Close'].copy()\n",
    "    \n",
    "    gc_close = gc_close.dropna()\n",
    "    print(f\"GC=F data fetched: {len(gc_close)} rows from {gc_close.index[0]} to {gc_close.index[-1]}\")\n",
    "    \n",
    "    # === 3. Compute derived quantities ===\n",
    "    # CNY daily return\n",
    "    cny_return = cny_close.pct_change()\n",
    "    \n",
    "    # CNY 5-day rolling volatility (for initial HMM input)\n",
    "    cny_vol_5d = cny_return.rolling(5).std()\n",
    "    \n",
    "    # Gold current-day return (for MI evaluation, not model input)\n",
    "    gold_return = gc_close.pct_change()\n",
    "    \n",
    "    # === 4. Align dates (inner join on trading dates) ===\n",
    "    df = pd.DataFrame({\n",
    "        'cny_close': cny_close,\n",
    "        'cny_return': cny_return,\n",
    "        'cny_vol_5d': cny_vol_5d,\n",
    "        'gold_return': gold_return\n",
    "    })\n",
    "    \n",
    "    # Drop NaN rows from returns/volatility computation\n",
    "    initial_rows = len(df)\n",
    "    df = df.dropna()\n",
    "    print(f\"After alignment and NaN removal: {len(df)} rows (dropped {initial_rows - len(df)} rows)\")\n",
    "    \n",
    "    # === 5. Validate data quality ===\n",
    "    # Check for extreme CNY returns (managed float should not have |return| > 0.05)\n",
    "    extreme_returns = (df['cny_return'].abs() > 0.05).sum()\n",
    "    if extreme_returns > 0:\n",
    "        print(f\"WARNING: {extreme_returns} CNY returns exceed 5% (max: {df['cny_return'].abs().max():.4f})\")\n",
    "    \n",
    "    # Check for constant volatility (should be varying)\n",
    "    if df['cny_vol_5d'].std() < 1e-6:\n",
    "        raise RuntimeError(\"CNY volatility is constant (std < 1e-6)\")\n",
    "    \n",
    "    # Check for zero/negative volatility (can occur in managed float periods)\n",
    "    zero_vol_count = (df['cny_vol_5d'] == 0).sum()\n",
    "    if zero_vol_count > 0:\n",
    "        print(f\"INFO: {zero_vol_count} periods with zero volatility (PBOC fixed rate)\")\n",
    "    \n",
    "    if (df['cny_vol_5d'] < 0).any():\n",
    "        raise RuntimeError(\"CNY volatility contains negative values (implementation error)\")\n",
    "    \n",
    "    # === 6. Trim to base_features date range ===\n",
    "    # Expected: 2015-01-30 to latest\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    base_start = pd.Timestamp('2015-01-30')\n",
    "    \n",
    "    if df.index[0] > base_start:\n",
    "        print(f\"WARNING: CNY data starts at {df.index[0]}, later than expected {base_start}\")\n",
    "    \n",
    "    df = df[df.index >= base_start]\n",
    "    print(f\"After trimming to base_features range (>= {base_start}): {len(df)} rows\")\n",
    "    \n",
    "    # === 7. Data split (70/15/15, time-series order) ===\n",
    "    n = len(df)\n",
    "    train_end = int(n * 0.70)\n",
    "    val_end = int(n * 0.85)\n",
    "    \n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Train: {len(train_df)} rows ({train_df.index[0]} to {train_df.index[-1]})\")\n",
    "    print(f\"  Val:   {len(val_df)} rows ({val_df.index[0]} to {val_df.index[-1]})\")\n",
    "    print(f\"  Test:  {len(test_df)} rows ({test_df.index[0]} to {test_df.index[-1]})\")\n",
    "    \n",
    "    # === 8. Summary statistics ===\n",
    "    print(f\"\\nSummary statistics (full dataset):\")\n",
    "    print(f\"  CNY/USD range: [{df['cny_close'].min():.4f}, {df['cny_close'].max():.4f}]\")\n",
    "    print(f\"  CNY return mean: {df['cny_return'].mean():.6f}, std: {df['cny_return'].std():.6f}\")\n",
    "    print(f\"  CNY return range: [{df['cny_return'].min():.6f}, {df['cny_return'].max():.6f}]\")\n",
    "    print(f\"  CNY vol_5d mean: {df['cny_vol_5d'].mean():.6f}, std: {df['cny_vol_5d'].std():.6f}\")\n",
    "    print(f\"  Gold return mean: {df['gold_return'].mean():.6f}, std: {df['gold_return'].std():.6f}\")\n",
    "    \n",
    "    # Autocorrelation check (lag 1)\n",
    "    cny_return_autocorr = df['cny_return'].autocorr(lag=1)\n",
    "    print(f\"  CNY return autocorr(lag=1): {cny_return_autocorr:.4f}\")\n",
    "    \n",
    "    return train_df, val_df, test_df, df\n",
    "\n",
    "\n",
    "# Fetch data\n",
    "train_df, val_df, test_df, full_df = fetch_and_preprocess()\n",
    "print(\"\\nData fetching complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regime_feature(cny_return, cny_vol, n_components, covariance_type, train_size):\n",
    "    \"\"\"\n",
    "    Fit 2D HMM on [CNY_return, CNY_vol] and return P(highest-return-variance state).\n",
    "    Best-of-5 random restarts.\n",
    "    \n",
    "    Args:\n",
    "        cny_return: Array of CNY daily returns\n",
    "        cny_vol: Array of CNY volatility (rolling std)\n",
    "        n_components: Number of HMM states (2 or 3)\n",
    "        covariance_type: 'full' or 'diag'\n",
    "        train_size: Number of training samples\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (regime_prob array, fitted HMM model)\n",
    "    \"\"\"\n",
    "    X = np.column_stack([cny_return, cny_vol])\n",
    "    valid_mask = ~np.isnan(X).any(axis=1)\n",
    "    X_valid = X[valid_mask]\n",
    "    X_train = X_valid[:train_size]\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    # Best-of-5 restarts (hmmlearn lacks n_init parameter)\n",
    "    for seed in [42, 123, 456, 789, 0]:\n",
    "        model = GaussianHMM(\n",
    "            n_components=n_components,\n",
    "            covariance_type=covariance_type,\n",
    "            n_iter=100,\n",
    "            tol=1e-4,\n",
    "            random_state=seed\n",
    "        )\n",
    "        try:\n",
    "            model.fit(X_train)\n",
    "            score = model.score(X_train)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        except Exception as e:\n",
    "            print(f\"  HMM seed {seed} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if best_model is None:\n",
    "        print(\"WARNING: All HMM seeds failed. Returning NaN.\")\n",
    "        return np.full(len(cny_return), np.nan), None\n",
    "    \n",
    "    # Generate probabilities for valid data\n",
    "    probs = best_model.predict_proba(X_valid)\n",
    "    \n",
    "    # Identify highest-return-variance state (first dimension = CNY_return)\n",
    "    state_vars = []\n",
    "    for i in range(n_components):\n",
    "        if covariance_type == 'full':\n",
    "            state_vars.append(float(best_model.covars_[i][0, 0]))\n",
    "        elif covariance_type == 'diag':\n",
    "            state_vars.append(float(best_model.covars_[i][0]))\n",
    "    \n",
    "    high_var_state = np.argmax(state_vars)\n",
    "    \n",
    "    # Map back to full array\n",
    "    result = np.full(len(cny_return), np.nan)\n",
    "    result[valid_mask] = probs[:, high_var_state]\n",
    "    \n",
    "    return result, best_model\n",
    "\n",
    "\n",
    "def generate_momentum_feature(cny_return, baseline_window):\n",
    "    \"\"\"\n",
    "    5d momentum z-scored against baseline_window-day baseline.\n",
    "    \n",
    "    Args:\n",
    "        cny_return: Array of CNY daily returns\n",
    "        baseline_window: Baseline window for z-score (60 or 120)\n",
    "    \n",
    "    Returns:\n",
    "        Array of momentum z-scores (clipped to [-4, 4])\n",
    "    \"\"\"\n",
    "    s = pd.Series(cny_return)\n",
    "    momentum = s.rolling(5).mean()\n",
    "    rolling_mean = momentum.rolling(baseline_window).mean()\n",
    "    rolling_std = momentum.rolling(baseline_window).std()\n",
    "    z = (momentum - rolling_mean) / rolling_std\n",
    "    z = z.clip(-4, 4)\n",
    "    return z.values\n",
    "\n",
    "\n",
    "def generate_vol_regime_feature(cny_return, vol_window, baseline_window):\n",
    "    \"\"\"\n",
    "    Rolling z-score of vol_window-day volatility against baseline_window-day baseline.\n",
    "    \n",
    "    Args:\n",
    "        cny_return: Array of CNY daily returns\n",
    "        vol_window: Short volatility window (5, 10, or 20)\n",
    "        baseline_window: Baseline window for z-score (60 or 120)\n",
    "    \n",
    "    Returns:\n",
    "        Array of volatility regime z-scores (clipped to [-4, 4])\n",
    "    \"\"\"\n",
    "    s = pd.Series(cny_return)\n",
    "    vol_short = s.rolling(vol_window).std()\n",
    "    rolling_mean = vol_short.rolling(baseline_window).mean()\n",
    "    rolling_std = vol_short.rolling(baseline_window).std()\n",
    "    z = (vol_short - rolling_mean) / rolling_std\n",
    "    z = z.clip(-4, 4)\n",
    "    return z.values\n",
    "\n",
    "\n",
    "print(\"Feature generation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, cny_return_full, target_full, train_size, val_start, val_end):\n",
    "    \"\"\"\n",
    "    Optuna objective: Maximize MI sum on validation set.\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object\n",
    "        cny_return_full: Full CNY return array\n",
    "        target_full: Full gold return array (for MI computation)\n",
    "        train_size: Number of training samples\n",
    "        val_start: Validation start index\n",
    "        val_end: Validation end index\n",
    "    \n",
    "    Returns:\n",
    "        MI sum on validation set\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    n_components = trial.suggest_categorical('hmm_n_components', [2, 3])\n",
    "    covariance_type = trial.suggest_categorical('hmm_covariance_type', ['full', 'diag'])\n",
    "    vol_window = trial.suggest_categorical('vol_short_window', [5, 10, 20])\n",
    "    vol_baseline = trial.suggest_categorical('vol_baseline_window', [60, 120])\n",
    "    mom_baseline = trial.suggest_categorical('momentum_baseline_window', [60, 120])\n",
    "    \n",
    "    try:\n",
    "        # Recompute HMM input vol with trial's vol_window\n",
    "        cny_vol = pd.Series(cny_return_full).rolling(vol_window).std().values\n",
    "        \n",
    "        # Generate features\n",
    "        regime, _ = generate_regime_feature(\n",
    "            cny_return_full, cny_vol, n_components, covariance_type, train_size\n",
    "        )\n",
    "        momentum = generate_momentum_feature(cny_return_full, mom_baseline)\n",
    "        vol_regime = generate_vol_regime_feature(cny_return_full, vol_window, vol_baseline)\n",
    "        \n",
    "        # Extract validation period\n",
    "        regime_val = regime[val_start:val_end]\n",
    "        momentum_val = momentum[val_start:val_end]\n",
    "        vol_regime_val = vol_regime[val_start:val_end]\n",
    "        target_val = target_full[val_start:val_end]\n",
    "        \n",
    "        # Discretize for MI computation\n",
    "        def discretize(x, bins=20):\n",
    "            valid = ~np.isnan(x)\n",
    "            if valid.sum() < bins:\n",
    "                return None\n",
    "            x_valid = x.copy()\n",
    "            x_valid[~valid] = np.nanmedian(x)\n",
    "            try:\n",
    "                return pd.qcut(x_valid, bins, labels=False, duplicates='drop')\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        # Compute MI sum\n",
    "        mi_sum = 0.0\n",
    "        for feat_val in [regime_val, momentum_val, vol_regime_val]:\n",
    "            mask = ~np.isnan(feat_val) & ~np.isnan(target_val)\n",
    "            if mask.sum() > 50:\n",
    "                feat_disc = discretize(feat_val[mask])\n",
    "                tgt_disc = discretize(target_val[mask])\n",
    "                if feat_disc is not None and tgt_disc is not None:\n",
    "                    mi = mutual_info_score(feat_disc, tgt_disc)\n",
    "                    mi_sum += mi\n",
    "        \n",
    "        return mi_sum\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Trial failed: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "print(\"Optuna objective function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Optuna\n",
    "cny_return_full = full_df['cny_return'].values\n",
    "gold_return_full = full_df['gold_return'].values\n",
    "\n",
    "n_total = len(full_df)\n",
    "train_size = int(n_total * 0.70)\n",
    "val_start = train_size\n",
    "val_end = int(n_total * 0.85)\n",
    "\n",
    "print(f\"\\nStarting Optuna HPO...\")\n",
    "print(f\"Total samples: {n_total}\")\n",
    "print(f\"Train size: {train_size}\")\n",
    "print(f\"Val range: [{val_start}, {val_end})\")\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, cny_return_full, gold_return_full, train_size, val_start, val_end),\n",
    "    n_trials=30,\n",
    "    timeout=300,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "n_completed = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "\n",
    "print(f\"\\nOptuna HPO complete!\")\n",
    "print(f\"Completed trials: {n_completed}\")\n",
    "print(f\"Best MI sum: {best_value:.6f}\")\n",
    "print(f\"Best parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model Training with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating final features with best parameters...\")\n",
    "\n",
    "# Extract best parameters\n",
    "n_components = best_params['hmm_n_components']\n",
    "covariance_type = best_params['hmm_covariance_type']\n",
    "vol_window = best_params['vol_short_window']\n",
    "vol_baseline = best_params['vol_baseline_window']\n",
    "mom_baseline = best_params['momentum_baseline_window']\n",
    "\n",
    "# Recompute CNY volatility with best vol_window\n",
    "cny_vol_full = pd.Series(cny_return_full).rolling(vol_window).std().values\n",
    "\n",
    "# Generate regime feature\n",
    "print(f\"Fitting HMM ({n_components} states, {covariance_type} covariance)...\")\n",
    "regime_prob, best_hmm = generate_regime_feature(\n",
    "    cny_return_full, cny_vol_full, n_components, covariance_type, train_size\n",
    ")\n",
    "\n",
    "# Generate momentum z-score\n",
    "print(f\"Computing momentum z-score (5d momentum, {mom_baseline}d baseline)...\")\n",
    "momentum_z = generate_momentum_feature(cny_return_full, mom_baseline)\n",
    "\n",
    "# Generate volatility regime z-score\n",
    "print(f\"Computing volatility regime z-score ({vol_window}d vol, {vol_baseline}d baseline)...\")\n",
    "vol_regime_z = generate_vol_regime_feature(cny_return_full, vol_window, vol_baseline)\n",
    "\n",
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'cny_regime_prob': regime_prob,\n",
    "    'cny_momentum_z': momentum_z,\n",
    "    'cny_vol_regime_z': vol_regime_z\n",
    "}, index=full_df.index)\n",
    "\n",
    "print(f\"\\nOutput shape: {output_df.shape}\")\n",
    "print(f\"Output columns: {list(output_df.columns)}\")\n",
    "print(f\"\\nOutput summary:\")\n",
    "print(output_df.describe())\n",
    "\n",
    "# Check for NaN and constant values\n",
    "nan_counts = output_df.isna().sum()\n",
    "print(f\"\\nNaN counts:\")\n",
    "print(nan_counts)\n",
    "\n",
    "stds = output_df.std()\n",
    "print(f\"\\nStandard deviations:\")\n",
    "print(stds)\n",
    "\n",
    "# Check for constant features\n",
    "for col in output_df.columns:\n",
    "    if stds[col] < 1e-6:\n",
    "        print(f\"WARNING: {col} is effectively constant (std={stds[col]:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute autocorrelations\n",
    "autocorrs = {}\n",
    "for col in output_df.columns:\n",
    "    valid_data = output_df[col].dropna()\n",
    "    if len(valid_data) > 10:\n",
    "        autocorr = valid_data.autocorr(lag=1)\n",
    "        autocorrs[col] = autocorr\n",
    "    else:\n",
    "        autocorrs[col] = np.nan\n",
    "\n",
    "print(f\"\\nAutocorrelations (lag 1):\")\n",
    "for col, ac in autocorrs.items():\n",
    "    print(f\"  {col}: {ac:.4f}\")\n",
    "    if ac > 0.99:\n",
    "        print(f\"    WARNING: Autocorrelation > 0.99 (likely leak)\")\n",
    "\n",
    "# Compute MI on validation set\n",
    "def compute_mi(feature, target, bins=20):\n",
    "    \"\"\"Compute mutual information between feature and target.\"\"\"\n",
    "    mask = ~np.isnan(feature) & ~np.isnan(target)\n",
    "    if mask.sum() < bins:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        feat_valid = feature[mask]\n",
    "        tgt_valid = target[mask]\n",
    "        feat_disc = pd.qcut(feat_valid, bins, labels=False, duplicates='drop')\n",
    "        tgt_disc = pd.qcut(tgt_valid, bins, labels=False, duplicates='drop')\n",
    "        return mutual_info_score(feat_disc, tgt_disc)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Extract validation data\n",
    "val_features = output_df.iloc[val_start:val_end]\n",
    "val_target = gold_return_full[val_start:val_end]\n",
    "\n",
    "mi_scores = {}\n",
    "for col in output_df.columns:\n",
    "    mi = compute_mi(val_features[col].values, val_target)\n",
    "    mi_scores[col] = mi\n",
    "\n",
    "mi_total = sum(mi_scores.values())\n",
    "\n",
    "print(f\"\\nMutual Information (validation set):\")\n",
    "for col, mi in mi_scores.items():\n",
    "    print(f\"  {col}: {mi:.6f}\")\n",
    "print(f\"  Total: {mi_total:.6f}\")\n",
    "\n",
    "# Create metrics dictionary\n",
    "metrics = {\n",
    "    'autocorrelations': autocorrs,\n",
    "    'mutual_information': mi_scores,\n",
    "    'mi_total': mi_total,\n",
    "    'output_std': stds.to_dict(),\n",
    "    'output_nan_pct': (nan_counts / len(output_df)).to_dict()\n",
    "}\n",
    "\n",
    "print(f\"\\nMetrics computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving results...\")\n",
    "\n",
    "# Save submodel output\n",
    "output_df.to_csv('submodel_output.csv')\n",
    "print(\"  Saved: submodel_output.csv\")\n",
    "\n",
    "# Save HMM model parameters (hmmlearn models are not directly serializable with torch)\n",
    "model_params = {\n",
    "    'n_components': n_components,\n",
    "    'covariance_type': covariance_type,\n",
    "    'means': best_hmm.means_.tolist() if best_hmm else None,\n",
    "    'covars': best_hmm.covars_.tolist() if best_hmm and hasattr(best_hmm, 'covars_') else None,\n",
    "    'transmat': best_hmm.transmat_.tolist() if best_hmm else None,\n",
    "    'startprob': best_hmm.startprob_.tolist() if best_hmm else None\n",
    "}\n",
    "\n",
    "with open('model_params.json', 'w') as f:\n",
    "    json.dump(model_params, f, indent=2)\n",
    "print(\"  Saved: model_params.json\")\n",
    "\n",
    "# Save training result\n",
    "result = {\n",
    "    'feature': 'cny_demand',\n",
    "    'attempt': 1,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_params': best_params,\n",
    "    'metrics': metrics,\n",
    "    'optuna_trials_completed': n_completed,\n",
    "    'optuna_best_value': best_value,\n",
    "    'output_shape': list(output_df.shape),\n",
    "    'output_columns': list(output_df.columns),\n",
    "    'data_info': {\n",
    "        'train_samples': len(train_df),\n",
    "        'val_samples': len(val_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'full_samples': len(full_df),\n",
    "        'date_range': {\n",
    "            'start': str(full_df.index[0]),\n",
    "            'end': str(full_df.index[-1])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2, default=str)\n",
    "print(\"  Saved: training_result.json\")\n",
    "\n",
    "print(f\"\\n=== Training Complete! ===\")\n",
    "print(f\"Finished: {datetime.now().isoformat()}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - submodel_output.csv ({output_df.shape[0]} rows x {output_df.shape[1]} columns)\")\n",
    "print(f\"  - model_params.json (HMM parameters)\")\n",
    "print(f\"  - training_result.json (full training report)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
