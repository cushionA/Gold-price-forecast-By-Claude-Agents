{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Meta-Model Training - Weekly (5-Day Return) Attempt 2\n",
    "\n",
    "**Architecture:** Single XGBoost with reg:squarederror (weekly target)\n",
    "\n",
    "**Key Changes from Attempt 1 (FAILED - trivial always-positive predictor):**\n",
    "1. **Non-overlapping training**: Every 5th row only (~425 train, ~91 val)\n",
    "2. **Centered targets**: Subtract training mean to remove positive bias\n",
    "3. **Naive-aware objective**: DA/HCDA measure skill above naive, not raw values\n",
    "4. **Trade-activity gate**: Sharpe component = 0 if < 3 position changes\n",
    "5. **Relaxed regularization**: max_depth [2,6], min_child_weight [3,20]\n",
    "6. **Constant-output penalty**: -1.0 if prediction std < 0.01\n",
    "\n",
    "**Unchanged from Attempt 1:**\n",
    "- Same 24 features (5 base + 19 submodel outputs)\n",
    "- Bootstrap variance-based confidence (5 models for HCDA)\n",
    "- OLS output scaling\n",
    "- Same metric functions and evaluation targets\n",
    "\n",
    "**Design:** `docs/design/meta_model_weekly_attempt_2.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"Started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = [\n",
    "    # Base features (5)\n",
    "    'real_rate_change',\n",
    "    'dxy_change',\n",
    "    'vix',\n",
    "    'yield_spread_change',\n",
    "    'inflation_exp_change',\n",
    "    # VIX submodel (3)\n",
    "    'vix_regime_probability',\n",
    "    'vix_mean_reversion_z',\n",
    "    'vix_persistence',\n",
    "    # Technical submodel (3)\n",
    "    'tech_trend_regime_prob',\n",
    "    'tech_mean_reversion_z',\n",
    "    'tech_volatility_regime',\n",
    "    # Cross-asset submodel (3)\n",
    "    'xasset_regime_prob',\n",
    "    'xasset_recession_signal',\n",
    "    'xasset_divergence',\n",
    "    # Yield curve submodel (2)\n",
    "    'yc_spread_velocity_z',\n",
    "    'yc_curvature_z',\n",
    "    # ETF flow submodel (3)\n",
    "    'etf_regime_prob',\n",
    "    'etf_capital_intensity',\n",
    "    'etf_pv_divergence',\n",
    "    # Inflation expectation submodel (3)\n",
    "    'ie_regime_prob',\n",
    "    'ie_anchoring_z',\n",
    "    'ie_gold_sensitivity_z',\n",
    "    # Options market submodel (1)\n",
    "    'options_risk_regime_prob',\n",
    "    # Temporal context submodel (1)\n",
    "    'temporal_context_score',\n",
    "]\n",
    "\n",
    "TARGET = 'gold_return_5d'\n",
    "\n",
    "assert len(FEATURE_COLUMNS) == 24, f\"Expected 24 features, got {len(FEATURE_COLUMNS)}\"\n",
    "print(f\"Features defined: {len(FEATURE_COLUMNS)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching (API-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# API-BASED DATA FETCHING\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FETCHING DATA FROM APIs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === Import libraries ===\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob as glob_mod\n",
    "\n",
    "# FRED API (install if needed)\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"fredapi\"], check=True)\n",
    "    from fredapi import Fred\n",
    "\n",
    "# === Dataset Path Resolution ===\n",
    "# Kaggle API v2 mounts datasets at /kaggle/input/datasets/{owner}/{dataset-slug}/\n",
    "# Older API uses /kaggle/input/{dataset-slug}/ or ../input/{dataset-slug}/\n",
    "DATASET_DIR = None\n",
    "candidate_paths = [\n",
    "    '/kaggle/input/datasets/bigbigzabuton/gold-prediction-submodels',\n",
    "    '/kaggle/input/gold-prediction-submodels',\n",
    "    '../input/gold-prediction-submodels',\n",
    "    '../input/datasets/bigbigzabuton/gold-prediction-submodels',\n",
    "]\n",
    "for p in candidate_paths:\n",
    "    if os.path.exists(p) and os.path.exists(os.path.join(p, 'vix.csv')):\n",
    "        DATASET_DIR = p\n",
    "        print(f\"Dataset found at: {p}\")\n",
    "        break\n",
    "\n",
    "if DATASET_DIR is None:\n",
    "    # Search broadly under /kaggle/input/\n",
    "    print(\"Dataset not at standard paths. Searching /kaggle/input/ recursively...\")\n",
    "    for d in glob_mod.glob('/kaggle/input/**/vix.csv', recursive=True):\n",
    "        DATASET_DIR = os.path.dirname(d)\n",
    "        print(f\"  Found via glob: {DATASET_DIR}\")\n",
    "        break\n",
    "\n",
    "if DATASET_DIR is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot find dataset. Tried paths:\\n\" +\n",
    "        \"\\n\".join(f\"  - {p}\" for p in candidate_paths) +\n",
    "        \"\\nPlease add 'bigbigzabuton/gold-prediction-submodels' as a dataset source in kernel settings.\"\n",
    "    )\n",
    "\n",
    "# Verify dataset files\n",
    "dataset_files = os.listdir(DATASET_DIR)\n",
    "assert 'vix.csv' in dataset_files, f\"vix.csv not found in {DATASET_DIR}. Contents: {dataset_files}\"\n",
    "print(f\"Dataset path: {DATASET_DIR}\")\n",
    "print(f\"  Files: {dataset_files}\")\n",
    "\n",
    "# === FRED API key (from Kaggle Secrets or hardcoded fallback) ===\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    FRED_API_KEY = user_secrets.get_secret(\"FRED_API_KEY\")\n",
    "    print(\"FRED API key loaded from Kaggle Secrets\")\n",
    "except Exception:\n",
    "    FRED_API_KEY = \"3ffb68facdf6321e180e380c00e909c8\"\n",
    "    print(\"FRED API key: using fallback\")\n",
    "\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "print(\"✓ FRED API initialized\")\n",
    "\n",
    "# === 1. Fetch Gold Price (target) ===\n",
    "print(\"\\nFetching gold price (GC=F)...\")\n",
    "gold = yf.download('GC=F', start='2014-01-01', end='2026-02-20', progress=False)\n",
    "gold_df = gold[['Close']].copy()\n",
    "gold_df.columns = ['gold_price']\n",
    "gold_df['gold_return'] = gold_df['gold_price'].pct_change() * 100\n",
    "gold_df['gold_return_5d'] = (gold_df['gold_price'].shift(-5) / gold_df['gold_price'] - 1) * 100\n",
    "gold_df['gold_return_daily'] = gold_df['gold_price'].pct_change() * 100\n",
    "gold_df = gold_df.dropna(subset=['gold_return_5d'])\n",
    "gold_df.index = pd.to_datetime(gold_df.index).strftime('%Y-%m-%d')\n",
    "print(f\"  Gold: {len(gold_df)} rows\")\n",
    "\n",
    "# === 2. Fetch Base Features ===\n",
    "print(\"\\nFetching base features...\")\n",
    "\n",
    "# Real Rate (DFII10)\n",
    "print(\"  Fetching real rate (DFII10)...\")\n",
    "real_rate = fred.get_series('DFII10', observation_start='2014-01-01')\n",
    "real_rate_df = real_rate.to_frame('real_rate_real_rate')\n",
    "real_rate_df.index = pd.to_datetime(real_rate_df.index).strftime('%Y-%m-%d')\n",
    "\n",
    "# DXY (DX-Y.NYB)\n",
    "print(\"  Fetching DXY (DX-Y.NYB)...\")\n",
    "dxy = yf.download('DX-Y.NYB', start='2014-01-01', end='2026-02-20', progress=False)\n",
    "dxy_df = dxy[['Close']].copy()\n",
    "dxy_df.columns = ['dxy_dxy']\n",
    "dxy_df.index = pd.to_datetime(dxy_df.index).strftime('%Y-%m-%d')\n",
    "\n",
    "# VIX (VIXCLS)\n",
    "print(\"  Fetching VIX (VIXCLS)...\")\n",
    "vix = fred.get_series('VIXCLS', observation_start='2014-01-01')\n",
    "vix_df = vix.to_frame('vix_vix')\n",
    "vix_df.index = pd.to_datetime(vix_df.index).strftime('%Y-%m-%d')\n",
    "\n",
    "# Yield Curve (DGS10 - DGS2)\n",
    "print(\"  Fetching yield curve (DGS10, DGS2)...\")\n",
    "dgs10 = fred.get_series('DGS10', observation_start='2014-01-01')\n",
    "dgs2 = fred.get_series('DGS2', observation_start='2014-01-01')\n",
    "yc_df = pd.DataFrame({'DGS10': dgs10, 'DGS2': dgs2})\n",
    "yc_df['yield_curve_yield_spread'] = yc_df['DGS10'] - yc_df['DGS2']\n",
    "yc_df = yc_df[['yield_curve_yield_spread']]\n",
    "yc_df.index = pd.to_datetime(yc_df.index).strftime('%Y-%m-%d')\n",
    "\n",
    "# Inflation Expectation (T10YIE)\n",
    "print(\"  Fetching inflation expectation (T10YIE)...\")\n",
    "infl_exp = fred.get_series('T10YIE', observation_start='2014-01-01')\n",
    "infl_exp_df = infl_exp.to_frame('inflation_expectation_inflation_expectation')\n",
    "infl_exp_df.index = pd.to_datetime(infl_exp_df.index).strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge base features\n",
    "base_features = gold_df[['gold_return_5d', 'gold_return_daily']].copy()\n",
    "for df in [real_rate_df, dxy_df, vix_df, yc_df, infl_exp_df]:\n",
    "    base_features = base_features.join(df, how='left')\n",
    "\n",
    "# Forward-fill missing values (weekends, holidays)\n",
    "base_features = base_features.ffill()\n",
    "print(f\"  Base features: {len(base_features)} rows, {len(base_features.columns)} columns\")\n",
    "\n",
    "# === 3. Load Submodel Outputs (from Kaggle Dataset) ===\n",
    "print(\"\\nLoading submodel outputs from Kaggle Dataset...\")\n",
    "\n",
    "submodel_files = {\n",
    "    'vix': {\n",
    "        'path': f'{DATASET_DIR}/vix.csv',\n",
    "        'columns': ['vix_regime_probability', 'vix_mean_reversion_z', 'vix_persistence'],\n",
    "        'date_col': 'date',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'technical': {\n",
    "        'path': f'{DATASET_DIR}/technical.csv',\n",
    "        'columns': ['tech_trend_regime_prob', 'tech_mean_reversion_z', 'tech_volatility_regime'],\n",
    "        'date_col': 'date',\n",
    "        'tz_aware': True,\n",
    "    },\n",
    "    'cross_asset': {\n",
    "        'path': f'{DATASET_DIR}/cross_asset.csv',\n",
    "        'columns': ['xasset_regime_prob', 'xasset_recession_signal', 'xasset_divergence'],\n",
    "        'date_col': 'Date',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'yield_curve': {\n",
    "        'path': f'{DATASET_DIR}/yield_curve.csv',\n",
    "        'columns': ['yc_spread_velocity_z', 'yc_curvature_z'],\n",
    "        'date_col': 'index',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'etf_flow': {\n",
    "        'path': f'{DATASET_DIR}/etf_flow.csv',\n",
    "        'columns': ['etf_regime_prob', 'etf_capital_intensity', 'etf_pv_divergence'],\n",
    "        'date_col': 'Date',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'inflation_expectation': {\n",
    "        'path': f'{DATASET_DIR}/inflation_expectation.csv',\n",
    "        'columns': ['ie_regime_prob', 'ie_anchoring_z', 'ie_gold_sensitivity_z'],\n",
    "        'date_col': 'Unnamed: 0',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "    'options_market': {\n",
    "        'path': f'{DATASET_DIR}/options_market.csv',\n",
    "        'columns': ['options_risk_regime_prob'],\n",
    "        'date_col': 'Date',\n",
    "        'tz_aware': True,\n",
    "    },\n",
    "    'temporal_context': {\n",
    "        'path': f'{DATASET_DIR}/temporal_context.csv',\n",
    "        'columns': ['temporal_context_score'],\n",
    "        'date_col': 'date',\n",
    "        'tz_aware': False,\n",
    "    },\n",
    "}\n",
    "\n",
    "submodel_dfs = {}\n",
    "for feature, spec in submodel_files.items():\n",
    "    df = pd.read_csv(spec['path'])\n",
    "\n",
    "    date_col = spec['date_col']\n",
    "    if spec['tz_aware']:\n",
    "        df['Date'] = pd.to_datetime(df[date_col], utc=True).dt.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        if date_col == 'index':\n",
    "            df['Date'] = pd.to_datetime(df.iloc[:, 0]).dt.strftime('%Y-%m-%d')\n",
    "        elif date_col == 'Unnamed: 0':\n",
    "            df['Date'] = pd.to_datetime(df['Unnamed: 0']).dt.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            df['Date'] = pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    df = df[['Date'] + spec['columns']]\n",
    "    df = df.set_index('Date')\n",
    "    submodel_dfs[feature] = df\n",
    "    print(f\"  {feature}: {len(df)} rows\")\n",
    "\n",
    "print(f\"\\n✓ Data fetching complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation and NaN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply transformations (stationary conversion) ===\n",
    "print(\"\\nApplying transformations...\")\n",
    "\n",
    "# Create final feature DataFrame\n",
    "final_df = base_features.copy()\n",
    "\n",
    "# Base features (4 diff, 1 level)\n",
    "final_df['real_rate_change'] = final_df['real_rate_real_rate'].diff()\n",
    "final_df['dxy_change'] = final_df['dxy_dxy'].diff()\n",
    "final_df['vix'] = final_df['vix_vix']\n",
    "final_df['yield_spread_change'] = final_df['yield_curve_yield_spread'].diff()\n",
    "final_df['inflation_exp_change'] = final_df['inflation_expectation_inflation_expectation'].diff()\n",
    "\n",
    "# Drop original raw columns\n",
    "final_df = final_df.drop(columns=['real_rate_real_rate', 'dxy_dxy', 'vix_vix',\n",
    "                                    'yield_curve_yield_spread', 'inflation_expectation_inflation_expectation'])\n",
    "\n",
    "# === Merge submodel features ===\n",
    "print(\"\\nMerging submodel outputs...\")\n",
    "for feature, df in submodel_dfs.items():\n",
    "    final_df = final_df.join(df, how='left')\n",
    "\n",
    "print(f\"  Features after merge: {final_df.shape[1]} columns, {len(final_df)} rows\")\n",
    "\n",
    "# === NaN Imputation (domain-specific) ===\n",
    "print(\"\\nApplying NaN imputation...\")\n",
    "\n",
    "nan_before = final_df.isna().sum().sum()\n",
    "print(f\"  NaN before imputation: {nan_before}\")\n",
    "\n",
    "# Regime probability columns → 0.5 (maximum uncertainty)\n",
    "regime_cols = ['vix_regime_probability', 'tech_trend_regime_prob', \n",
    "               'xasset_regime_prob', 'etf_regime_prob', 'ie_regime_prob',\n",
    "               'options_risk_regime_prob',\n",
    "               'temporal_context_score',\n",
    "               ]\n",
    "for col in regime_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(0.5)\n",
    "\n",
    "# Z-score columns → 0.0 (at mean)\n",
    "z_cols = ['vix_mean_reversion_z', 'tech_mean_reversion_z', \n",
    "          'yc_spread_velocity_z', 'yc_curvature_z',\n",
    "          'etf_capital_intensity', 'etf_pv_divergence',\n",
    "          'ie_anchoring_z', 'ie_gold_sensitivity_z']\n",
    "for col in z_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(0.0)\n",
    "\n",
    "# Divergence/signal columns → 0.0 (neutral)\n",
    "div_cols = ['xasset_recession_signal', 'xasset_divergence']\n",
    "for col in div_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(0.0)\n",
    "\n",
    "# Continuous state columns → median\n",
    "cont_cols = ['tech_volatility_regime', 'vix_persistence']\n",
    "for col in cont_cols:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].fillna(final_df[col].median())\n",
    "\n",
    "# Drop rows with NaN in target or base features (critical rows)\n",
    "final_df = final_df.dropna(subset=['gold_return_5d', 'real_rate_change', 'dxy_change', \n",
    "                                     'vix', 'yield_spread_change', 'inflation_exp_change'])\n",
    "\n",
    "nan_after = final_df.isna().sum().sum()\n",
    "print(f\"  NaN after imputation: {nan_after}\")\n",
    "print(f\"  Final dataset: {len(final_df)} rows\")\n",
    "\n",
    "# === Verify feature set ===\n",
    "assert all(col in final_df.columns for col in FEATURE_COLUMNS), \"Missing features after merge!\"\n",
    "assert TARGET in final_df.columns, \"Target not found!\"\n",
    "print(f\"\\n✓ All {len(FEATURE_COLUMNS)} features present\")\n",
    "print(f\"✓ Dataset shape: {final_df.shape}\")\n",
    "print(f\"✓ Date range: {final_df.index.min()} to {final_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val/Test Split (70/15/15) + Non-Overlapping Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train/Val/Test Split (70/15/15, time-series order) ===\n",
    "n_total = len(final_df)\n",
    "n_train = int(n_total * 0.70)\n",
    "n_val = int(n_total * 0.15)\n",
    "\n",
    "train_df_full = final_df.iloc[:n_train].copy()\n",
    "val_df_full = final_df.iloc[n_train:n_train+n_val].copy()\n",
    "test_df_full = final_df.iloc[n_train+n_val:].copy()\n",
    "\n",
    "print(f\"\\n✓ Full splits (overlapping):\")\n",
    "print(f\"  Train: {len(train_df_full)} rows ({len(train_df_full)/n_total*100:.1f}%) - {train_df_full.index.min()} to {train_df_full.index.max()}\")\n",
    "print(f\"  Val:   {len(val_df_full)} rows ({len(val_df_full)/n_total*100:.1f}%) - {val_df_full.index.min()} to {val_df_full.index.max()}\")\n",
    "print(f\"  Test:  {len(test_df_full)} rows ({len(test_df_full)/n_total*100:.1f}%) - {test_df_full.index.min()} to {test_df_full.index.max()}\")\n",
    "\n",
    "# === Non-overlapping subsampling ===\n",
    "print(\"\\n--- NON-OVERLAPPING SUBSAMPLING ---\")\n",
    "train_df = train_df_full.iloc[::5].copy()\n",
    "val_df = val_df_full.iloc[::5].copy()\n",
    "test_df = test_df_full.copy()  # Test remains full for evaluation comparability\n",
    "\n",
    "print(f\"Non-overlapping: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)} (full)\")\n",
    "print(f\"Samples per feature (non-overlapping train): {len(train_df) / len(FEATURE_COLUMNS):.1f}:1\")\n",
    "\n",
    "# Verify no data leakage\n",
    "assert train_df_full.index.max() < val_df_full.index.min(), \"Train-val overlap detected!\"\n",
    "assert val_df_full.index.max() < test_df_full.index.min(), \"Val-test overlap detected!\"\n",
    "print(f\"\\n✓ No time-series leakage detected\")\n",
    "\n",
    "# === Target centering ===\n",
    "X_train = train_df[FEATURE_COLUMNS].values\n",
    "y_train = train_df[TARGET].values\n",
    "X_val = val_df[FEATURE_COLUMNS].values\n",
    "y_val = val_df[TARGET].values\n",
    "X_test = test_df[FEATURE_COLUMNS].values\n",
    "y_test = test_df[TARGET].values\n",
    "\n",
    "# Compute and apply centering\n",
    "train_mean_5d = y_train.mean()\n",
    "y_train_centered = y_train - train_mean_5d\n",
    "y_val_centered = y_val - train_mean_5d\n",
    "\n",
    "print(f\"\\nTarget centering:\")\n",
    "print(f\"  Train mean (5d return): {train_mean_5d:.4f}%\")\n",
    "print(f\"  Centered train mean: {y_train_centered.mean():.6f}% (should be ~0)\")\n",
    "print(f\"  Train positive fraction: {(y_train > 0).sum() / len(y_train)*100:.1f}%\")\n",
    "print(f\"  Centered train positive fraction: {(y_train_centered > 0).sum() / len(y_train_centered)*100:.1f}%\")\n",
    "\n",
    "# Store dates for output\n",
    "dates_train = train_df.index\n",
    "dates_val = val_df.index\n",
    "dates_test = test_df.index\n",
    "\n",
    "# Store daily returns for Sharpe computation\n",
    "daily_returns_train = train_df['gold_return_daily'].values\n",
    "daily_returns_val = val_df['gold_return_daily'].values\n",
    "daily_returns_test = test_df['gold_return_daily'].values\n",
    "\n",
    "print(f\"\\nArray shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_direction_accuracy(y_true, y_pred):\n",
    "    \"\"\"Direction accuracy, excluding zeros.\"\"\"\n",
    "    mask = (y_true != 0) & (y_pred != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return (np.sign(y_pred[mask]) == np.sign(y_true[mask])).mean()\n",
    "\n",
    "def compute_mae(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Error.\"\"\"\n",
    "    return np.abs(y_pred - y_true).mean()\n",
    "\n",
    "def compute_sharpe_weekly_simple(y_true_5d, y_pred_5d, cost_bps=5.0):\n",
    "    \"\"\"Simplified Sharpe for weekly predictions (used in Optuna).\"\"\"\n",
    "    positions = np.sign(y_pred_5d)\n",
    "    strategy_returns = positions * y_true_5d / 100.0\n",
    "    position_changes = np.abs(np.diff(positions, prepend=0))\n",
    "    trade_costs = position_changes * (cost_bps / 10000.0)\n",
    "    net_returns = strategy_returns - trade_costs\n",
    "    if len(net_returns) < 2 or net_returns.std() == 0:\n",
    "        return 0.0\n",
    "    return (net_returns.mean() / net_returns.std()) * np.sqrt(52)\n",
    "\n",
    "def compute_sharpe_trade_cost(y_true, y_pred, cost_bps=5.0):\n",
    "    \"\"\"Sharpe ratio with position-change cost (5bps per change).\"\"\"\n",
    "    positions = np.sign(y_pred)\n",
    "    strategy_returns = positions * y_true / 100.0\n",
    "    position_changes = np.abs(np.diff(positions, prepend=0))\n",
    "    trade_costs = position_changes * (cost_bps / 10000.0)\n",
    "    net_returns = strategy_returns - trade_costs\n",
    "    if len(net_returns) < 2 or net_returns.std() == 0:\n",
    "        return 0.0\n",
    "    return (net_returns.mean() / net_returns.std()) * np.sqrt(252)\n",
    "\n",
    "def compute_hcda(y_true, y_pred, threshold_percentile=80):\n",
    "    \"\"\"High-confidence direction accuracy (top 20% by |prediction|).\"\"\"\n",
    "    threshold = np.percentile(np.abs(y_pred), threshold_percentile)\n",
    "    hc_mask = np.abs(y_pred) > threshold\n",
    "    \n",
    "    if hc_mask.sum() == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    coverage = hc_mask.sum() / len(y_pred)\n",
    "    hc_pred = y_pred[hc_mask]\n",
    "    hc_actual = y_true[hc_mask]\n",
    "    \n",
    "    mask = (hc_actual != 0) & (hc_pred != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0, coverage\n",
    "    \n",
    "    da = (np.sign(hc_pred[mask]) == np.sign(hc_actual[mask])).mean()\n",
    "    return da, coverage\n",
    "\n",
    "def compute_hcda_bootstrap(y_true, y_pred, bootstrap_std, threshold_percentile=80):\n",
    "    \"\"\"\n",
    "    HCDA using bootstrap variance-based confidence.\n",
    "    High confidence = LOW variance (certain predictions)\n",
    "    Top 20% by inverse variance: 1 / (1 + std)\n",
    "    \"\"\"\n",
    "    confidence = 1.0 / (1.0 + bootstrap_std)\n",
    "    threshold = np.percentile(confidence, threshold_percentile)\n",
    "    hc_mask = confidence > threshold\n",
    "    \n",
    "    if hc_mask.sum() == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    coverage = hc_mask.sum() / len(y_pred)\n",
    "    hc_pred = y_pred[hc_mask]\n",
    "    hc_actual = y_true[hc_mask]\n",
    "    \n",
    "    mask = (hc_actual != 0) & (hc_pred != 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0, coverage\n",
    "    \n",
    "    da = (np.sign(hc_pred[mask]) == np.sign(hc_actual[mask])).mean()\n",
    "    return da, coverage\n",
    "\n",
    "def compute_naive_da(y_true):\n",
    "    \"\"\"DA of naive always-up strategy.\"\"\"\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return (y_true[mask] > 0).mean()\n",
    "\n",
    "# Compute naive baselines\n",
    "naive_da_train = compute_naive_da(y_train)\n",
    "naive_da_val = compute_naive_da(y_val)\n",
    "naive_da_test = compute_naive_da(y_test)\n",
    "\n",
    "print(\"\\nNaive always-up DA:\")\n",
    "print(f\"  Train: {naive_da_train*100:.2f}%\")\n",
    "print(f\"  Val:   {naive_da_val*100:.2f}%\")\n",
    "print(f\"  Test:  {naive_da_test*100:.2f}%\")\n",
    "\n",
    "print(\"\\nMetric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna HPO (100 trials) - ATTEMPT 2 HP RANGES (RELAXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_objective(trial):\n",
    "    \"\"\"\n",
    "    Naive-aware objective function for weekly prediction.\n",
    "\n",
    "    Key differences from Attempt 1:\n",
    "    1. Train on centered targets (y_train_centered, y_val_centered)\n",
    "    2. Un-center predictions before computing metrics\n",
    "    3. DA component measures SKILL above naive, not raw DA\n",
    "    4. Sharpe component has trade-activity gate\n",
    "    5. Constant-output penalty prevents near-constant predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # === Sample hyperparameters (RELAXED RANGES) ===\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 3, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 5.0, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'rmse',\n",
    "        'verbosity': 0,\n",
    "        'seed': 42 + trial.number,\n",
    "    }\n",
    "\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "\n",
    "    # === Train model on CENTERED targets ===\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=n_estimators, early_stopping_rounds=50)\n",
    "    model.fit(X_train, y_train_centered, eval_set=[(X_val, y_val_centered)], verbose=False)\n",
    "\n",
    "    # === Predictions (un-center for metric computation) ===\n",
    "    train_pred_centered = model.predict(X_train)\n",
    "    val_pred_centered = model.predict(X_val)\n",
    "\n",
    "    train_pred = train_pred_centered + train_mean_5d\n",
    "    val_pred = val_pred_centered + train_mean_5d\n",
    "\n",
    "    # === Compute model metrics ===\n",
    "    train_da = compute_direction_accuracy(y_train, train_pred)\n",
    "    val_da = compute_direction_accuracy(y_val, val_pred)\n",
    "    val_mae = compute_mae(y_val, val_pred)\n",
    "    val_sharpe = compute_sharpe_weekly_simple(y_val, val_pred)\n",
    "    val_hc_da, val_hc_coverage = compute_hcda(y_val, val_pred, threshold_percentile=80)\n",
    "\n",
    "    # === COMPONENT 1: DA SKILL (35%) ===\n",
    "    # Measures directional accuracy ABOVE naive always-up\n",
    "    da_skill_pp = (val_da - naive_da_val) * 100  # In percentage points\n",
    "    da_skill_norm = np.clip(da_skill_pp / 10.0, -0.5, 1.0)\n",
    "    # Maps: -5pp -> -0.5, 0pp -> 0, +10pp -> 1.0\n",
    "    # Negative skill is penalized, not just ignored\n",
    "\n",
    "    # === COMPONENT 2: SHARPE with trade-activity gate (30%) ===\n",
    "    # Require minimum position variation to earn Sharpe reward\n",
    "    positions = np.sign(val_pred)\n",
    "    n_position_changes = np.sum(np.abs(np.diff(positions)) > 0)\n",
    "\n",
    "    if n_position_changes < 3:\n",
    "        # Fewer than 3 position changes = effectively constant direction\n",
    "        sharpe_norm = 0.0\n",
    "    else:\n",
    "        sharpe_norm = np.clip((val_sharpe + 2.0) / 4.0, 0.0, 1.0)\n",
    "        # Maps [-2, +2] to [0, 1]. Tighter range than Attempt 1's [-3, +3]/6\n",
    "\n",
    "    # === COMPONENT 3: MAE (15%) ===\n",
    "    mae_norm = np.clip((2.5 - val_mae) / 1.5, 0.0, 1.0)\n",
    "    # Same as Attempt 1: maps [1.0%, 2.5%] to [1, 0]\n",
    "\n",
    "    # === COMPONENT 4: HCDA SKILL (20%) ===\n",
    "    hcda_skill_pp = (val_hc_da - naive_da_val) * 100\n",
    "    hcda_skill_norm = np.clip(hcda_skill_pp / 10.0, -0.5, 1.0)\n",
    "\n",
    "    # === OVERFITTING PENALTY ===\n",
    "    da_gap = (train_da - val_da) * 100\n",
    "    overfit_penalty = max(0.0, (da_gap - 8.0) / 20.0)\n",
    "    # Stricter: penalty starts at 8pp gap (was 10pp), ramps faster\n",
    "\n",
    "    # === CONSTANT-OUTPUT PENALTY ===\n",
    "    pred_std = np.std(val_pred_centered)  # Variation in centered predictions\n",
    "    if pred_std < 0.01:\n",
    "        constant_penalty = 1.0  # Nuclear: effectively zero objective\n",
    "    elif pred_std < 0.1:\n",
    "        constant_penalty = (0.1 - pred_std) / 0.09 * 0.5\n",
    "    else:\n",
    "        constant_penalty = 0.0\n",
    "\n",
    "    # === COMPOSITE OBJECTIVE ===\n",
    "    objective = (\n",
    "        0.35 * da_skill_norm +\n",
    "        0.30 * sharpe_norm +\n",
    "        0.15 * mae_norm +\n",
    "        0.20 * hcda_skill_norm\n",
    "    ) - 0.30 * overfit_penalty - constant_penalty\n",
    "\n",
    "    # === Log trial details ===\n",
    "    trial.set_user_attr('val_da', float(val_da))\n",
    "    trial.set_user_attr('val_mae', float(val_mae))\n",
    "    trial.set_user_attr('val_sharpe', float(val_sharpe))\n",
    "    trial.set_user_attr('val_hc_da', float(val_hc_da))\n",
    "    trial.set_user_attr('val_hc_coverage', float(val_hc_coverage))\n",
    "    trial.set_user_attr('train_da', float(train_da))\n",
    "    trial.set_user_attr('da_gap_pp', float(da_gap))\n",
    "    trial.set_user_attr('da_skill_pp', float(da_skill_pp))\n",
    "    trial.set_user_attr('naive_da_val', float(naive_da_val))\n",
    "    trial.set_user_attr('n_position_changes', int(n_position_changes))\n",
    "    trial.set_user_attr('pred_std_centered', float(pred_std))\n",
    "    trial.set_user_attr('constant_penalty', float(constant_penalty))\n",
    "    trial.set_user_attr('n_estimators_used',\n",
    "                         int(model.best_iteration + 1) if hasattr(model, 'best_iteration')\n",
    "                         and model.best_iteration is not None else n_estimators)\n",
    "\n",
    "    return objective\n",
    "\n",
    "print(\"Optuna objective function defined (Attempt 2: Naive-aware, relaxed HP ranges)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING OPTUNA HPO (100 trials, 1-hour timeout)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    optuna_objective,\n",
    "    n_trials=100,\n",
    "    timeout=3600,  # REDUCED from 7200\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOptuna optimization complete\")\n",
    "print(f\"  Trials completed: {len(study.trials)}\")\n",
    "print(f\"  Best value: {study.best_value:.4f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest trial validation metrics:\")\n",
    "print(f\"  DA:     {best_trial.user_attrs['val_da']*100:.2f}%\")\n",
    "print(f\"  HCDA:   {best_trial.user_attrs['val_hc_da']*100:.2f}%\")\n",
    "print(f\"  MAE:    {best_trial.user_attrs['val_mae']:.4f}%\")\n",
    "print(f\"  Sharpe: {best_trial.user_attrs['val_sharpe']:.2f}\")\n",
    "print(f\"  DA gap: {best_trial.user_attrs['da_gap_pp']:.2f}pp\")\n",
    "\n",
    "# Skill analysis\n",
    "print(f\"\\nBest trial naive analysis:\")\n",
    "print(f\"  Val DA:       {best_trial.user_attrs['val_da']*100:.2f}%\")\n",
    "print(f\"  Naive DA val: {best_trial.user_attrs['naive_da_val']*100:.2f}%\")\n",
    "print(f\"  DA skill:     {best_trial.user_attrs['da_skill_pp']:+.2f}pp\")\n",
    "print(f\"  Position changes: {best_trial.user_attrs['n_position_changes']}\")\n",
    "print(f\"  Pred std (centered): {best_trial.user_attrs['pred_std_centered']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallback Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FALLBACK CONFIGURATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fallback A: Attempt 1 best params (conservative)\n",
    "FALLBACK_A_PARAMS = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 21,\n",
    "    'reg_lambda': 5.19,\n",
    "    'reg_alpha': 2.04,\n",
    "    'subsample': 0.459,\n",
    "    'colsample_bytree': 0.375,\n",
    "    'learning_rate': 0.017,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'verbosity': 0,\n",
    "    'seed': 42,\n",
    "}\n",
    "FALLBACK_A_N_EST = 175\n",
    "\n",
    "# Fallback B: Medium expressiveness (new)\n",
    "FALLBACK_B_PARAMS = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 8,\n",
    "    'reg_lambda': 2.0,\n",
    "    'reg_alpha': 0.5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'learning_rate': 0.03,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'verbosity': 0,\n",
    "    'seed': 42,\n",
    "}\n",
    "FALLBACK_B_N_EST = 200\n",
    "\n",
    "def evaluate_fallback(params, n_est, name):\n",
    "    \"\"\"Evaluate a fallback configuration.\"\"\"\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=n_est, early_stopping_rounds=50)\n",
    "    model.fit(X_train, y_train_centered, eval_set=[(X_val, y_val_centered)], verbose=False)\n",
    "    \n",
    "    train_pred_c = model.predict(X_train)\n",
    "    val_pred_c = model.predict(X_val)\n",
    "    train_pred = train_pred_c + train_mean_5d\n",
    "    val_pred = val_pred_c + train_mean_5d\n",
    "    \n",
    "    train_da = compute_direction_accuracy(y_train, train_pred)\n",
    "    val_da = compute_direction_accuracy(y_val, val_pred)\n",
    "    val_mae = compute_mae(y_val, val_pred)\n",
    "    val_sharpe = compute_sharpe_weekly_simple(y_val, val_pred)\n",
    "    val_hc_da, _ = compute_hcda(y_val, val_pred, threshold_percentile=80)\n",
    "    da_gap = (train_da - val_da) * 100\n",
    "    \n",
    "    # Compute objective with same formula\n",
    "    da_skill_pp = (val_da - naive_da_val) * 100\n",
    "    da_skill_norm = np.clip(da_skill_pp / 10.0, -0.5, 1.0)\n",
    "    \n",
    "    positions = np.sign(val_pred)\n",
    "    n_position_changes = np.sum(np.abs(np.diff(positions)) > 0)\n",
    "    if n_position_changes < 3:\n",
    "        sharpe_norm = 0.0\n",
    "    else:\n",
    "        sharpe_norm = np.clip((val_sharpe + 2.0) / 4.0, 0.0, 1.0)\n",
    "    \n",
    "    mae_norm = np.clip((2.5 - val_mae) / 1.5, 0.0, 1.0)\n",
    "    \n",
    "    hcda_skill_pp = (val_hc_da - naive_da_val) * 100\n",
    "    hcda_skill_norm = np.clip(hcda_skill_pp / 10.0, -0.5, 1.0)\n",
    "    \n",
    "    overfit_penalty = max(0.0, (da_gap - 8.0) / 20.0)\n",
    "    \n",
    "    pred_std = np.std(val_pred_c)\n",
    "    if pred_std < 0.01:\n",
    "        constant_penalty = 1.0\n",
    "    elif pred_std < 0.1:\n",
    "        constant_penalty = (0.1 - pred_std) / 0.09 * 0.5\n",
    "    else:\n",
    "        constant_penalty = 0.0\n",
    "    \n",
    "    objective = (\n",
    "        0.35 * da_skill_norm +\n",
    "        0.30 * sharpe_norm +\n",
    "        0.15 * mae_norm +\n",
    "        0.20 * hcda_skill_norm\n",
    "    ) - 0.30 * overfit_penalty - constant_penalty\n",
    "    \n",
    "    print(f\"  DA:     {val_da*100:.2f}%\")\n",
    "    print(f\"  HCDA:   {val_hc_da*100:.2f}%\")\n",
    "    print(f\"  MAE:    {val_mae:.4f}%\")\n",
    "    print(f\"  Sharpe: {val_sharpe:.2f}\")\n",
    "    print(f\"  DA gap: {da_gap:.2f}pp\")\n",
    "    print(f\"  DA skill: {da_skill_pp:+.2f}pp\")\n",
    "    print(f\"  Position changes: {n_position_changes}\")\n",
    "    print(f\"  Pred std (centered): {pred_std:.4f}\")\n",
    "    print(f\"  Composite objective: {objective:.4f}\")\n",
    "    \n",
    "    return objective, params, n_est\n",
    "\n",
    "fallback_a_obj, _, _ = evaluate_fallback(FALLBACK_A_PARAMS, FALLBACK_A_N_EST, \"Fallback A (Attempt 1 params)\")\n",
    "fallback_b_obj, _, _ = evaluate_fallback(FALLBACK_B_PARAMS, FALLBACK_B_N_EST, \"Fallback B (Medium expressiveness)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURATION SELECTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Optuna best:  {study.best_value:.4f}\")\n",
    "print(f\"  Fallback A:   {fallback_a_obj:.4f}\")\n",
    "print(f\"  Fallback B:   {fallback_b_obj:.4f}\")\n",
    "\n",
    "if study.best_value >= max(fallback_a_obj, fallback_b_obj):\n",
    "    print(\"\\n✓ Using Optuna best configuration\")\n",
    "    selected_config = 'optuna'\n",
    "    selected_params = study.best_params\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        max_depth=selected_params['max_depth'],\n",
    "        min_child_weight=selected_params['min_child_weight'],\n",
    "        subsample=selected_params['subsample'],\n",
    "        colsample_bytree=selected_params['colsample_bytree'],\n",
    "        reg_lambda=selected_params['reg_lambda'],\n",
    "        reg_alpha=selected_params['reg_alpha'],\n",
    "        learning_rate=selected_params['learning_rate'],\n",
    "        tree_method='hist',\n",
    "        eval_metric='rmse',\n",
    "        verbosity=0,\n",
    "        seed=42,\n",
    "        n_estimators=selected_params['n_estimators'],\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "elif fallback_a_obj > fallback_b_obj:\n",
    "    print(\"\\n✓ Using Fallback A (Attempt 1 params)\")\n",
    "    selected_config = 'fallback_a'\n",
    "    selected_params = FALLBACK_A_PARAMS.copy()\n",
    "    selected_params['n_estimators'] = FALLBACK_A_N_EST\n",
    "    final_model = xgb.XGBRegressor(**FALLBACK_A_PARAMS, n_estimators=FALLBACK_A_N_EST, early_stopping_rounds=50)\n",
    "else:\n",
    "    print(\"\\n✓ Using Fallback B (Medium expressiveness)\")\n",
    "    selected_config = 'fallback_b'\n",
    "    selected_params = FALLBACK_B_PARAMS.copy()\n",
    "    selected_params['n_estimators'] = FALLBACK_B_N_EST\n",
    "    final_model = xgb.XGBRegressor(**FALLBACK_B_PARAMS, n_estimators=FALLBACK_B_N_EST, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TRAINING FINAL MODEL ({selected_config.upper()} CONFIG)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_model.fit(X_train, y_train_centered, eval_set=[(X_val, y_val_centered)], verbose=False)\n",
    "\n",
    "# Predictions (UN-CENTER)\n",
    "pred_train_centered = final_model.predict(X_train)\n",
    "pred_val_centered = final_model.predict(X_val)\n",
    "pred_test_centered = final_model.predict(X_test)\n",
    "\n",
    "pred_train = pred_train_centered + train_mean_5d\n",
    "pred_val = pred_val_centered + train_mean_5d\n",
    "pred_test = pred_test_centered + train_mean_5d\n",
    "\n",
    "pred_full = np.concatenate([pred_train, pred_val, pred_test])\n",
    "dates_full = pd.Index(list(dates_train) + list(dates_val) + list(dates_test))\n",
    "y_full = np.concatenate([y_train, y_val, y_test])\n",
    "\n",
    "print(\"\\nRaw predictions generated (un-centered):\")\n",
    "print(f\"  Train: mean={pred_train.mean():.4f}, std={pred_train.std():.4f}\")\n",
    "print(f\"  Val:   mean={pred_val.mean():.4f}, std={pred_val.std():.4f}\")\n",
    "print(f\"  Test:  mean={pred_test.mean():.4f}, std={pred_test.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST-TRAINING STEP 1: OLS Output Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OLS OUTPUT SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# OLS scaling on un-centered predictions\n",
    "numerator = np.sum(pred_val * y_val)\n",
    "denominator = np.sum(pred_val ** 2)\n",
    "alpha_ols = numerator / denominator if denominator != 0 else 1.0\n",
    "alpha_ols = np.clip(alpha_ols, 0.5, 10.0)\n",
    "\n",
    "print(f\"\\nOLS scaling factor: {alpha_ols:.2f}\")\n",
    "\n",
    "scaled_pred_train = pred_train * alpha_ols\n",
    "scaled_pred_val = pred_val * alpha_ols\n",
    "scaled_pred_test = pred_test * alpha_ols\n",
    "scaled_pred_full = pred_full * alpha_ols\n",
    "\n",
    "mae_raw = np.mean(np.abs(pred_test - y_test))\n",
    "mae_scaled = np.mean(np.abs(scaled_pred_test - y_test))\n",
    "print(f\"\\nMAE (raw):    {mae_raw:.4f}%\")\n",
    "print(f\"MAE (scaled): {mae_scaled:.4f}%\")\n",
    "print(f\"MAE delta:    {mae_scaled - mae_raw:+.4f}%\")\n",
    "\n",
    "da_raw = compute_direction_accuracy(y_test, pred_test)\n",
    "da_scaled = compute_direction_accuracy(y_test, scaled_pred_test)\n",
    "assert abs(da_raw - da_scaled) < 1e-10, \"Scaling changed DA!\"\n",
    "print(\"\\n✓ DA and Sharpe: unchanged by scaling (verified)\")\n",
    "\n",
    "use_scaled = mae_scaled < mae_raw\n",
    "if use_scaled:\n",
    "    print(f\"\\n✓ Using SCALED predictions for MAE (improvement: {mae_raw - mae_scaled:.4f}%)\")\n",
    "else:\n",
    "    print(f\"\\n✓ Using RAW predictions for MAE (scaling degraded by {mae_scaled - mae_raw:.4f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST-TRAINING STEP 2: Bootstrap Ensemble for Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BOOTSTRAP ENSEMBLE CONFIDENCE SCORING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTraining bootstrap ensemble (5 models)...\")\n",
    "bootstrap_models = []\n",
    "bootstrap_seeds = [42, 43, 44, 45, 46]\n",
    "\n",
    "for i, seed in enumerate(bootstrap_seeds):\n",
    "    print(f\"  Training model {i+1}/5 (seed={seed})...\")\n",
    "    \n",
    "    bootstrap_params = selected_params.copy()\n",
    "    \n",
    "    model_boot = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        max_depth=bootstrap_params['max_depth'],\n",
    "        min_child_weight=bootstrap_params['min_child_weight'],\n",
    "        subsample=bootstrap_params['subsample'],\n",
    "        colsample_bytree=bootstrap_params['colsample_bytree'],\n",
    "        reg_lambda=bootstrap_params['reg_lambda'],\n",
    "        reg_alpha=bootstrap_params['reg_alpha'],\n",
    "        learning_rate=bootstrap_params['learning_rate'],\n",
    "        tree_method='hist',\n",
    "        eval_metric='rmse',\n",
    "        verbosity=0,\n",
    "        seed=seed,\n",
    "        n_estimators=bootstrap_params['n_estimators'],\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    \n",
    "    # Train on centered targets\n",
    "    model_boot.fit(X_train, y_train_centered, eval_set=[(X_val, y_val_centered)], verbose=False)\n",
    "    bootstrap_models.append(model_boot)\n",
    "\n",
    "print(f\"\\n✓ Bootstrap ensemble trained: {len(bootstrap_models)} models\")\n",
    "\n",
    "print(\"\\nGenerating predictions from ensemble...\")\n",
    "# Un-center predictions\n",
    "ensemble_preds_train = np.array([m.predict(X_train) + train_mean_5d for m in bootstrap_models])\n",
    "ensemble_preds_val = np.array([m.predict(X_val) + train_mean_5d for m in bootstrap_models])\n",
    "ensemble_preds_test = np.array([m.predict(X_test) + train_mean_5d for m in bootstrap_models])\n",
    "\n",
    "bootstrap_std_train = np.std(ensemble_preds_train, axis=0)\n",
    "bootstrap_std_val = np.std(ensemble_preds_val, axis=0)\n",
    "bootstrap_std_test = np.std(ensemble_preds_test, axis=0)\n",
    "\n",
    "bootstrap_conf_train = 1.0 / (1.0 + bootstrap_std_train)\n",
    "bootstrap_conf_val = 1.0 / (1.0 + bootstrap_std_val)\n",
    "bootstrap_conf_test = 1.0 / (1.0 + bootstrap_std_test)\n",
    "\n",
    "print(f\"\\nBootstrap variance statistics (test set):\")\n",
    "print(f\"  Std range: [{bootstrap_std_test.min():.4f}, {bootstrap_std_test.max():.4f}]\")\n",
    "print(f\"  Std mean:  {bootstrap_std_test.mean():.4f}\")\n",
    "print(f\"  Confidence range: [{bootstrap_conf_test.min():.4f}, {bootstrap_conf_test.max():.4f}]\")\n",
    "print(f\"  Confidence mean:  {bootstrap_conf_test.mean():.4f}\")\n",
    "\n",
    "hcda_bootstrap_test, hcda_bootstrap_cov = compute_hcda_bootstrap(y_test, pred_test, bootstrap_std_test)\n",
    "hcda_pred_test, hcda_pred_cov = compute_hcda(y_test, pred_test)\n",
    "\n",
    "print(f\"\\nHCDA comparison (test set):\")\n",
    "print(f\"  Bootstrap variance: {hcda_bootstrap_test*100:.2f}% (N={int(hcda_bootstrap_cov*len(y_test))})\")\n",
    "print(f\"  |prediction|:       {hcda_pred_test*100:.2f}% (N={int(hcda_pred_cov*len(y_test))})\")\n",
    "print(f\"  Improvement:        {(hcda_bootstrap_test - hcda_pred_test)*100:+.2f}pp\")\n",
    "\n",
    "use_bootstrap_hcda = hcda_bootstrap_test > hcda_pred_test\n",
    "if use_bootstrap_hcda:\n",
    "    print(f\"\\n✓ Using bootstrap variance for HCDA (better by {(hcda_bootstrap_test - hcda_pred_test)*100:.2f}pp)\")\n",
    "    primary_hcda_method = 'bootstrap'\n",
    "    primary_hcda_value = hcda_bootstrap_test\n",
    "else:\n",
    "    print(f\"\\n✓ Using |prediction| for HCDA (better by {(hcda_pred_test - hcda_bootstrap_test)*100:.2f}pp)\")\n",
    "    primary_hcda_method = 'pred'\n",
    "    primary_hcda_value = hcda_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on All Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_all = {}\n",
    "for split_name, y_true, y_pred_raw, y_pred_scaled in [\n",
    "    ('train', y_train, pred_train, scaled_pred_train),\n",
    "    ('val', y_val, pred_val, scaled_pred_val),\n",
    "    ('test', y_test, pred_test, scaled_pred_test),\n",
    "]:\n",
    "    da = compute_direction_accuracy(y_true, y_pred_raw)\n",
    "    mae_raw_split = compute_mae(y_true, y_pred_raw)\n",
    "    mae_scaled_split = compute_mae(y_true, y_pred_scaled)\n",
    "    mae = min(mae_raw_split, mae_scaled_split)\n",
    "    sharpe = compute_sharpe_weekly_simple(y_true, y_pred_raw)\n",
    "    hc_da, hc_coverage = compute_hcda(y_true, y_pred_raw, threshold_percentile=80)\n",
    "    \n",
    "    metrics_all[split_name] = {\n",
    "        'direction_accuracy': float(da),\n",
    "        'high_confidence_da': float(hc_da),\n",
    "        'high_confidence_coverage': float(hc_coverage),\n",
    "        'mae': float(mae),\n",
    "        'mae_raw': float(mae_raw_split),\n",
    "        'mae_scaled': float(mae_scaled_split),\n",
    "        'sharpe_ratio': float(sharpe),\n",
    "    }\n",
    "\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    m = metrics_all[split_name]\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    print(f\"  DA:     {m['direction_accuracy']*100:.2f}%\")\n",
    "    print(f\"  HCDA:   {m['high_confidence_da']*100:.2f}% (coverage: {m['high_confidence_coverage']*100:.1f}%)\")\n",
    "    print(f\"  MAE:    {m['mae']:.4f}% (raw: {m['mae_raw']:.4f}%, scaled: {m['mae_scaled']:.4f}%)\")\n",
    "    print(f\"  Sharpe: {m['sharpe_ratio']:.2f}\")\n",
    "\n",
    "train_test_da_gap = (metrics_all['train']['direction_accuracy'] - metrics_all['test']['direction_accuracy']) * 100\n",
    "print(f\"\\nOVERFITTING:\")\n",
    "print(f\"  Train-Test DA gap: {train_test_da_gap:.2f}pp (target: <10pp)\")\n",
    "\n",
    "test_m = metrics_all['test']\n",
    "targets_met = [\n",
    "    test_m['direction_accuracy'] > 0.56,\n",
    "    primary_hcda_value > 0.60,\n",
    "    test_m['mae'] < 0.0170,\n",
    "    test_m['sharpe_ratio'] > 0.8,\n",
    "]\n",
    "\n",
    "print(f\"\\nTARGET STATUS:\")\n",
    "print(f\"  DA > 56%:     {'✓' if targets_met[0] else '✗'} ({test_m['direction_accuracy']*100:.2f}%)\")\n",
    "print(f\"  HCDA > 60%:   {'✓' if targets_met[1] else '✗'} ({primary_hcda_value*100:.2f}% via {primary_hcda_method})\")\n",
    "print(f\"  MAE < 1.70%:  {'✓' if targets_met[2] else '✗'} ({test_m['mae']:.4f}%)\")\n",
    "print(f\"  Sharpe > 0.8: {'✓' if targets_met[3] else '✗'} ({test_m['sharpe_ratio']:.2f})\")\n",
    "print(f\"\\nTargets passed: {sum(targets_met)}/4\")\n",
    "\n",
    "# === SUBSTANTIVE SKILL TESTS ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBSTANTIVE SKILL TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "naive_da_test_check = compute_naive_da(y_test)\n",
    "da_vs_naive = test_m['direction_accuracy'] - naive_da_test_check\n",
    "n_unique = len(np.unique(np.round(pred_test, 6)))\n",
    "n_pos_changes = np.sum(np.abs(np.diff(np.sign(pred_test))) > 0)\n",
    "positive_pct = (pred_test > 0).sum() / len(pred_test) * 100\n",
    "pred_std_test = np.std(pred_test)\n",
    "\n",
    "skill_tests = {\n",
    "    'da_above_naive': da_vs_naive > 0.005,         # > 0.5pp\n",
    "    'prediction_diversity': n_unique > 50,\n",
    "    'trade_activity': n_pos_changes > 10,\n",
    "    'prediction_balance': 30 < positive_pct < 90,\n",
    "    'prediction_variation': pred_std_test > 0.1,\n",
    "}\n",
    "\n",
    "for name, passed in skill_tests.items():\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "\n",
    "print(f\"\\n  DA vs naive: {da_vs_naive*100:+.2f}pp\")\n",
    "print(f\"  Unique predictions: {n_unique}\")\n",
    "print(f\"  Position changes in test: {n_pos_changes}\")\n",
    "print(f\"  Positive prediction %: {positive_pct:.1f}%\")\n",
    "print(f\"  Prediction std: {pred_std_test:.4f}\")\n",
    "print(f\"\\n  Substantive tests passed: {sum(skill_tests.values())}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Diagnostic Analysis (Weekly Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. HCDA at multiple thresholds\n",
    "print(\"\\nHCDA at different confidence thresholds (|prediction| method, test set):\")\n",
    "for pct in [70, 75, 80, 85, 90]:\n",
    "    hc_da, hc_cov = compute_hcda(y_test, pred_test, threshold_percentile=pct)\n",
    "    n_samples = int(len(y_test) * hc_cov)\n",
    "    print(f\"  Top {100-pct}% (N={n_samples}): {hc_da*100:.2f}%\")\n",
    "\n",
    "# 2. Weekly Rebalance Sharpe (Approach A)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEEKLY REBALANCE SHARPE (Approach A)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rebalance_indices = np.arange(0, len(dates_test), 5)\n",
    "positions_daily = np.zeros(len(dates_test))\n",
    "for i, idx in enumerate(rebalance_indices):\n",
    "    end_idx = min(idx + 5, len(positions_daily))\n",
    "    positions_daily[idx:end_idx] = np.sign(pred_test[idx])\n",
    "\n",
    "strategy_returns = positions_daily * daily_returns_test / 100.0\n",
    "position_changes = np.abs(np.diff(positions_daily, prepend=0))\n",
    "trade_costs = position_changes * (5.0 / 10000.0)\n",
    "net_returns = strategy_returns - trade_costs\n",
    "sharpe_approach_a = (net_returns.mean() / net_returns.std()) * np.sqrt(252) if net_returns.std() > 0 else 0\n",
    "n_trades_weekly = int(np.sum(position_changes > 0))\n",
    "\n",
    "print(f\"\\nApproach A (daily returns, weekly rebalancing):\")\n",
    "print(f\"  Sharpe ratio: {sharpe_approach_a:.2f}\")\n",
    "print(f\"  Trades in test: {n_trades_weekly}\")\n",
    "print(f\"  Annualized trades: ~{n_trades_weekly * 252 / len(dates_test):.0f}\")\n",
    "\n",
    "# 3. Non-overlapping evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NON-OVERLAPPING EVALUATION (every 5th day)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "non_overlap_idx = np.arange(0, len(test_df), 5)\n",
    "y_no = y_test[non_overlap_idx]\n",
    "pred_no = pred_test[non_overlap_idx]\n",
    "da_non_overlap = compute_direction_accuracy(y_no, pred_no)\n",
    "mae_non_overlap = compute_mae(y_no, pred_no)\n",
    "sharpe_non_overlap = compute_sharpe_weekly_simple(y_no, pred_no)\n",
    "\n",
    "print(f\"\\nNon-overlapping metrics:\")\n",
    "print(f\"  Samples: {len(non_overlap_idx)}\")\n",
    "print(f\"  DA:      {da_non_overlap*100:.2f}%\")\n",
    "print(f\"  MAE:     {mae_non_overlap:.4f}%\")\n",
    "print(f\"  Sharpe (Approach B): {sharpe_non_overlap:.2f}\")\n",
    "\n",
    "# 4. Feature importance\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'feature': FEATURE_COLUMNS,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE (XGBoost Gain)\")\n",
    "print(\"=\"*60)\n",
    "for i, row in feature_ranking.head(10).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# 5. Prediction distribution\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION DISTRIBUTION (test set, raw)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Mean:     {pred_test.mean():.4f}%\")\n",
    "print(f\"  Std:      {pred_test.std():.4f}%\")\n",
    "print(f\"  Min:      {pred_test.min():.4f}%\")\n",
    "print(f\"  Max:      {pred_test.max():.4f}%\")\n",
    "print(f\"  Positive: {(pred_test > 0).sum() / len(pred_test) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. predictions.csv\n",
    "split_labels = ['train'] * len(dates_train) + ['val'] * len(dates_val) + ['test'] * len(dates_test)\n",
    "predictions_df = pd.DataFrame({\n",
    "    'date': dates_full,\n",
    "    'split': split_labels,\n",
    "    'actual': y_full,\n",
    "    'prediction_raw': pred_full,\n",
    "    'prediction_scaled': scaled_pred_full,\n",
    "    'direction_correct': (np.sign(pred_full) == np.sign(y_full)).astype(int),\n",
    "    'abs_prediction': np.abs(pred_full),\n",
    "})\n",
    "\n",
    "threshold_80_pred = np.percentile(np.abs(pred_full), 80)\n",
    "predictions_df['high_confidence_pred'] = (predictions_df['abs_prediction'] > threshold_80_pred).astype(int)\n",
    "\n",
    "bootstrap_conf_full = np.concatenate([bootstrap_conf_train, bootstrap_conf_val, bootstrap_conf_test])\n",
    "threshold_80_bootstrap = np.percentile(bootstrap_conf_full, 80)\n",
    "predictions_df['bootstrap_confidence'] = bootstrap_conf_full\n",
    "predictions_df['high_confidence_bootstrap'] = (predictions_df['bootstrap_confidence'] > threshold_80_bootstrap).astype(int)\n",
    "\n",
    "bootstrap_std_full = np.concatenate([bootstrap_std_train, bootstrap_std_val, bootstrap_std_test])\n",
    "predictions_df['bootstrap_std'] = bootstrap_std_full\n",
    "\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"✓ Saved predictions.csv\")\n",
    "\n",
    "# 2. test_predictions.csv\n",
    "test_predictions_df = predictions_df[predictions_df['split'] == 'test'].copy()\n",
    "test_predictions_df.to_csv('test_predictions.csv', index=False)\n",
    "print(\"✓ Saved test_predictions.csv\")\n",
    "\n",
    "# 3. submodel_output.csv\n",
    "predictions_df.to_csv('submodel_output.csv', index=False)\n",
    "print(\"✓ Saved submodel_output.csv\")\n",
    "\n",
    "# 4. model.json\n",
    "final_model.save_model('model.json')\n",
    "print(\"✓ Saved model.json\")\n",
    "\n",
    "# 5. training_result.json\n",
    "training_result = {\n",
    "    'feature': 'meta_model_weekly',\n",
    "    'attempt': 2,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'architecture': 'XGBoost reg:squarederror + Bootstrap + OLS (weekly target, non-overlapping training)',\n",
    "    'phase': '3_meta_model',\n",
    "    'target_type': 'gold_return_5d',\n",
    "    'target_description': 'Forward 5-day gold return (%)',\n",
    "    \n",
    "    'design_changes': {\n",
    "        'non_overlapping_training': True,\n",
    "        'target_centering': True,\n",
    "        'train_mean_5d': float(train_mean_5d),\n",
    "        'naive_aware_objective': True,\n",
    "        'trade_activity_gate': True,\n",
    "        'constant_output_penalty': True,\n",
    "        'relaxed_regularization': True,\n",
    "        'non_overlapping_train_samples': len(X_train),\n",
    "        'non_overlapping_val_samples': len(X_val),\n",
    "    },\n",
    "    \n",
    "    'model_config': {\n",
    "        'n_features': 24,\n",
    "        'train_samples': len(X_train),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'samples_per_feature_ratio': len(X_train) / 24,\n",
    "        'selected_configuration': selected_config,\n",
    "        'optuna_trials_completed': len(study.trials),\n",
    "        'best_params': selected_params,\n",
    "    },\n",
    "    \n",
    "    'optuna_search': {\n",
    "        'n_trials': len(study.trials),\n",
    "        'best_value': float(study.best_value),\n",
    "        'best_trial_number': study.best_trial.number,\n",
    "        'top_5_trials': [\n",
    "            {\n",
    "                'number': t.number,\n",
    "                'value': float(t.value),\n",
    "                'params': t.params,\n",
    "                'val_da': float(t.user_attrs['val_da']),\n",
    "                'val_hc_da': float(t.user_attrs['val_hc_da']),\n",
    "                'da_skill_pp': float(t.user_attrs['da_skill_pp']),\n",
    "            }\n",
    "            for t in sorted(study.trials, key=lambda x: x.value, reverse=True)[:5]\n",
    "        ],\n",
    "    },\n",
    "    \n",
    "    'bootstrap_analysis': {\n",
    "        'bootstrap_ensemble_size': 5,\n",
    "        'bootstrap_seeds': bootstrap_seeds,\n",
    "        'bootstrap_std_range_test': [float(bootstrap_std_test.min()), float(bootstrap_std_test.max())],\n",
    "        'bootstrap_std_mean_test': float(bootstrap_std_test.mean()),\n",
    "        'bootstrap_conf_range_test': [float(bootstrap_conf_test.min()), float(bootstrap_conf_test.max())],\n",
    "        'bootstrap_conf_mean_test': float(bootstrap_conf_test.mean()),\n",
    "        'hcda_bootstrap': float(hcda_bootstrap_test),\n",
    "        'hcda_pred': float(hcda_pred_test),\n",
    "        'hcda_improvement': float(hcda_bootstrap_test - hcda_pred_test),\n",
    "    },\n",
    "    \n",
    "    'ols_scaling': {\n",
    "        'alpha_ols': float(alpha_ols),\n",
    "        'mae_raw': float(mae_raw),\n",
    "        'mae_scaled': float(mae_scaled),\n",
    "        'mae_improvement': float(mae_raw - mae_scaled),\n",
    "    },\n",
    "    \n",
    "    'primary_hcda_method': primary_hcda_method,\n",
    "    'primary_hcda_value': float(primary_hcda_value),\n",
    "    'primary_mae': float(min(mae_raw, mae_scaled)),\n",
    "    \n",
    "    'metrics': metrics_all,\n",
    "    \n",
    "    'substantive_skill_tests': skill_tests,\n",
    "    \n",
    "    'naive_comparison': {\n",
    "        'naive_da_train': float(naive_da_train),\n",
    "        'naive_da_val': float(naive_da_val),\n",
    "        'naive_da_test': float(naive_da_test_check),\n",
    "        'model_da_test': float(test_m['direction_accuracy']),\n",
    "        'da_skill_pp': float(da_vs_naive * 100),\n",
    "    },\n",
    "    \n",
    "    'weekly_evaluation': {\n",
    "        'sharpe_approach_a': float(sharpe_approach_a),\n",
    "        'sharpe_approach_b': float(sharpe_non_overlap),\n",
    "        'non_overlapping_metrics': {\n",
    "            'n_samples': len(non_overlap_idx),\n",
    "            'da': float(da_non_overlap),\n",
    "            'mae': float(mae_non_overlap),\n",
    "        },\n",
    "        'overlapping_metrics': metrics_all['test'],\n",
    "        'trades_in_test': int(n_trades_weekly),\n",
    "        'annualized_trades': int(n_trades_weekly * 252 / len(dates_test)),\n",
    "    },\n",
    "    \n",
    "    'target_evaluation': {\n",
    "        'direction_accuracy': {\n",
    "            'target': '> 56.0%',\n",
    "            'actual': f\"{test_m['direction_accuracy']*100:.2f}%\",\n",
    "            'gap': f\"{(test_m['direction_accuracy'] - 0.56)*100:+.2f}pp\",\n",
    "            'passed': bool(targets_met[0]),\n",
    "        },\n",
    "        'high_confidence_da': {\n",
    "            'target': '> 60.0%',\n",
    "            'actual': f\"{primary_hcda_value*100:.2f}%\",\n",
    "            'gap': f\"{(primary_hcda_value - 0.60)*100:+.2f}pp\",\n",
    "            'passed': bool(targets_met[1]),\n",
    "            'method_used': primary_hcda_method,\n",
    "        },\n",
    "        'mae': {\n",
    "            'target': '< 1.70%',\n",
    "            'actual': f\"{test_m['mae']:.4f}%\",\n",
    "            'gap': f\"{(0.0170 - test_m['mae']):.4f}%\",\n",
    "            'passed': bool(targets_met[2]),\n",
    "        },\n",
    "        'sharpe_ratio': {\n",
    "            'target': '> 0.80',\n",
    "            'actual': f\"{sharpe_approach_a:.2f}\",\n",
    "            'gap': f\"{(sharpe_approach_a - 0.8):+.2f}\",\n",
    "            'passed': bool(targets_met[3]),\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    'targets_passed': sum(targets_met),\n",
    "    'targets_total': 4,\n",
    "    'overall_passed': all(targets_met),\n",
    "    \n",
    "    'overfitting_analysis': {\n",
    "        'train_test_da_gap_pp': float(train_test_da_gap),\n",
    "        'target_gap_pp': 10.0,\n",
    "        'overfitting_check': 'PASS' if train_test_da_gap < 10 else 'FAIL',\n",
    "    },\n",
    "    \n",
    "    'feature_importance': {\n",
    "        'top_10_xgb': feature_ranking.head(10).to_dict('records'),\n",
    "    },\n",
    "    \n",
    "    'prediction_characteristics': {\n",
    "        'mean_raw': float(pred_test.mean()),\n",
    "        'std_raw': float(pred_test.std()),\n",
    "        'min_raw': float(pred_test.min()),\n",
    "        'max_raw': float(pred_test.max()),\n",
    "        'positive_pct': float((pred_test > 0).sum() / len(pred_test) * 100),\n",
    "        'unique_predictions': int(n_unique),\n",
    "        'position_changes': int(n_pos_changes),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('training_result.json', 'w') as f:\n",
    "    json.dump(training_result, f, indent=2, default=str)\n",
    "print(\"✓ Saved training_result.json\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Finished: {datetime.now().isoformat()}\")\n",
    "print(f\"\\nFinal Status:\")\n",
    "print(f\"  Configuration: {selected_config.upper()}\")\n",
    "print(f\"  HCDA method: {primary_hcda_method.upper()}\")\n",
    "print(f\"  MAE method: {'SCALED' if use_scaled else 'RAW'}\")\n",
    "print(f\"  Targets passed: {sum(targets_met)}/4\")\n",
    "print(f\"  Substantive tests passed: {sum(skill_tests.values())}/5\")\n",
    "if all(targets_met) and all(skill_tests.values()):\n",
    "    print(f\"  ✓✓✓ ALL TARGETS AND SKILL TESTS MET ✓✓✓\")\n",
    "elif all(targets_met):\n",
    "    print(f\"  ✓ All formal targets met, but some skill tests failed\")\n",
    "    failed_skill = [t for t, m in skill_tests.items() if not m]\n",
    "    print(f\"  Failed skill tests: {failed_skill}\")\n",
    "else:\n",
    "    failed = [t for t, m in zip(['DA', 'HCDA', 'MAE', 'Sharpe'], targets_met) if not m]\n",
    "    print(f\"  Improvements needed on: {failed}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
