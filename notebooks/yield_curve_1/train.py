"""
Gold Prediction SubModel Training - Yield Curve Attempt 1
Self-contained: Data fetch -> Preprocessing -> HMM + Velocity + Curvature -> Optuna HPO -> Save results
Generated by builder_model agent
"""

# ============================================================
# 1. IMPORTS
# ============================================================
import subprocess
import sys

# Install hmmlearn (not pre-installed on Kaggle)
print("Installing hmmlearn...")
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'hmmlearn', '-q'])

import numpy as np
import pandas as pd
import yfinance as yf
import json
import os
from datetime import datetime
from sklearn.metrics import mutual_info_score
import optuna

np.random.seed(42)

# ============================================================
# 2. DATA FETCHING
# ============================================================
def fetch_data():
    """
    Fetch yield curve data from FRED and gold target from Yahoo Finance.
    Self-contained. No external file dependencies.

    Returns:
        tuple: (features_df, target_df, full_df)
    """
    # --- FRED API Setup ---
    try:
        from fredapi import Fred
    except ImportError:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'fredapi', '-q'])
        from fredapi import Fred

    # --- Fetch Yield Data from Yahoo Finance (no API key needed) ---
    print("Fetching Treasury yields from Yahoo Finance...")
    # Start from 2014-10-01 for warmup buffer
    start_date = '2014-10-01'

    import yfinance as yf

    # Fetch yields (Yahoo Finance provides yields as percentages)
    tnx = yf.download('^TNX', start=start_date, progress=False)  # 10-year
    irx_or_2y = yf.download('^IRX', start=start_date, progress=False)  # 13-week (proxy for short end)
    fvx = yf.download('^FVX', start=start_date, progress=False)  # 5-year

    # Extract close prices (handle MultiIndex)
    def extract_close(df):
        if isinstance(df.columns, pd.MultiIndex):
            return df['Close'].iloc[:, 0]
        else:
            return df['Close']

    # Yahoo Finance yields are in percentages already
    df = pd.DataFrame({
        'dgs10': extract_close(tnx).values / 100,  # Convert to decimal (e.g., 3.5% -> 0.035)
        'dgs2': extract_close(irx_or_2y).values / 100 * (365/91),  # Annualize 13-week rate as proxy
        'dgs5': extract_close(fvx).values / 100
    }, index=tnx.index)

    df.index.name = 'date'
    df = df.reset_index()
    df['date'] = pd.to_datetime(df['date'])

    print(f"Raw data fetched: {len(df)} rows from {df['date'].min()} to {df['date'].max()}")

    # --- Handle Missing Values ---
    # Drop rows where any series is NaN (inner join approach)
    initial_rows = len(df)
    df = df.dropna(subset=['dgs10', 'dgs2', 'dgs5'])
    print(f"After dropping NaN: {len(df)} rows (dropped {initial_rows - len(df)} rows)")

    # --- Compute Derived Features ---
    # 1. Daily changes
    df['dgs10_change'] = df['dgs10'].diff()
    df['dgs2_change'] = df['dgs2'].diff()

    # 2. Spread: DGS10 - DGS2
    df['spread'] = df['dgs10'] - df['dgs2']
    df['spread_change'] = df['spread'].diff()

    # 3. Curvature: DGS5 - 0.5*(DGS2 + DGS10)
    df['curvature_raw'] = df['dgs5'] - 0.5 * (df['dgs2'] + df['dgs10'])
    df['curvature_change'] = df['curvature_raw'].diff()

    # --- Forward-fill small gaps (max 3 days) ---
    # Only for change columns that now have NaN
    change_cols = ['dgs10_change', 'dgs2_change', 'spread_change', 'curvature_change']
    for col in change_cols:
        df[col] = df[col].ffill(limit=3)

    # Drop remaining NaN rows (first row after diff)
    df = df.dropna(subset=change_cols)
    df = df.set_index('date')

    print(f"After preprocessing: {len(df)} rows")
    print(f"Date range: {df.index.min()} to {df.index.max()}")

    # --- Fetch Gold Target ---
    print("\nFetching GLD for target variable...")
    gld = yf.download("GLD", start="2015-01-01", progress=False)

    # Extract close price (handle MultiIndex)
    if isinstance(gld.columns, pd.MultiIndex):
        gold_close = gld['Close'].iloc[:, 0]
    else:
        gold_close = gld['Close']

    target_df = pd.DataFrame({
        'gold_close': gold_close.values
    }, index=gld.index)

    # Compute next-day return (%)
    target_df['gold_return_next'] = target_df['gold_close'].pct_change().shift(-1) * 100

    # Drop last row (no target)
    target_df = target_df.dropna(subset=['gold_return_next'])

    print(f"Gold target: {len(target_df)} rows")
    print(f"Date range: {target_df.index.min()} to {target_df.index.max()}")

    # Align yield data to target (inner join)
    common_idx = df.index.intersection(target_df.index)
    df = df.loc[common_idx]
    target_df = target_df.loc[common_idx]

    print(f"\nAligned data: {len(df)} rows")

    return df, target_df

# ============================================================
# 3. FEATURE GENERATION FUNCTIONS
# ============================================================
def generate_regime_feature(dgs10_changes, dgs2_changes, n_components, covariance_type,
                            train_size, gold_returns_train):
    """
    Fit 2D HMM on [DGS10_change, DGS2_change] and return P(bearish-for-gold state) for full data.
    Multi-restart: 5 seeds, select best log-likelihood.
    """
    from hmmlearn.hmm import GaussianHMM

    X_train = np.column_stack([dgs10_changes[:train_size], dgs2_changes[:train_size]])
    X_full = np.column_stack([dgs10_changes, dgs2_changes])

    # Remove NaN rows from training data
    valid_train = ~np.any(np.isnan(X_train), axis=1)
    X_train_clean = X_train[valid_train]

    best_model = None
    best_score = -np.inf
    seeds = [0, 42, 123, 456, 789]

    for seed in seeds:
        try:
            model = GaussianHMM(
                n_components=n_components,
                covariance_type=covariance_type,
                n_iter=100,
                tol=1e-4,
                random_state=seed
            )
            model.fit(X_train_clean)
            score = model.score(X_train_clean)
            if score > best_score:
                best_score = score
                best_model = model
        except Exception:
            continue

    if best_model is None:
        return np.full(len(dgs10_changes), 0.5)

    # Handle NaN in full data for predict_proba
    valid_full = ~np.any(np.isnan(X_full), axis=1)
    probs = np.full((len(dgs10_changes), n_components), np.nan)
    if valid_full.sum() > 0:
        probs[valid_full] = best_model.predict_proba(X_full[valid_full])

    # Identify state most negatively correlated with gold returns
    # Compute mean gold return per state on training set
    states_train = best_model.predict(X_train_clean)
    gold_train_clean = gold_returns_train[valid_train[:len(gold_returns_train)]]

    state_gold_means = []
    for s in range(n_components):
        mask = states_train == s
        if mask.sum() > 0:
            state_gold_means.append(np.nanmean(gold_train_clean[mask[:len(gold_train_clean)]]))
        else:
            state_gold_means.append(0.0)

    target_state = np.argmin(state_gold_means)  # most bearish for gold

    # Forward-fill NaN values
    result = probs[:, target_state]
    result_series = pd.Series(result)
    result_series = result_series.ffill()

    return result_series.values

def generate_velocity_feature(spread, change_window, zscore_window):
    """
    Z-score of N-day spread change.
    """
    spread_change = spread.diff(change_window)
    rolling_mean = spread_change.rolling(zscore_window).mean()
    rolling_std = spread_change.rolling(zscore_window).std()
    z = (spread_change - rolling_mean) / rolling_std
    z = z.clip(-4, 4)
    z = z.ffill()  # Forward-fill warmup NaN
    return z

def generate_curvature_feature(curvature_raw, zscore_window):
    """
    Z-score of daily curvature change.
    curvature_raw = DGS5 - 0.5*(DGS2 + DGS10)
    """
    curvature_change = curvature_raw.diff()
    rolling_mean = curvature_change.rolling(zscore_window).mean()
    rolling_std = curvature_change.rolling(zscore_window).std()
    z = (curvature_change - rolling_mean) / rolling_std
    z = z.clip(-4, 4)
    z = z.ffill()  # Forward-fill warmup NaN
    return z

# ============================================================
# 4. OPTUNA OBJECTIVE
# ============================================================
def create_objective(data_df, target_df):
    """
    Create Optuna objective function to maximize MI sum on validation set.
    """
    # Data split (70/15/15, time-series order)
    n = len(data_df)
    train_end = int(n * 0.70)
    val_end = int(n * 0.85)

    def objective(trial):
        # Hyperparameters
        hmm_n_components = trial.suggest_categorical('hmm_n_components', [2, 3, 4])
        hmm_covariance_type = trial.suggest_categorical('hmm_covariance_type', ['full', 'diag'])
        velocity_change_window = trial.suggest_categorical('velocity_change_window', [3, 5, 10])
        velocity_zscore_window = trial.suggest_categorical('velocity_zscore_window', [30, 60, 90, 120])
        curvature_zscore_window = trial.suggest_categorical('curvature_zscore_window', [30, 60, 90, 120])

        # Generate features
        try:
            regime_prob = generate_regime_feature(
                data_df['dgs10_change'].values,
                data_df['dgs2_change'].values,
                hmm_n_components,
                hmm_covariance_type,
                train_end,
                target_df['gold_return_next'].values[:train_end]
            )

            velocity_z = generate_velocity_feature(
                data_df['spread'],
                velocity_change_window,
                velocity_zscore_window
            )

            curvature_z = generate_curvature_feature(
                data_df['curvature_raw'],
                curvature_zscore_window
            )

            # Create feature DataFrame
            features = pd.DataFrame({
                'yc_regime_prob': regime_prob,
                'yc_spread_velocity_z': velocity_z.values,
                'yc_curvature_z': curvature_z.values
            }, index=data_df.index)

            # Validation set
            val_X = features.iloc[train_end:val_end]
            val_y = target_df['gold_return_next'].iloc[train_end:val_end]

            # Compute MI for each feature
            def discretize(x, bins=20):
                valid = ~np.isnan(x)
                if valid.sum() < bins:
                    return None
                x_clean = x.copy()
                x_clean[~valid] = np.nanmedian(x)
                try:
                    return pd.qcut(x_clean, bins, labels=False, duplicates='drop')
                except Exception:
                    return None

            mi_sum = 0.0
            for col in features.columns:
                feat_val = val_X[col].values
                tgt_val = val_y.values

                mask = ~np.isnan(feat_val) & ~np.isnan(tgt_val)
                if mask.sum() < 50:
                    continue

                feat_disc = discretize(feat_val[mask])
                tgt_disc = discretize(tgt_val[mask])

                if feat_disc is not None and tgt_disc is not None:
                    try:
                        mi = mutual_info_score(feat_disc, tgt_disc)
                        mi_sum += mi
                    except Exception:
                        pass

            return mi_sum

        except Exception as e:
            print(f"Trial failed: {e}")
            return 0.0

    return objective

# ============================================================
# 5. MAIN
# ============================================================
if __name__ == "__main__":
    print(f"=== Gold SubModel Training: yield_curve attempt 1 ===")
    print(f"Started: {datetime.now().isoformat()}")

    # Fetch data
    data_df, target_df = fetch_data()
    print(f"\nData: {len(data_df)} aligned rows")

    # Data split
    n = len(data_df)
    train_end = int(n * 0.70)
    val_end = int(n * 0.85)

    print(f"Train: {train_end} rows")
    print(f"Val: {val_end - train_end} rows")
    print(f"Test: {n - val_end} rows")

    # Run Optuna HPO
    print("\nRunning Optuna HPO...")
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42)
    )

    objective_fn = create_objective(data_df, target_df)
    study.optimize(objective_fn, n_trials=30, timeout=600)

    print(f"\nOptuna complete:")
    print(f"  Best value (MI sum): {study.best_value:.4f}")
    print(f"  Best params: {study.best_params}")
    print(f"  Completed trials: {len(study.trials)}")

    # Generate final output with best params
    print("\nGenerating final submodel output...")
    best_params = study.best_params

    regime_prob = generate_regime_feature(
        data_df['dgs10_change'].values,
        data_df['dgs2_change'].values,
        best_params['hmm_n_components'],
        best_params['hmm_covariance_type'],
        train_end,
        target_df['gold_return_next'].values[:train_end]
    )

    velocity_z = generate_velocity_feature(
        data_df['spread'],
        best_params['velocity_change_window'],
        best_params['velocity_zscore_window']
    )

    curvature_z = generate_curvature_feature(
        data_df['curvature_raw'],
        best_params['curvature_zscore_window']
    )

    output = pd.DataFrame({
        'yc_regime_prob': regime_prob,
        'yc_spread_velocity_z': velocity_z.values,
        'yc_curvature_z': curvature_z.values
    }, index=data_df.index)

    print(f"\nOutput shape: {output.shape}")
    print(f"Output columns: {list(output.columns)}")
    print(f"Date range: {output.index.min()} to {output.index.max()}")
    print(f"\nOutput summary:")
    print(output.describe())

    # Compute final metrics
    print("\nComputing final metrics...")

    # Autocorrelation
    autocorr_results = {}
    train_output = output.iloc[:train_end]
    for col in output.columns:
        vals = train_output[col].dropna().values
        if len(vals) > 1:
            autocorr = np.corrcoef(vals[:-1], vals[1:])[0, 1]
            autocorr_results[col] = autocorr
        else:
            autocorr_results[col] = 0.0

    print(f"\nAutocorrelation (lag 1, training set):")
    for col, ac in autocorr_results.items():
        print(f"  {col}: {ac:.4f}")

    # Mutual Information on validation set
    val_X = output.iloc[train_end:val_end]
    val_y = target_df['gold_return_next'].iloc[train_end:val_end]

    def discretize(x, bins=20):
        valid = ~np.isnan(x)
        if valid.sum() < bins:
            return None
        x_clean = x.copy()
        x_clean[~valid] = np.nanmedian(x)
        try:
            return pd.qcut(x_clean, bins, labels=False, duplicates='drop')
        except Exception:
            return None

    mi_results = {}
    for col in output.columns:
        feat_val = val_X[col].values
        tgt_val = val_y.values

        mask = ~np.isnan(feat_val) & ~np.isnan(tgt_val)
        if mask.sum() < 50:
            mi_results[col] = 0.0
            continue

        feat_disc = discretize(feat_val[mask])
        tgt_disc = discretize(tgt_val[mask])

        if feat_disc is None or tgt_disc is None:
            mi_results[col] = 0.0
        else:
            try:
                mi = mutual_info_score(feat_disc, tgt_disc)
                mi_results[col] = mi
            except Exception:
                mi_results[col] = 0.0

    mi_sum = sum(mi_results.values())

    print(f"\nMutual Information (validation set):")
    for col, mi in mi_results.items():
        print(f"  {col}: {mi:.4f}")
    print(f"  MI Sum: {mi_sum:.4f}")

    # === SAVE RESULTS (Kaggle output directory) ===
    print("\nSaving results...")

    # Reset index to include Date column
    output_with_date = output.reset_index()
    output_with_date.rename(columns={'date': 'Date'}, inplace=True)
    output_with_date.to_csv("submodel_output.csv", index=False)

    result = {
        "feature": "yield_curve",
        "attempt": 1,
        "timestamp": datetime.now().isoformat(),
        "best_params": best_params,
        "metrics": {
            "mi_individual": mi_results,
            "mi_sum": mi_sum,
            "autocorr": autocorr_results,
            "optuna_best_value": study.best_value,
            "optuna_trials_completed": len(study.trials)
        },
        "output_shape": list(output.shape),
        "output_columns": list(output.columns),
        "data_info": {
            "total_samples": len(data_df),
            "train_samples": train_end,
            "val_samples": val_end - train_end,
            "test_samples": n - val_end,
            "date_range_start": str(output.index.min()),
            "date_range_end": str(output.index.max())
        }
    }

    with open("training_result.json", "w") as f:
        json.dump(result, f, indent=2, default=str)

    print(f"\n=== Training complete! ===")
    print(f"Finished: {datetime.now().isoformat()}")
    print(f"Files saved:")
    print(f"  - submodel_output.csv")
    print(f"  - training_result.json")
