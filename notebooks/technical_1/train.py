"""
Gold Prediction SubModel Training - Technical Attempt 1
Self-contained: Data fetch -> HMM + Z-Score + GK Vol -> Optuna HPO -> Save results
Generated by builder_model agent
"""

# ============================================================
# 1. INSTALL DEPENDENCIES
# ============================================================
import subprocess
import sys

print("Installing hmmlearn...")
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'hmmlearn', '-q'])

# ============================================================
# 2. IMPORTS
# ============================================================
import numpy as np
import pandas as pd
import yfinance as yf
from hmmlearn.hmm import GaussianHMM
from sklearn.metrics import mutual_info_score
import optuna
import json
import os
from datetime import datetime

np.random.seed(42)

print("=" * 60)
print("Gold Technical SubModel Training - Attempt 1")
print("=" * 60)

# ============================================================
# 3. DATA FETCHING
# ============================================================
def fetch_data():
    """
    Fetch GLD OHLC data and compute Gold target (next-day return).
    Self-contained for Kaggle execution.

    Returns:
        tuple: (train_df, val_df, test_df, full_df)
    """
    print("\n=== FETCHING GLD DATA ===")

    # Fetch GLD data with buffer for warmup period
    # Need at least 60 trading days before 2015-01-30 for GK vol z-score baseline
    today = datetime.now().strftime("%Y-%m-%d")
    ticker = yf.Ticker("GLD")
    df = ticker.history(start="2014-10-01", end=today, auto_adjust=False)

    if df.empty:
        raise ValueError("Failed to fetch GLD data from Yahoo Finance")

    print(f"Fetched {len(df)} rows from Yahoo Finance")

    # Rename columns to lowercase
    df.columns = [col.lower() for col in df.columns]

    # Keep only required columns
    df = df[['open', 'high', 'low', 'close', 'volume']].copy()

    # Reset index to make date a column
    df.reset_index(inplace=True)
    df.rename(columns={'Date': 'date'}, inplace=True)

    # Ensure date is datetime
    df['date'] = pd.to_datetime(df['date'])

    # Filter to historical data only (exclude future dates)
    today_ts = pd.Timestamp.now(tz='UTC').normalize()
    df = df[df['date'] <= today_ts].copy()

    # Sort by date
    df.sort_values('date', inplace=True)
    df.reset_index(drop=True, inplace=True)

    print(f"Date range: {df['date'].min()} to {df['date'].max()}")

    # Check for flat bars (H==L==O==C) - should be 0 for GLD
    flat_bars = ((df['high'] == df['low']) &
                 (df['low'] == df['open']) &
                 (df['open'] == df['close']))
    n_flat = flat_bars.sum()
    if n_flat > 0:
        print(f"WARNING: Found {n_flat} flat bars (H==L==O==C)")

    # Compute returns
    df['returns'] = df['close'].pct_change()

    # Compute Garman-Klass volatility
    # GK formula: sqrt(0.5 * (ln(H/L))^2 - (2*ln(2)-1) * (ln(C/O))^2)
    high_safe = df['high'].clip(lower=1e-8)
    low_safe = df['low'].clip(lower=1e-8)
    close_safe = df['close'].clip(lower=1e-8)
    open_safe = df['open'].clip(lower=1e-8)

    log_hl = np.log(high_safe / low_safe)
    log_co = np.log(close_safe / open_safe)

    df['gk_vol'] = np.sqrt(
        0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)
    )

    # Clip GK vol to avoid zero values
    df['gk_vol'] = df['gk_vol'].clip(lower=1e-8)

    # Check for zero GK volatility days
    zero_gk = (df['gk_vol'] == 0).sum()
    if zero_gk > 0:
        print(f"WARNING: Found {zero_gk} days with GK vol = 0")

    # Handle missing values
    df['returns'] = df['returns'].ffill(limit=3)
    df['gk_vol'] = df['gk_vol'].ffill(limit=3)

    # Compute Gold target (next-day return)
    df['gold_return_next'] = df['close'].pct_change().shift(-1)

    # Basic statistics
    print(f"Total rows: {len(df)}")
    print(f"Returns range: [{df['returns'].min():.4f}, {df['returns'].max():.4f}]")
    print(f"GK vol range: [{df['gk_vol'].min():.6f}, {df['gk_vol'].max():.6f}]")

    # Split data into train/val/test (70/15/15, time-series order)
    n = len(df)
    train_end = int(n * 0.70)
    val_end = int(n * 0.85)

    train_df = df.iloc[:train_end].copy()
    val_df = df.iloc[train_end:val_end].copy()
    test_df = df.iloc[val_end:].copy()

    print(f"\nTrain: {len(train_df)} rows ({train_df['date'].min()} to {train_df['date'].max()})")
    print(f"Val:   {len(val_df)} rows ({val_df['date'].min()} to {val_df['date'].max()})")
    print(f"Test:  {len(test_df)} rows ({test_df['date'].min()} to {test_df['date'].max()})")

    return train_df, val_df, test_df, df


# ============================================================
# 4. FEATURE GENERATION FUNCTIONS
# ============================================================
def generate_regime_feature(returns, gk_vol, n_components, n_restarts, train_size):
    """
    Fit HMM on 2D [returns, GK_vol] on training set.
    Return P(highest-covariance-trace state) for full data.

    IMPORTANT: hmmlearn does NOT have n_init parameter.
    Must loop over random_state values manually.
    """
    X = np.column_stack([returns, gk_vol])
    X_train = X[:train_size]

    # Handle NaN values in training data
    train_mask = ~np.isnan(X_train).any(axis=1)
    if train_mask.sum() < 100:
        # Not enough valid data
        return np.full(len(returns), 0.5)

    X_train_clean = X_train[train_mask]

    best_model = None
    best_score = -np.inf

    for seed in range(n_restarts):
        try:
            model = GaussianHMM(
                n_components=n_components,
                covariance_type='full',
                n_iter=200,
                tol=1e-4,
                random_state=seed
            )
            model.fit(X_train_clean)
            score = model.score(X_train_clean)

            if score > best_score:
                best_score = score
                best_model = model
        except Exception as e:
            continue

    if best_model is None:
        return np.full(len(returns), 0.5)

    # Generate probabilities for full dataset
    # Handle NaN by replacing with median
    X_full = X.copy()
    for i in range(X_full.shape[1]):
        col_median = np.nanmedian(X_full[:, i])
        X_full[np.isnan(X_full[:, i]), i] = col_median

    probs = best_model.predict_proba(X_full)

    # Identify highest-covariance-trace state (highest total variance)
    traces = []
    for i in range(n_components):
        traces.append(np.trace(best_model.covars_[i]))
    high_var_state = np.argmax(traces)

    return probs[:, high_var_state]


def generate_zscore_feature(returns, window):
    """
    Rolling z-score of returns: (return - rolling_mean) / rolling_std
    """
    s = pd.Series(returns)
    rolling_mean = s.rolling(window, min_periods=window).mean()
    rolling_std = s.rolling(window, min_periods=window).std()
    z = (s - rolling_mean) / rolling_std
    z = z.clip(-4, 4)
    return z.values


def generate_gk_vol_zscore(high, low, open_, close, baseline_window):
    """
    Daily Garman-Klass volatility z-scored against baseline window.

    IMPORTANT: Use daily GK vol directly, NOT smoothed with rolling(20).mean().
    Smoothed version has autocorr = 0.979 (Gate 1 risk).
    Daily version has autocorr = 0.206 (safe).
    """
    # Clip to avoid log(0)
    high_safe = np.clip(high, 1e-8, None)
    low_safe = np.clip(low, 1e-8, None)
    close_safe = np.clip(close, 1e-8, None)
    open_safe = np.clip(open_, 1e-8, None)

    log_hl = np.log(high_safe / low_safe)
    log_co = np.log(close_safe / open_safe)

    gk_vol = np.sqrt(
        0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)
    )
    gk_vol = np.clip(gk_vol, 1e-8, None)

    s = pd.Series(gk_vol)
    gk_mean = s.rolling(baseline_window, min_periods=baseline_window).mean()
    gk_std = s.rolling(baseline_window, min_periods=baseline_window).std()
    z = (s - gk_mean) / gk_std
    z = z.clip(-4, 4)
    return z.values


# ============================================================
# 5. OPTUNA OBJECTIVE
# ============================================================
def objective(trial, df_full, train_size, val_start, val_end):
    """Maximize MI sum on validation set"""
    n_components = trial.suggest_categorical('hmm_n_components', [2, 3])
    n_restarts = trial.suggest_categorical('hmm_n_restarts', [5, 10])
    zscore_window = trial.suggest_categorical('zscore_window', [15, 20, 30])
    gk_baseline_window = trial.suggest_categorical('gk_baseline_window', [40, 60, 90])

    try:
        # Extract arrays
        returns = df_full['returns'].values
        gk_vol = df_full['gk_vol'].values
        high = df_full['high'].values
        low = df_full['low'].values
        open_ = df_full['open'].values
        close = df_full['close'].values
        target = df_full['gold_return_next'].values

        # Generate features
        regime = generate_regime_feature(
            returns, gk_vol, n_components, n_restarts, train_size
        )
        zscore = generate_zscore_feature(returns, zscore_window)
        vol_z = generate_gk_vol_zscore(high, low, open_, close, gk_baseline_window)

        # Extract validation period
        regime_val = regime[val_start:val_end]
        zscore_val = zscore[val_start:val_end]
        vol_z_val = vol_z[val_start:val_end]
        target_val = target[val_start:val_end]

        # Compute MI (discretize continuous variables)
        def discretize(x, bins=20):
            valid = ~np.isnan(x)
            if valid.sum() < bins:
                return None
            x_valid = x.copy()
            x_valid[~valid] = np.nanmedian(x)
            try:
                return pd.qcut(x_valid, bins, labels=False, duplicates='drop')
            except:
                return None

        mi_sum = 0.0
        target_disc = discretize(target_val)
        if target_disc is None:
            return 0.0

        for feat_val in [regime_val, zscore_val, vol_z_val]:
            mask = ~np.isnan(feat_val) & ~np.isnan(target_val)
            if mask.sum() > 50:
                feat_disc = discretize(feat_val[mask])
                tgt_disc = discretize(target_val[mask])
                if feat_disc is not None and tgt_disc is not None:
                    mi = mutual_info_score(feat_disc, tgt_disc)
                    mi_sum += mi

        return mi_sum

    except Exception as e:
        print(f"Trial failed: {e}")
        return 0.0


# ============================================================
# 6. MAIN EXECUTION
# ============================================================
if __name__ == "__main__":
    start_time = datetime.now()
    print(f"Started: {start_time.isoformat()}")

    # Fetch data
    train_df, val_df, test_df, full_df = fetch_data()

    train_size = len(train_df)
    val_start = train_size
    val_end = train_size + len(val_df)

    # Run Optuna HPO
    print("\n=== RUNNING OPTUNA HPO ===")
    print(f"n_trials: 30, timeout: 600s")

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42)
    )

    study.optimize(
        lambda trial: objective(trial, full_df, train_size, val_start, val_end),
        n_trials=30,
        timeout=600,
        show_progress_bar=True
    )

    print(f"\nBest params: {study.best_params}")
    print(f"Best value (MI sum): {study.best_value:.6f}")
    print(f"Completed trials: {len(study.trials)}")

    # Generate final output with best params
    print("\n=== GENERATING FINAL OUTPUT ===")
    best_params = study.best_params

    returns = full_df['returns'].values
    gk_vol = full_df['gk_vol'].values
    high = full_df['high'].values
    low = full_df['low'].values
    open_ = full_df['open'].values
    close = full_df['close'].values

    regime = generate_regime_feature(
        returns, gk_vol,
        best_params['hmm_n_components'],
        best_params['hmm_n_restarts'],
        train_size
    )
    zscore = generate_zscore_feature(returns, best_params['zscore_window'])
    vol_z = generate_gk_vol_zscore(
        high, low, open_, close,
        best_params['gk_baseline_window']
    )

    # Create output DataFrame
    output = pd.DataFrame({
        'date': full_df['date'],
        'tech_trend_regime_prob': regime,
        'tech_mean_reversion_z': zscore,
        'tech_volatility_regime': vol_z
    })

    # Forward-fill any remaining NaN from rolling windows
    output['tech_trend_regime_prob'] = output['tech_trend_regime_prob'].ffill()
    output['tech_mean_reversion_z'] = output['tech_mean_reversion_z'].ffill()
    output['tech_volatility_regime'] = output['tech_volatility_regime'].ffill()

    print(f"Output shape: {output.shape}")
    print(f"Output columns: {list(output.columns)}")
    print("\nOutput statistics:")
    print(output.describe())

    # Check for NaN
    nan_counts = output.isna().sum()
    if nan_counts.any():
        print(f"\nWARNING: NaN counts:\n{nan_counts[nan_counts > 0]}")

    # Save results
    print("\n=== SAVING RESULTS ===")
    output.to_csv("submodel_output.csv", index=False)
    print("Saved: submodel_output.csv")

    # Compute final metrics for logging
    target = full_df['gold_return_next'].values

    def compute_mi(feat, tgt, mask):
        """Helper to compute MI"""
        try:
            f = feat[mask]
            t = tgt[mask]
            valid = ~np.isnan(f) & ~np.isnan(t)
            if valid.sum() < 50:
                return 0.0
            f_disc = pd.qcut(f[valid], 20, labels=False, duplicates='drop')
            t_disc = pd.qcut(t[valid], 20, labels=False, duplicates='drop')
            return mutual_info_score(f_disc, t_disc)
        except:
            return 0.0

    val_mask = np.zeros(len(full_df), dtype=bool)
    val_mask[val_start:val_end] = True

    mi_regime = compute_mi(regime, target, val_mask)
    mi_zscore = compute_mi(zscore, target, val_mask)
    mi_vol_z = compute_mi(vol_z, target, val_mask)

    metrics = {
        'mi_regime': mi_regime,
        'mi_zscore': mi_zscore,
        'mi_vol_z': mi_vol_z,
        'mi_sum': mi_regime + mi_zscore + mi_vol_z,
        'optuna_best_value': study.best_value
    }

    result = {
        "feature": "technical",
        "attempt": 1,
        "timestamp": datetime.now().isoformat(),
        "best_params": best_params,
        "metrics": metrics,
        "optuna_trials_completed": len(study.trials),
        "optuna_best_value": study.best_value,
        "output_shape": list(output.shape),
        "output_columns": list(output.columns),
        "data_info": {
            "train_samples": len(train_df),
            "val_samples": len(val_df),
            "test_samples": len(test_df),
            "full_samples": len(full_df),
        }
    }

    with open("training_result.json", "w") as f:
        json.dump(result, f, indent=2, default=str)

    print("Saved: training_result.json")

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print("\n" + "=" * 60)
    print("TRAINING COMPLETE")
    print("=" * 60)
    print(f"Duration: {duration:.1f}s")
    print(f"Best MI sum: {study.best_value:.6f}")
    print(f"Output files: submodel_output.csv, training_result.json")
    print("=" * 60)
