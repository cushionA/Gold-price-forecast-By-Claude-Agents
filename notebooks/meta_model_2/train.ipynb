{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold Meta-Model Training - Attempt 3\n",
        "\n",
        "**Architecture**: 2-stage ensemble with confidence-based re-ranking\n",
        "\n",
        "**Key improvements from Attempt 2**:\n",
        "- 5-model ensemble with different random seeds [42, 137, 256, 389, 512]\n",
        "- Confidence-based re-ranking for HCDA (magnitude_rank + agreement_fraction)\n",
        "- Optuna objective reweighting: Sharpe 35%, DA 25%, MAE 15%, HCDA 25%\n",
        "- Relaxed regularization ranges\n",
        "\n",
        "**Data**: Reuse meta_model_attempt_2 datasets from bigbigzabuton/gold-prediction-complete\n",
        "\n",
        "**Target**: DA >56%, HCDA >60%, Sharpe >0.8, MAE <0.75%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1. IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for base reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"Training started: {datetime.now().isoformat()}\")\n",
        "print(f\"XGBoost version: {xgb.__version__}\")\n",
        "print(f\"Optuna version: {optuna.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2. DATA LOADING\n",
        "# ============================================================\n",
        "# Reuse Attempt 2 datasets from Kaggle dataset\n",
        "train_df = pd.read_csv('../input/gold-prediction-complete/meta_model_attempt_2_train.csv', index_col=0, parse_dates=True)\n",
        "val_df = pd.read_csv('../input/gold-prediction-complete/meta_model_attempt_2_val.csv', index_col=0, parse_dates=True)\n",
        "test_df = pd.read_csv('../input/gold-prediction-complete/meta_model_attempt_2_test.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "print(f\"Data loaded successfully\")\n",
        "print(f\"Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")\n",
        "print(f\"Date ranges:\")\n",
        "print(f\"  Train: {train_df.index.min()} to {train_df.index.max()}\")\n",
        "print(f\"  Val: {val_df.index.min()} to {val_df.index.max()}\")\n",
        "print(f\"  Test: {test_df.index.min()} to {test_df.index.max()}\")\n",
        "\n",
        "# Prepare X, y\n",
        "target_col = 'gold_return_next'\n",
        "feature_cols = [c for c in train_df.columns if c != target_col]\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df[target_col]\n",
        "X_val = val_df[feature_cols]\n",
        "y_val = val_df[target_col]\n",
        "X_test = test_df[feature_cols]\n",
        "y_test = test_df[target_col]\n",
        "\n",
        "print(f\"\\nFeatures ({len(feature_cols)}): {feature_cols[:5]}...\")\n",
        "print(f\"Target: {target_col}\")\n",
        "print(f\"\\nTarget statistics:\")\n",
        "print(f\"  Train: mean={y_train.mean():.4f}, std={y_train.std():.4f}\")\n",
        "print(f\"  Val: mean={y_val.mean():.4f}, std={y_val.std():.4f}\")\n",
        "print(f\"  Test: mean={y_test.mean():.4f}, std={y_test.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. EVALUATION METRICS\n",
        "# ============================================================\n",
        "def direction_accuracy(y_true, y_pred):\n",
        "    \"\"\"Direction accuracy, excluding exact zeros\"\"\"\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return 0.5\n",
        "    return (np.sign(y_true[mask]) == np.sign(y_pred[mask])).mean()\n",
        "\n",
        "def sharpe_ratio(y_true, y_pred, transaction_cost_bps=5.0):\n",
        "    \"\"\"Sharpe ratio with transaction costs\"\"\"\n",
        "    positions = np.sign(y_pred)\n",
        "    strategy_returns = positions * y_true\n",
        "    \n",
        "    # Transaction costs (one-way)\n",
        "    trades = np.abs(np.diff(positions, prepend=0)) > 0\n",
        "    costs = trades * (transaction_cost_bps / 10000.0)\n",
        "    net_returns = strategy_returns - costs\n",
        "    \n",
        "    if net_returns.std() == 0:\n",
        "        return 0.0\n",
        "    return net_returns.mean() / net_returns.std() * np.sqrt(252)\n",
        "\n",
        "def high_confidence_direction_accuracy(y_true, y_pred, top_pct=0.2):\n",
        "    \"\"\"Standard HCDA: top 20% by absolute prediction magnitude\"\"\"\n",
        "    abs_pred = np.abs(y_pred)\n",
        "    threshold = np.percentile(abs_pred, (1 - top_pct) * 100)\n",
        "    mask = (abs_pred >= threshold) & (y_true != 0)\n",
        "    \n",
        "    if mask.sum() == 0:\n",
        "        return 0.5\n",
        "    return (np.sign(y_true[mask]) == np.sign(y_pred[mask])).mean()\n",
        "\n",
        "def reranked_hcda(y_true, y_pred_ensemble, alpha=0.5, top_pct=0.2):\n",
        "    \"\"\"\n",
        "    Confidence-based re-ranking HCDA\n",
        "    \n",
        "    Args:\n",
        "        y_true: Ground truth\n",
        "        y_pred_ensemble: (n_samples, n_models) predictions from ensemble\n",
        "        alpha: Weight for magnitude_rank (1-alpha for agreement_fraction)\n",
        "        top_pct: Top percentage to use for HCDA\n",
        "    \"\"\"\n",
        "    # Ensemble mean prediction\n",
        "    y_pred_mean = y_pred_ensemble.mean(axis=1)\n",
        "    \n",
        "    # 1. Magnitude rank (normalized to [0, 1])\n",
        "    abs_pred = np.abs(y_pred_mean)\n",
        "    magnitude_rank = abs_pred / (abs_pred.max() + 1e-10)\n",
        "    \n",
        "    # 2. Agreement fraction\n",
        "    pred_signs = np.sign(y_pred_ensemble)\n",
        "    mean_sign = np.sign(y_pred_mean)\n",
        "    agreement = (pred_signs == mean_sign[:, None]).mean(axis=1)\n",
        "    \n",
        "    # 3. Combined confidence score\n",
        "    confidence_score = alpha * magnitude_rank + (1 - alpha) * agreement\n",
        "    \n",
        "    # 4. Select top 20% by confidence\n",
        "    threshold = np.percentile(confidence_score, (1 - top_pct) * 100)\n",
        "    mask = (confidence_score >= threshold) & (y_true != 0)\n",
        "    \n",
        "    if mask.sum() == 0:\n",
        "        return 0.5\n",
        "    \n",
        "    return (np.sign(y_true[mask]) == np.sign(y_pred_mean[mask])).mean()\n",
        "\n",
        "def compute_all_metrics(y_true, y_pred, y_pred_ensemble=None, alpha_confidence=0.5):\n",
        "    \"\"\"Compute all metrics including both standard and reranked HCDA\"\"\"\n",
        "    metrics = {\n",
        "        'mae': mean_absolute_error(y_true, y_pred),\n",
        "        'da': direction_accuracy(y_true, y_pred),\n",
        "        'sharpe': sharpe_ratio(y_true, y_pred),\n",
        "        'hcda': high_confidence_direction_accuracy(y_true, y_pred),\n",
        "    }\n",
        "    \n",
        "    if y_pred_ensemble is not None:\n",
        "        metrics['hcda_reranked'] = reranked_hcda(y_true, y_pred_ensemble, alpha=alpha_confidence)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"Evaluation metrics defined\")\n",
        "print(\"  - MAE (Mean Absolute Error)\")\n",
        "print(\"  - DA (Direction Accuracy, excluding zeros)\")\n",
        "print(\"  - Sharpe Ratio (with 5bps transaction costs)\")\n",
        "print(\"  - HCDA (High-Confidence DA, top 20% by magnitude)\")\n",
        "print(\"  - HCDA Reranked (top 20% by confidence score)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. OPTUNA OBJECTIVE\n",
        "# ============================================================\n",
        "ENSEMBLE_SEEDS = [42, 137, 256, 389, 512]\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective: Train 5-model ensemble with same HP, different seeds\n",
        "    Objective = 0.35*Sharpe + 0.25*DA + 0.15*(1-MAE/2) + 0.25*HCDA_reranked - overfitting_penalty\n",
        "    \"\"\"\n",
        "    # Hyperparameters (relaxed regularization ranges)\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 3, 15),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 2.0, 20.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 10.0, log=True),\n",
        "        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
        "        'objective': 'reg:squarederror',\n",
        "        'tree_method': 'hist',\n",
        "        'eval_metric': 'mae',\n",
        "    }\n",
        "    \n",
        "    # Alpha for confidence score\n",
        "    alpha_confidence = trial.suggest_float('alpha_confidence', 0.2, 0.8)\n",
        "    \n",
        "    # Train 5 models with different seeds\n",
        "    models = []\n",
        "    train_preds_list = []\n",
        "    val_preds_list = []\n",
        "    \n",
        "    for seed in ENSEMBLE_SEEDS:\n",
        "        params['random_state'] = seed\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False\n",
        "        )\n",
        "        models.append(model)\n",
        "        train_preds_list.append(model.predict(X_train))\n",
        "        val_preds_list.append(model.predict(X_val))\n",
        "    \n",
        "    # Ensemble predictions (n_samples, n_models)\n",
        "    train_preds_ensemble = np.column_stack(train_preds_list)\n",
        "    val_preds_ensemble = np.column_stack(val_preds_list)\n",
        "    \n",
        "    # Mean predictions\n",
        "    train_pred_mean = train_preds_ensemble.mean(axis=1)\n",
        "    val_pred_mean = val_preds_ensemble.mean(axis=1)\n",
        "    \n",
        "    # Train metrics\n",
        "    train_metrics = compute_all_metrics(y_train, train_pred_mean, train_preds_ensemble, alpha_confidence)\n",
        "    \n",
        "    # Validation metrics\n",
        "    val_metrics = compute_all_metrics(y_val, val_pred_mean, val_preds_ensemble, alpha_confidence)\n",
        "    \n",
        "    # Weighted objective with reranked HCDA\n",
        "    mae_norm = 1 - val_metrics['mae'] / 2.0  # Normalize MAE (assume max ~2%)\n",
        "    \n",
        "    objective_value = (\n",
        "        0.35 * val_metrics['sharpe'] +\n",
        "        0.25 * val_metrics['da'] +\n",
        "        0.15 * mae_norm +\n",
        "        0.25 * val_metrics['hcda_reranked']\n",
        "    )\n",
        "    \n",
        "    # Overfitting penalty (same as Attempt 2)\n",
        "    overfit_penalty = 0\n",
        "    if train_metrics['da'] - val_metrics['da'] > 0.10:\n",
        "        overfit_penalty += 0.1\n",
        "    if train_metrics['sharpe'] - val_metrics['sharpe'] > 0.3:\n",
        "        overfit_penalty += 0.1\n",
        "    if val_metrics['mae'] > train_metrics['mae'] * 1.3:\n",
        "        overfit_penalty += 0.05\n",
        "    \n",
        "    final_objective = objective_value - overfit_penalty\n",
        "    \n",
        "    # Store metrics for logging\n",
        "    trial.set_user_attr('train_mae', train_metrics['mae'])\n",
        "    trial.set_user_attr('val_mae', val_metrics['mae'])\n",
        "    trial.set_user_attr('train_da', train_metrics['da'])\n",
        "    trial.set_user_attr('val_da', val_metrics['da'])\n",
        "    trial.set_user_attr('train_sharpe', train_metrics['sharpe'])\n",
        "    trial.set_user_attr('val_sharpe', val_metrics['sharpe'])\n",
        "    trial.set_user_attr('val_hcda', val_metrics['hcda'])\n",
        "    trial.set_user_attr('val_hcda_reranked', val_metrics['hcda_reranked'])\n",
        "    trial.set_user_attr('overfit_penalty', overfit_penalty)\n",
        "    \n",
        "    return final_objective\n",
        "\n",
        "print(\"Optuna objective defined\")\n",
        "print(\"  - 5-model ensemble with seeds: [42, 137, 256, 389, 512]\")\n",
        "print(\"  - Objective: 0.35*Sharpe + 0.25*DA + 0.15*(1-MAE/2) + 0.25*HCDA_reranked - penalty\")\n",
        "print(\"  - Overfitting penalties: DA gap >0.10, Sharpe gap >0.3, MAE ratio >1.3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. HYPERPARAMETER OPTIMIZATION\n",
        "# ============================================================\n",
        "print(\"Starting Optuna hyperparameter optimization...\")\n",
        "print(f\"  - n_trials: 80\")\n",
        "print(f\"  - timeout: 3600 seconds (1 hour)\")\n",
        "print(f\"  - Pruning: MedianPruner with 5 warmup steps\\n\")\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=80, timeout=3600, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\nOptimization complete!\")\n",
        "print(f\"  - Total trials: {len(study.trials)}\")\n",
        "print(f\"  - Best objective value: {study.best_value:.4f}\")\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Best trial metrics\n",
        "best_trial = study.best_trial\n",
        "print(f\"\\nBest trial validation metrics:\")\n",
        "print(f\"  MAE: {best_trial.user_attrs['val_mae']:.4f}\")\n",
        "print(f\"  DA: {best_trial.user_attrs['val_da']:.4f}\")\n",
        "print(f\"  Sharpe: {best_trial.user_attrs['val_sharpe']:.4f}\")\n",
        "print(f\"  HCDA (standard): {best_trial.user_attrs['val_hcda']:.4f}\")\n",
        "print(f\"  HCDA (reranked): {best_trial.user_attrs['val_hcda_reranked']:.4f}\")\n",
        "print(f\"  Overfit penalty: {best_trial.user_attrs['overfit_penalty']:.4f}\")\n",
        "\n",
        "best_params = study.best_params.copy()\n",
        "alpha_confidence_final = best_params.pop('alpha_confidence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. TRAIN FINAL ENSEMBLE\n",
        "# ============================================================\n",
        "print(\"\\nTraining final 5-model ensemble with best hyperparameters...\")\n",
        "\n",
        "final_models = []\n",
        "final_params = {\n",
        "    **best_params,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'tree_method': 'hist',\n",
        "    'eval_metric': 'mae',\n",
        "}\n",
        "\n",
        "for i, seed in enumerate(ENSEMBLE_SEEDS, 1):\n",
        "    print(f\"  Training model {i}/5 (seed={seed})...\")\n",
        "    final_params['random_state'] = seed\n",
        "    model = xgb.XGBRegressor(**final_params)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "    final_models.append(model)\n",
        "    print(f\"    Best iteration: {model.best_iteration}\")\n",
        "\n",
        "print(f\"\\nFinal ensemble trained successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7. GENERATE ENSEMBLE PREDICTIONS\n",
        "# ============================================================\n",
        "print(\"Generating ensemble predictions...\")\n",
        "\n",
        "def get_ensemble_predictions(models, X):\n",
        "    \"\"\"Get predictions from all models in ensemble\"\"\"\n",
        "    preds = [model.predict(X) for model in models]\n",
        "    return np.column_stack(preds)\n",
        "\n",
        "# Get predictions on all splits\n",
        "train_preds_ensemble = get_ensemble_predictions(final_models, X_train)\n",
        "val_preds_ensemble = get_ensemble_predictions(final_models, X_val)\n",
        "test_preds_ensemble = get_ensemble_predictions(final_models, X_test)\n",
        "\n",
        "# Mean predictions\n",
        "train_pred = train_preds_ensemble.mean(axis=1)\n",
        "val_pred = val_preds_ensemble.mean(axis=1)\n",
        "test_pred = test_preds_ensemble.mean(axis=1)\n",
        "\n",
        "print(f\"  Train predictions: shape {train_preds_ensemble.shape}\")\n",
        "print(f\"  Val predictions: shape {val_preds_ensemble.shape}\")\n",
        "print(f\"  Test predictions: shape {test_preds_ensemble.shape}\")\n",
        "\n",
        "# Save predictions\n",
        "pd.DataFrame({\n",
        "    'actual': y_train,\n",
        "    'predicted': train_pred,\n",
        "}).to_csv('train_predictions.csv')\n",
        "\n",
        "pd.DataFrame({\n",
        "    'actual': y_val,\n",
        "    'predicted': val_pred,\n",
        "}).to_csv('val_predictions.csv')\n",
        "\n",
        "pd.DataFrame({\n",
        "    'actual': y_test,\n",
        "    'predicted': test_pred,\n",
        "}).to_csv('test_predictions.csv')\n",
        "\n",
        "print(\"\\nPrediction CSVs saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8. EVALUATE ON ALL SPLITS\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_metrics_final = compute_all_metrics(y_train, train_pred, train_preds_ensemble, alpha_confidence_final)\n",
        "val_metrics_final = compute_all_metrics(y_val, val_pred, val_preds_ensemble, alpha_confidence_final)\n",
        "test_metrics_final = compute_all_metrics(y_test, test_pred, test_preds_ensemble, alpha_confidence_final)\n",
        "\n",
        "print(\"\\nTRAIN SET:\")\n",
        "print(f\"  MAE: {train_metrics_final['mae']:.4f}\")\n",
        "print(f\"  Direction Accuracy: {train_metrics_final['da']:.4f}\")\n",
        "print(f\"  Sharpe Ratio: {train_metrics_final['sharpe']:.4f}\")\n",
        "print(f\"  HCDA (standard): {train_metrics_final['hcda']:.4f}\")\n",
        "print(f\"  HCDA (reranked, alpha={alpha_confidence_final:.2f}): {train_metrics_final['hcda_reranked']:.4f}\")\n",
        "\n",
        "print(\"\\nVALIDATION SET:\")\n",
        "print(f\"  MAE: {val_metrics_final['mae']:.4f}\")\n",
        "print(f\"  Direction Accuracy: {val_metrics_final['da']:.4f}\")\n",
        "print(f\"  Sharpe Ratio: {val_metrics_final['sharpe']:.4f}\")\n",
        "print(f\"  HCDA (standard): {val_metrics_final['hcda']:.4f}\")\n",
        "print(f\"  HCDA (reranked, alpha={alpha_confidence_final:.2f}): {val_metrics_final['hcda_reranked']:.4f}\")\n",
        "\n",
        "print(\"\\nTEST SET (HELD-OUT):\")\n",
        "print(f\"  MAE: {test_metrics_final['mae']:.4f}\")\n",
        "print(f\"  Direction Accuracy: {test_metrics_final['da']:.4f}\")\n",
        "print(f\"  Sharpe Ratio: {test_metrics_final['sharpe']:.4f}\")\n",
        "print(f\"  HCDA (standard): {test_metrics_final['hcda']:.4f}\")\n",
        "print(f\"  HCDA (reranked, alpha={alpha_confidence_final:.2f}): {test_metrics_final['hcda_reranked']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TARGET COMPARISON (Test Set):\")\n",
        "print(\"=\"*60)\n",
        "targets = {\n",
        "    'Direction Accuracy': (test_metrics_final['da'], 0.56, '✓' if test_metrics_final['da'] > 0.56 else '✗'),\n",
        "    'HCDA (reranked)': (test_metrics_final['hcda_reranked'], 0.60, '✓' if test_metrics_final['hcda_reranked'] > 0.60 else '✗'),\n",
        "    'MAE': (test_metrics_final['mae'], 0.75, '✓' if test_metrics_final['mae'] < 0.75 else '✗'),\n",
        "    'Sharpe Ratio': (test_metrics_final['sharpe'], 0.80, '✓' if test_metrics_final['sharpe'] > 0.80 else '✗'),\n",
        "}\n",
        "\n",
        "for metric_name, (value, target, status) in targets.items():\n",
        "    if metric_name == 'MAE':\n",
        "        print(f\"{status} {metric_name}: {value:.4f} (target: <{target})\")\n",
        "    else:\n",
        "        print(f\"{status} {metric_name}: {value:.4f} (target: >{target})\")\n",
        "\n",
        "# Overall pass/fail\n",
        "all_passed = all(status == '✓' for _, _, status in targets.values())\n",
        "print(f\"\\nOVERALL: {'PASS' if all_passed else 'FAIL'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9. FEATURE IMPORTANCE (from first model)\n",
        "# ============================================================\n",
        "print(\"\\nFeature Importance (from model with seed=42):\")\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': final_models[0].feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(feature_importance.to_string(index=False))\n",
        "\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)\n",
        "print(\"\\nFeature importance saved to feature_importance.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10. SAVE RESULTS\n",
        "# ============================================================\n",
        "print(\"\\nSaving final results...\")\n",
        "\n",
        "# Save all 5 models\n",
        "for i, model in enumerate(final_models):\n",
        "    model.save_model(f'model_seed_{ENSEMBLE_SEEDS[i]}.json')\n",
        "    print(f\"  Saved model_seed_{ENSEMBLE_SEEDS[i]}.json\")\n",
        "\n",
        "# Training result summary\n",
        "result = {\n",
        "    \"feature\": \"meta_model\",\n",
        "    \"attempt\": 3,\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"architecture\": \"5-model XGBoost ensemble with confidence-based re-ranking\",\n",
        "    \"ensemble_seeds\": ENSEMBLE_SEEDS,\n",
        "    \"alpha_confidence\": alpha_confidence_final,\n",
        "    \"best_params\": best_params,\n",
        "    \"optuna\": {\n",
        "        \"n_trials_completed\": len(study.trials),\n",
        "        \"best_objective_value\": study.best_value,\n",
        "        \"optimization_time\": sum(t.duration.total_seconds() for t in study.trials if t.duration),\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"train\": {\n",
        "            \"mae\": train_metrics_final['mae'],\n",
        "            \"direction_accuracy\": train_metrics_final['da'],\n",
        "            \"sharpe_ratio\": train_metrics_final['sharpe'],\n",
        "            \"hcda_standard\": train_metrics_final['hcda'],\n",
        "            \"hcda_reranked\": train_metrics_final['hcda_reranked'],\n",
        "        },\n",
        "        \"val\": {\n",
        "            \"mae\": val_metrics_final['mae'],\n",
        "            \"direction_accuracy\": val_metrics_final['da'],\n",
        "            \"sharpe_ratio\": val_metrics_final['sharpe'],\n",
        "            \"hcda_standard\": val_metrics_final['hcda'],\n",
        "            \"hcda_reranked\": val_metrics_final['hcda_reranked'],\n",
        "        },\n",
        "        \"test\": {\n",
        "            \"mae\": test_metrics_final['mae'],\n",
        "            \"direction_accuracy\": test_metrics_final['da'],\n",
        "            \"sharpe_ratio\": test_metrics_final['sharpe'],\n",
        "            \"hcda_standard\": test_metrics_final['hcda'],\n",
        "            \"hcda_reranked\": test_metrics_final['hcda_reranked'],\n",
        "        },\n",
        "    },\n",
        "    \"targets_met\": {\n",
        "        \"direction_accuracy\": test_metrics_final['da'] > 0.56,\n",
        "        \"hcda_reranked\": test_metrics_final['hcda_reranked'] > 0.60,\n",
        "        \"mae\": test_metrics_final['mae'] < 0.75,\n",
        "        \"sharpe_ratio\": test_metrics_final['sharpe'] > 0.80,\n",
        "        \"all_targets_met\": all_passed,\n",
        "    },\n",
        "    \"data_info\": {\n",
        "        \"train_samples\": len(X_train),\n",
        "        \"val_samples\": len(X_val),\n",
        "        \"test_samples\": len(X_test),\n",
        "        \"n_features\": len(feature_cols),\n",
        "        \"feature_cols\": feature_cols,\n",
        "    },\n",
        "}\n",
        "\n",
        "with open('training_result.json', 'w') as f:\n",
        "    json.dump(result, f, indent=2, default=str)\n",
        "\n",
        "print(\"\\nResults saved:\")\n",
        "print(\"  - training_result.json (complete metrics and metadata)\")\n",
        "print(\"  - model_seed_{42,137,256,389,512}.json (5 trained models)\")\n",
        "print(\"  - train_predictions.csv, val_predictions.csv, test_predictions.csv\")\n",
        "print(\"  - feature_importance.csv\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Finished: {datetime.now().isoformat()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}