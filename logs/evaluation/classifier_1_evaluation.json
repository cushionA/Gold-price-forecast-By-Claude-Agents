{
  "feature": "classifier",
  "attempt": 1,
  "timestamp": "2026-02-18T14:00:00",
  "evaluation_type": "classifier_standalone",
  "note": "Classifier does not follow standard Gate 2/3 submodel evaluation. Uses standalone + ensemble criteria per current_task.json.",

  "gate1_standalone_quality": {
    "passed": false,
    "checks": {
      "trivial_predictor": {
        "value": "ALL 2959 predictions are DOWN (100%)",
        "passed": false,
        "severity": "CRITICAL",
        "detail": "Model outputs P(DOWN) in [0.517, 0.559] with std=0.007. Every sample exceeds 0.5 threshold -> all predicted as DOWN. This is a degenerate trivial predictor, equivalent to np.ones(n)."
      },
      "balanced_accuracy": {
        "value": 0.50,
        "threshold": "> 0.52",
        "passed": false,
        "detail": "Balanced accuracy = 0.50 (exactly random). Train/val/test all identical because predictions are constant."
      },
      "down_recall": {
        "value": 1.0,
        "threshold": "> 0.30",
        "passed_nominal": true,
        "passed_substantive": false,
        "detail": "DOWN recall = 1.0 is MEANINGLESS because ALL predictions are DOWN. This is the same as always predicting the positive class."
      },
      "down_precision": {
        "train": 0.4858,
        "val": 0.4752,
        "test": 0.4099,
        "threshold": "> 0.45",
        "passed": false,
        "detail": "DOWN precision equals the class proportion (% of actual DOWN samples). This confirms trivial all-DOWN behavior."
      },
      "f1_down": {
        "train": 0.6539,
        "val": 0.6443,
        "test": 0.5815,
        "threshold": "> 0.35",
        "passed_nominal": true,
        "passed_substantive": false,
        "detail": "F1(DOWN) is INFLATED by trivial prediction. When recall=1.0, F1 = 2*precision / (1+precision). With 48.6% DOWN base rate, F1 = 0.654 for free. ZERO model skill."
      },
      "roc_auc": {
        "train": 0.8003,
        "val": 0.5033,
        "test": 0.5497,
        "passed": false,
        "detail": "ROC-AUC is threshold-independent, so it reveals the truth: Train=0.80 (overfit), Val=0.50 (random), Test=0.55 (weak signal). Massive train-val gap (0.30) confirms overfitting."
      },
      "prediction_variance": {
        "p_down_std": 0.007,
        "threshold": "> 0.05",
        "passed": false,
        "detail": "P(DOWN) std = 0.007 means virtually no discrimination between samples. Range [0.517, 0.559] is far too narrow."
      },
      "prediction_balance": {
        "down_pct": 100.0,
        "up_pct": 0.0,
        "threshold": "20% < DOWN < 60%",
        "passed": false,
        "detail": "100% DOWN predictions. Completely degenerate."
      },
      "overfit_check": {
        "train_auc": 0.8003,
        "val_auc": 0.5033,
        "gap": 0.2970,
        "passed": false,
        "detail": "ROC-AUC gap of 0.30 between train and val. Model learned training set noise but nothing generalizable."
      }
    }
  },

  "gate2_ensemble": {
    "passed": false,
    "skipped": true,
    "reason": "Cannot evaluate ensemble when classifier outputs constant DOWN for all samples. Ensemble would degrade to always-DOWN predictor regardless of threshold."
  },

  "root_cause_analysis": {
    "primary_cause": "OPTUNA_OBJECTIVE_FUNCTION_EXPLOIT",
    "secondary_causes": [
      "HEAVY_REGULARIZATION_SUPPRESSING_SIGNAL",
      "WEAK_FEATURE_DISCRIMINATION_ON_VALIDATION",
      "SCALE_POS_WEIGHT_DIRECTION_ERROR"
    ],
    "detailed_analysis": {
      "cause_1_optuna_objective_exploit": {
        "severity": "CRITICAL",
        "explanation": "The composite objective (40% F1_DOWN + 30% AUC + 30% balanced_acc) is MAXIMIZABLE by predicting all-DOWN. With ~48% DOWN base rate: F1_DOWN = 2*0.486/(1+0.486) = 0.654, balanced_acc = 0.50, AUC depends on model. Composite lower bound for all-DOWN = 0.40*0.654 + 0.30*0.50 + 0.30*0.50 = 0.562. This is ALREADY higher than many legitimate configurations. The objective function REWARDS trivial all-DOWN prediction.",
        "evidence": "Optuna best value = 0.567, which is barely above the 0.562 trivial floor. This means Optuna found no configuration significantly better than all-DOWN.",
        "fix_required": true,
        "fix": "Replace F1_DOWN in objective with Matthews Correlation Coefficient (MCC), which equals 0 for any trivial predictor. Or add hard penalty for prediction_std < 0.05 or down_pct > 80%."
      },
      "cause_2_heavy_regularization": {
        "severity": "HIGH",
        "explanation": "Best params: reg_lambda=6.28, reg_alpha=1.38, learning_rate=0.003, n_estimators=100. This is extremely conservative. With learning_rate=0.003 and n_estimators=100, the model takes only 100 * 0.003 = 0.3 effective steps. The regularization prevents the model from learning ANY signal, pushing all predictions toward the prior (51% DOWN -> P(DOWN) slightly above 0.5 for everything).",
        "evidence": "P(DOWN) range [0.517, 0.559] centered around 0.538, which is close to the training DOWN base rate of 48.6%. The model is outputting the prior with negligible adjustments.",
        "fix_required": true,
        "fix": "Constrain Optuna search: min learning_rate=0.01, min n_estimators=200. Reduce max reg_lambda to 3.0, max reg_alpha to 1.0."
      },
      "cause_3_weak_validation_signal": {
        "severity": "MODERATE",
        "explanation": "Val ROC-AUC = 0.50 (random). The 18 features have ZERO discriminative power on the validation set (2022-08 to 2024-05), despite having moderate power on training (AUC 0.80). This suggests either (a) feature-target relationships changed between periods, or (b) the model overfits to training noise. Test AUC = 0.55 suggests some weak signal exists in the 2024-2026 period.",
        "evidence": "Train AUC 0.80 vs Val AUC 0.50 — massive gap. But Test AUC 0.55 — some signal.",
        "fix_required": true,
                "fix": "Two approaches: (1) Use expanding window validation instead of single val split. (2) Add feature interaction terms that may be more stable across periods. (3) Consider that the val period (2022-08 to 2024-05) was a particularly unusual market regime (post-COVID rate hikes) that may not generalize."
      },
      "cause_4_scale_pos_weight_error": {
        "severity": "MODERATE",
        "explanation": "scale_pos_weight=0.81. In XGBoost, scale_pos_weight scales the positive class (UP=1). A value of 0.81 means UP class is DOWN-WEIGHTED, making DOWN class relatively MORE important. Combined with the objective function exploit, this ENCOURAGES all-DOWN prediction.",
        "evidence": "Optuna chose scale_pos_weight=0.81, which amplifies the all-DOWN incentive.",
        "fix_required": true,
        "fix": "Invert the logic: if we want better DOWN detection, we should scale_pos_weight > 1.0 (UP-weight UP to PREVENT all-DOWN) while using a better objective that rewards genuine discrimination. Or remove scale_pos_weight from search and use sample_weight explicitly."
      }
    }
  },

  "historical_pattern_analysis": {
    "this_is_a_recurring_failure_mode": true,
    "instances": [
      {
        "attempt": "meta_model attempt 8",
        "pathology": "100% positive predictions (trivial all-UP)",
        "cause": "Ridge meta-learner collapsed coefficients to near-zero"
      },
      {
        "attempt": "meta_model attempt 9",
        "pathology": "100% positive predictions (trivial all-UP)",
        "cause": "Directional loss still collapsed to constant"
      },
      {
        "attempt": "meta_model weekly attempt 1",
        "pathology": "Always positive, 10 unique predictions out of 457",
        "cause": "Regularization overwhelmed weak signal"
      },
      {
        "attempt": "backtest classification with 24 features",
        "pathology": "P(UP) always 0.46-0.51, zero discrimination",
        "cause": "Features lack directional information"
      },
      {
        "attempt": "classifier attempt 1 (this)",
        "pathology": "100% DOWN predictions, P(DOWN) 0.517-0.559",
        "cause": "Objective function exploit + heavy regularization"
      }
    ],
    "common_thread": "Models consistently collapse to trivial predictors when the signal-to-noise ratio is low. The optimization process finds that predicting the majority class (or near-majority class) minimizes the loss, and regularization prevents the model from learning the weak discriminative signal.",
    "lesson": "Any objective function that can be satisfied by a constant predictor WILL be exploited. The optimization must include EXPLICIT penalties for trivial prediction."
  },

  "feature_assessment": {
    "features_are_valid": "INCONCLUSIVE",
    "reasoning": "Cannot determine whether the 18 features have genuine discriminative power because the model never learned to use them. Train AUC=0.80 suggests they CAN discriminate in-sample. Val AUC=0.50 suggests poor generalization. Test AUC=0.55 suggests weak but non-zero OOT signal. The features MIGHT work with a properly constrained model.",
    "feature_importance_interpretation": "The well-distributed feature importance (top: 9.5%, bottom: 4.0%) is MISLEADING — it comes from the training set where AUC=0.80 (overfitting). In an all-DOWN predictor, the 'importance' reflects which features the model tried to use before regularization pushed everything toward the prior.",
    "recommendation": "Keep the 18 features for attempt 2 but add trivial-prediction safeguards. If attempt 2 with proper objective still produces val AUC < 0.52, THEN the features are fundamentally inadequate."
  },

  "improvement_plan": {
    "priority_1_fix_optuna_objective": {
      "description": "Replace composite objective with Matthews Correlation Coefficient (MCC)",
      "rationale": "MCC = 0 for ANY trivial predictor (all-UP, all-DOWN, random). MCC ranges from -1 to +1 and is the single best metric for binary classification quality. It inherently penalizes class imbalance exploitation.",
      "implementation": "objective = matthews_corrcoef(y_val, y_val_pred). If MCC < 0: return 0 (prune). Add secondary: if prediction_std < 0.03, return -1 (force reject).",
      "alternative": "If MCC alone is too harsh (many trials pruned), use MCC + 0.2*AUC. This still prevents trivial prediction while allowing gradient for AUC improvement."
    },
    "priority_2_add_trivial_prediction_guard": {
      "description": "Add hard constraints in the Optuna objective to reject trivial predictions",
      "rationale": "Even with MCC, the model might still collapse if regularization is too strong. An explicit guard prevents this.",
      "implementation": "In objective function: minority_pct = min(sum(y_pred==0), sum(y_pred==1)) / len(y_pred). If minority_pct < 0.15: return -1.0 (force prune). If pred_std < 0.03: return -1.0 (force prune)."
    },
    "priority_3_relax_regularization_bounds": {
      "description": "Tighten Optuna search space to prevent over-regularization",
      "rationale": "Current search allows reg_lambda up to 10.0, reg_alpha up to 5.0, learning_rate as low as 0.005. These extreme regularization settings push the model to the prior.",
      "implementation": {
        "learning_rate": "[0.01, 0.15] (was [0.005, 0.1])",
        "n_estimators": "[200, 800] (was [100, 500])",
        "reg_lambda": "[0.1, 3.0] (was [0.5, 10.0])",
        "reg_alpha": "[0.0, 1.0] (was [0.0, 5.0])",
        "min_child_weight": "[3, 15] (was [5, 20])",
        "colsample_bytree": "[0.5, 0.95] (was [0.4, 0.9])",
        "note": "The goal is to allow the model enough capacity to learn weak signals without over-regularizing them away."
      }
    },
    "priority_4_fix_scale_pos_weight": {
      "description": "Either remove scale_pos_weight from search or constrain it to [1.0, 2.5]",
      "rationale": "scale_pos_weight < 1 down-weights UP class, which combined with a flawed objective ENCOURAGES all-DOWN. We want the model to learn BOTH classes. If using MCC as objective, scale_pos_weight is less critical, but constraining it avoids accidental class weight interaction.",
      "implementation": "Option A: Fix scale_pos_weight = n_up / n_down (natural class balance correction). Option B: Search [0.9, 2.0] with default = 1.0."
    },
    "priority_5_add_early_stopping_on_val_auc": {
      "description": "Use ROC-AUC as early stopping metric instead of logloss",
      "rationale": "Logloss can decrease as the model converges to the prior (constant output). ROC-AUC directly measures discrimination and will not improve for trivial predictions.",
      "implementation": "eval_metric='auc', early_stopping_rounds=50. This ensures training stops when the model stops improving on actual discrimination, not just on probability calibration."
    },
    "priority_6_consider_lightgbm": {
      "description": "Try LightGBM as alternative to XGBoost",
      "rationale": "LightGBM uses leaf-wise growth (vs XGBoost's level-wise), which may find weak signals more efficiently. It also has native categorical support for day_of_week/month_of_year. Different gradient computation may avoid the same collapse mode.",
      "implementation": "Include both XGBoost and LightGBM in the Optuna search as a categorical hyperparameter (model_type). This doubles the search space but tests whether the model framework itself matters.",
      "priority": "LOW — fix the objective first, then consider model changes"
    },
    "priority_7_optional_feature_adjustments": {
      "description": "Minor feature engineering improvements for attempt 2",
      "rationale": "The current 18 features are reasonable but some may need adjustment",
      "changes": [
        "Drop rate_surprise (unsigned) — rate_surprise_signed already captures direction AND magnitude. Having both may confuse the model with correlated inputs.",
        "Add rv_ratio_5_20 (5d/20d vol ratio) — shorter window may capture faster regime shifts that 10d/30d misses.",
        "Consider replacing equity_gold_beta_20d with equity_gold_beta_5d — 20d beta is too slow-moving for daily classification.",
        "Keep calendar features despite no standalone significance — they are zero-cost and may capture interactions."
      ],
      "priority": "LOW — the objective function fix is 10x more important than feature tweaks"
    }
  },

  "attempt_2_specification": {
    "must_fix": [
      "Optuna objective: MCC-based (not F1_DOWN composite)",
      "Trivial prediction guard: reject trials with minority_pct < 15% or pred_std < 0.03",
      "Regularization bounds: learning_rate [0.01, 0.15], reg_lambda [0.1, 3.0], reg_alpha [0.0, 1.0]",
      "scale_pos_weight: [0.9, 2.0] or fixed to class ratio",
      "Early stopping metric: auc (not logloss)"
    ],
    "should_fix": [
      "Drop rate_surprise (keep only rate_surprise_signed)",
      "Add rv_ratio_5_20 as additional feature",
      "Try both XGBoost and LightGBM within Optuna"
    ],
    "keep_unchanged": [
      "18 features (core set, minus/plus minor adjustments)",
      "Data splits (70/15/15 time-series)",
      "100 Optuna trials",
      "Ensemble threshold optimization approach"
    ]
  },

  "decision": {
    "status": "attempt+1",
    "attempt_consumed": true,
    "new_attempt": 2,
    "rationale": "Clear FAIL on all substantive checks. However, the failure is caused by a fixable bug (objective function exploit + heavy regularization), NOT by fundamentally inadequate features. Train AUC=0.80 proves features contain in-sample signal. The fix (MCC objective + trivial-prediction guard + relaxed regularization) addresses the root cause directly. This is analogous to meta-model attempts 8-9 which collapsed to trivial predictors but attempt 7 (with proper configuration) succeeded. High confidence (70%+) that fixing the objective will produce non-trivial predictions in attempt 2.",
    "resume_from": "architect",
    "research_needed": false,
    "why_not_no_further_improvement": "This is attempt 1 with an identifiable, fixable bug. The feature set has not been properly tested because the model never learned to use the features. Declaring no_further_improvement would be premature — equivalent to declaring a car broken because the engine was never started."
  },

  "overall_passed": false,
  "final_gate_reached": 1
}
