{
  "feature": "meta_model_weekly",
  "attempt": 2,
  "source": "evaluator",
  "phase": "phase3_weekly_meta_model",

  "summary": "Weekly meta-model attempt 2. Attempt 1 collapsed to trivial always-positive prediction (DA = naive, 0 trades). Must fundamentally fix training methodology: non-overlapping targets, naive-aware objective, relaxed regularization, centered targets.",

  "requirements": "Redesign weekly meta-model training to prevent constant-output collapse. Primary changes: (1) train on non-overlapping 5-day periods, (2) Optuna objective must measure skill above naive, (3) wider HP ranges for expressiveness, (4) center targets to remove positive bias. Same 24 features and XGBoost architecture.",

  "previous_feedback": {
    "attempt_1_result": "FAIL -- 0/4 substantive targets passed",
    "root_cause": "Model collapsed to trivial always-positive prediction",
    "key_evidence": [
      "100% positive predictions, only 10 unique values out of 457 test samples",
      "DA = naive always-up DA in all splits (0.00pp delta on val and test)",
      "0 trades during entire test period",
      "All top-5 Optuna trials have identical val_da = 53.07% = naive",
      "Bootstrap confidence near 1.0 for all predictions (5 nearly identical models)",
      "OLS alpha = 2.76 (predictions 3x too small even after scaling)"
    ],
    "root_cause_analysis": [
      "Overlapping 5-day targets share 4/5 days with neighbors -> massive target autocorrelation",
      "XGBoost with strong regularization (max_depth=2, mcw=21) converges to unconditional mean",
      "Composite objective rewards positive predictions (train ~54% positive)",
      "Sharpe component rewards always-long during historical positive drift"
    ]
  },

  "required_changes": {
    "priority_1_non_overlapping_training": {
      "description": "Train on non-overlapping 5-day periods only (every 5th row)",
      "rationale": "Eliminates target autocorrelation. ~426 training samples is still viable for XGBoost with 24 features (17.8:1 ratio). The daily model succeeded with 2129 samples; 426 is smaller but XGBoost handles small datasets well with proper regularization.",
      "implementation": "train_idx = np.arange(0, n_train, 5); X_train = X_train_all[train_idx]; y_train = y_train_all[train_idx]",
      "val_test_handling": "Validation/test can remain overlapping for evaluation (more data points for metric stability). Non-overlapping test metrics reported separately as secondary diagnostic."
    },
    "priority_2_naive_aware_objective": {
      "description": "Optuna objective must penalize naive-matching and measure skill above baseline",
      "rationale": "Current objective rewards raw DA. Since majority of returns are positive, always-up maximizes raw DA. Objective must measure DA minus naive DA.",
      "implementation": [
        "naive_da_val = (y_val > 0).mean()",
        "da_skill = max(0, val_da - naive_da_val)",
        "da_norm = np.clip(da_skill * 100 / 15.0, 0.0, 1.0)",
        "Add penalty: if da_skill < 0.001: objective -= 0.15",
        "Sharpe: measure excess return over buy-and-hold, not raw strategy return"
      ]
    },
    "priority_3_relaxed_regularization": {
      "description": "Wider HP ranges for weekly targets",
      "new_ranges": {
        "max_depth": "[2, 6] (was [2, 4])",
        "min_child_weight": "[5, 25] (was [12, 25])",
        "reg_lambda": "[0.1, 15.0] (was [1.0, 15.0])",
        "reg_alpha": "[0.01, 10.0] (was [0.5, 10.0])"
      },
      "caution": "Only safe because non-overlapping targets reduce overfitting risk"
    },
    "priority_4_centered_targets": {
      "description": "Subtract training mean from targets",
      "implementation": "train_mean = y_train.mean(); y_centered = y - train_mean; add back at prediction time"
    }
  },

  "constraints": [
    "Same 24 features (5 base + 19 submodel outputs) as attempt 1",
    "Same XGBoost architecture (reg:squarederror)",
    "Same Kaggle dataset (bigbigzabuton/gold-prediction-submodels)",
    "Same data split (70/15/15 time-series)",
    "Bootstrap ensemble (5 models) retained but check diversity",
    "OLS scaling retained but verify alpha is reasonable",
    "Must produce both positive and negative predictions",
    "Must have non-trivial number of trades (position changes > 5 in test)",
    "DA on validation must exceed naive by at least 1pp to be considered valid"
  ],

  "success_criteria": {
    "minimum": "DA > naive + 1pp on both val and test. At least 10 trades in test period.",
    "target": "3/4 weekly targets (DA > 56%, HCDA > 60%, MAE < 1.70%, Sharpe > 0.80) with substantive skill above naive",
    "validation": "Model must produce meaningful prediction variation (std > 0.1 on centered predictions)"
  },

  "created_at": "2026-02-17T12:00:00",
  "created_by": "evaluator"
}
