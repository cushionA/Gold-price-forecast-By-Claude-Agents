{
  "feature": "temporal_context",
  "attempt": 1,
  "source": "entrance",
  "phase": "phase2_submodel",

  "summary": "Transformerベースの時系列文脈サブモデル。既存23特徴量のうち文脈依存性の高い特徴量群の過去ウィンドウを処理し、XGBoostが単一時点からは捕捉できないクロス時間・クロス特徴量の相互作用パターンを抽出する。",

  "requirements": "meta_modelの23特徴量のうち、時系列文脈が最も重要な特徴量群（base features 5個 + 選択的submodel outputs）の過去N日ウィンドウを入力とし、Transformer self-attentionにより多時点間の関係性を捕捉する。出力は1-3個の低次元の文脈埋め込み特徴量。既存HMM/統計的サブモデルとの差別化は、固定ウィンドウ・固定状態数の制約を超えた適応的な時系列パターン認識にある。",

  "background_analysis": {
    "current_meta_model_limitation": "XGBoostは各行（各日）を独立に処理する。既存サブモデルはHMM regime_prob, rolling z-score, persistence等で一部の時系列情報を圧縮しているが、(1) 固定ウィンドウ長（20d/60d）に制約される、(2) 特徴量間の時系列相互作用を捉えられない、(3) HMMの状態数が2-3に固定されレジーム遷移の複雑なダイナミクスを表現できない。",

    "feature_context_dependency_ranking": {
      "high_dependency": [
        {
          "feature": "cross_asset (xasset_regime_prob, xasset_recession_signal, xasset_divergence)",
          "reason": "金・銀・銅の相関構造は時間的に進化する。相関の崩壊・回復は数日から数週間かけて進行し、その遷移パターンが金の将来リターンに影響する。現在のHMM 3-stateは静的なスナップショットのみで遷移速度・方向を捉えられない。Gate 3でDA +0.76%、MAE -0.0866と強い結果だが、遷移ダイナミクスの追加でさらなる改善余地がある。",
          "current_submodel": "Deterministic HMM 3-state on [gold, silver, copper] daily returns",
          "gate3_mae_delta": -0.0866,
          "feature_importance_rank": "xasset_divergence #6, xasset_regime_prob #8"
        },
        {
          "feature": "vix (vix_regime_probability, vix_mean_reversion_z, vix_persistence)",
          "reason": "VIXスパイクの形状（急騰→段階的減衰 vs 徐々に上昇→急落）は金への影響が異なる。現在のHMMは瞬間的なレジームのみで、スパイクの進行段階・減衰速度を区別できない。vix_persistenceは20d固定ウィンドウの自己相関のみで、適応的なパターン認識ではない。",
          "current_submodel": "HMM 2-3 state on log-VIX changes + rolling z-score + autocorrelation",
          "gate3_da_delta": "+0.96%",
          "feature_importance_rank": "vix #7"
        },
        {
          "feature": "technical (tech_trend_regime_prob, tech_mean_reversion_z, tech_volatility_regime)",
          "reason": "金のモメンタム/ミーンリバージョン遷移は可変長で発生する。20d z-scoreでは10日で完了するミーンリバージョンも30日かけて進む遷移も同じ扱い。ボラティリティ・クラスタリングのパターンも時間的文脈に依存する。",
          "current_submodel": "HMM on [returns, GK_vol] + 20d z-score + daily GK vol z-score",
          "gate3_mae_delta": -0.1824,
          "feature_importance_rank": "tech_trend_regime_prob #2, tech_mean_reversion_z #4, tech_volatility_regime #5"
        }
      ],
      "medium_dependency": [
        {
          "feature": "etf_flow (etf_regime_prob, etf_capital_intensity, etf_pv_divergence)",
          "reason": "ETFフローの蓄積パターン（数日にわたる持続的流入/流出）は重要だが、現在のHMMで一定程度捕捉済み。Sharpe +0.377と最も強いSharpe改善サブモデル。",
          "gate3_sharpe_delta": "+0.377"
        },
        {
          "feature": "options_market (options_risk_regime_prob)",
          "reason": "オプション市場のセンチメント変化は数日かけて展開する。単一カラム出力で情報量が限定的だが、feature importance #3。",
          "feature_importance_rank": "#3"
        }
      ],
      "low_dependency": [
        {
          "feature": "base features (real_rate_change, dxy_change, yield_spread_change, inflation_exp_change)",
          "reason": "日次変化量は本質的にステートレス。ただしこれらの変化の累積パターン（3日連続の金利上昇等）は文脈情報を持つ。base featuresそのものではなくbase features + high-dependency submodel outputsの組み合わせのウィンドウを処理すべき。"
        },
        {
          "feature": "yield_curve, inflation_expectation, cny_demand",
          "reason": "マクロ特徴量は長期的な傾向が支配的で、日次Transformerの短期ウィンドウ（5-20日）からの追加情報は限定的。既存z-score/regime_probで十分に圧縮されている。"
        }
      ]
    },

    "transformer_vs_hmm_differentiation": {
      "hmm_limitations": [
        "状態数が固定（2-3）: 複雑な市場レジームの連続体を離散的にしか表現できない",
        "マルコフ性: 現在の状態は直前の状態のみに依存し、より長期の文脈を無視",
        "単一特徴量: 各HMMは1つの特徴量群のみを処理し、特徴量間の時系列相互作用を捉えられない",
        "固定emission分布: ガウス仮定により非正規な市場ダイナミクスを見逃す"
      ],
      "transformer_advantages": [
        "Self-attention: ウィンドウ内の任意の2時点間の関係を直接モデリング",
        "可変長パターン: 固定ウィンドウではなく、attentionが重要な時点に自動的に集中",
        "クロス特徴量: 複数特徴量の時系列を同時に処理し、時間的相互作用を捕捉",
        "連続的表現: 離散的なレジームではなく連続的な埋め込みベクトルを出力",
        "非線形パターン: HMMのガウスemissionでは捕捉できない複雑な分布パターンを学習可能"
      ]
    },

    "why_not_replace_meta_model": "XGBoostメタモデルをTransformerに置き換えるのではなく、サブモデルとしてTransformerを追加する理由: (1) XGBoostは2131サンプルで安定した学習が可能だが、Transformerメタモデルは過学習リスクが高い、(2) 既存のサブモデル体系（HMM regime + z-score + 統計量）との補完関係を維持できる、(3) Transformerサブモデルの出力は1-3次元に圧縮されるため、XGBoostの特徴量数を大幅に増やさない、(4) Gate 1/2/3評価フレームワークが使える"
  },

  "proposed_approach": {
    "architecture": "Temporal Context Transformer",
    "input_features": {
      "primary_candidates": [
        "real_rate_change (base)",
        "dxy_change (base)",
        "vix (base)",
        "yield_spread_change (base)",
        "inflation_exp_change (base)",
        "vix_regime_probability",
        "vix_mean_reversion_z",
        "tech_trend_regime_prob",
        "tech_mean_reversion_z",
        "tech_volatility_regime",
        "xasset_regime_prob",
        "xasset_divergence",
        "etf_regime_prob",
        "options_risk_regime_prob"
      ],
      "total_input_dim": "14 (5 base + 9 selected submodel outputs)",
      "selection_rationale": "高文脈依存性の特徴量を選択。yield_curve, inflation_expectation, cny_demandのsubmodel出力は除外（低文脈依存性、マクロ長期傾向が支配的）。persistence, capital_intensity等の補助的特徴量も除外（VIFリスクと情報冗長性のため）。"
    },
    "window_size": "researcher/architectが決定。候補: 5日、10日、20日。Optuna探索可能にする。短すぎると文脈不足、長すぎると学習困難。",
    "output": {
      "n_features": "1-3 (architectが決定)",
      "format": "連続値の文脈埋め込みベクトル。日次。",
      "interpretation": "XGBoostに提供される「時系列文脈スコア」。例: temporal_context_1 = 複数特徴量の同期的変動度合い、temporal_context_2 = レジーム遷移の加速/減速"
    },
    "training_approach": {
      "method": "教師なし or 自己教師あり（サブモデルは金相場を予測しない原則に従う）",
      "candidates": [
        "自己教師あり: 過去ウィンドウから次時点の特徴量ベクトルを予測（masked token prediction的アプローチ）",
        "教師なし: Transformer Autoencoder（入力ウィンドウの再構成、ボトルネック層を文脈特徴量として使用）",
        "半教師あり: 再構成損失 + 補助タスク（次時点特徴量予測）の組み合わせ"
      ],
      "recommendation_for_researcher": "自己教師ありアプローチを優先調査。理由: (1) 再構成損失は安定した学習を保証、(2) ボトルネック層が自然に低次元文脈表現を生成、(3) 金相場予測を含まないため原則に準拠"
    }
  },

  "research_questions": [
    "時系列Transformer Autoencoder（TAE）のアーキテクチャ設計: 金融時系列に適した注意機構の選択肢は？（vanilla self-attention vs causal attention vs sparse attention）",
    "ウィンドウサイズの決定方法: 金融日次データにおける最適なコンテキストウィンドウ長に関する実証研究は？5日、10日、20日の理論的根拠は？",
    "Transformerの学習安定化: 2000-3000サンプルの小規模データセットでTransformerを安定的に学習させる手法は？（事前学習、データ拡張、正則化戦略）",
    "Position Encoding: 金融時系列に適した位置エンコーディングは？（固定 sinusoidal vs 学習可能 vs 相対位置エンコーディング）",
    "出力次元の決定: Autoencoderのボトルネック次元を1-3に設定する際の情報量保持と過学習防止のトレードオフに関する指針は？",
    "既存手法との比較: 時系列文脈抽出において、Transformer Autoencoder vs Temporal Convolutional Network (TCN) vs LSTM Autoencoder の性能比較研究は？小規模データでの優位性は？",
    "多変量時系列におけるchannel-independent vs channel-dependent Transformerの選択基準は？PatchTST等の最新アーキテクチャとの比較は？",
    "学習目標の設計: masked token prediction vs full reconstruction vs contrastive learning -- 金融時系列の文脈抽出に最適な自己教師あり目標は？"
  ],

  "constraints": [
    "サブモデルは金相場を予測しない（自己教師あり/教師なしアプローチのみ）",
    "5日以上遅延するデータは使用禁止（全データは日次で利用可能なので該当なし）",
    "出力は1-3カラム以内（Gate 2 VIF制約、real_rate attempt 5で7カラムが失敗した教訓）",
    "出力の自己相関 < 0.99（Gate 1制約）",
    "Kaggle GPU環境で30分以内に学習完了（Optuna 20-30 trials）",
    "PyTorchで実装（CLAUDE.md準拠）",
    "既存サブモデル出力（HMM regime_prob等）を入力として使用可能だが、それらを単にコピーするだけの出力は禁止（情報利得が必要）",
    "学習データ2131サンプルでTransformerが過学習しないための正則化必須（Dropout, LayerNorm, weight decay, 小モデルサイズ）",
    "base_features.csvとsubmodel_outputs/*.csvの既存データパイプラインを再利用",
    "入力特徴量はbase_features + submodel_outputsから選択（新規データ取得不要）"
  ],

  "success_hypothesis": "時系列文脈特徴量（1-3次元）を追加することで、meta_modelの方向精度（DA）を+0.5%以上改善する。根拠: (1) XGBoostは各行独立処理のため、複数日にわたるレジーム遷移パターンを捉えられない、(2) 既存submodelのregime_prob等は瞬間スナップショットで遷移速度・方向を表現しない、(3) meta_model attempt 5でtech_trend_regime_prob(#2), options_risk_regime_prob(#3), tech_mean_reversion_z(#4)が上位feature importanceであり、これらの時系列パターンは更に有用な情報を含む可能性がある。",

  "risk_assessment": {
    "primary_risk": {
      "description": "2131サンプルでTransformerが過学習し、有意義な文脈特徴量を生成できない",
      "probability": "30-40%",
      "mitigation": "極小モデル（1-2層、32-64次元、2-4ヘッド）、強いDropout(0.3-0.5)、weight decay、early stopping。real_rate attempt 2-3でのTransformer/GRU経験を活かす。"
    },
    "secondary_risk": {
      "description": "Transformer出力がHMM regime_probと高相関で独立情報が不足（Gate 2 MI < 5%、VIF > 10）",
      "probability": "25-35%",
      "mitigation": "入力にsubmodel outputsを含めるが、出力がそれらと直交するようにcorrelation regularization損失を追加する方向を検討。出力がregime_probの単なるスムージングにならないよう、masked prediction等の複雑な学習目標を採用。"
    },
    "tertiary_risk": {
      "description": "Gate 3で既存submodel outputsとの冗長性により、XGBoostに追加価値を提供できない",
      "probability": "20-30%",
      "mitigation": "出力を1カラムに絞り、最もDA/Sharpe改善に寄与する次元のみを使用。real_rate/options_marketの教訓: 少ないカラム数が安定した改善をもたらす。"
    }
  },

  "previous_feedback": {
    "real_rate_transformer_attempt_3": {
      "result": "Gate 1 PASS, Gate 2 PASS (MI +23.8%), Gate 3 FAIL (DA marginal miss -0.48%, MAE degradation)",
      "lessons": [
        "multi-country Transformerは過学習を解決した（overfit 1.28）",
        "MI +23.8%は高い情報利得だが、forward-fillされたstep functionがMAEを悪化",
        "月次データ→日次補間が根本的な問題だった（今回は全て日次データで問題なし）",
        "30 Optuna trialsで十分な探索が可能だった"
      ]
    },
    "real_rate_attempts_overall": {
      "key_learning": "Gate 2は常にPASS（MI +10-39%）だがGate 3は常にFAIL。月次マクロ→日次の周波数ミスマッチが根本原因。今回は全入力が日次なのでこの問題は解消。",
      "relevance": "temporal_contextサブモデルは全入力が日次であるため、real_rateの根本的失敗原因は該当しない。"
    },
    "submodel_output_column_count": {
      "key_learning": "出力カラム数が多いほどXGBoostにノイズを与える（real_rate 7カラム→Gate 3全失敗、options_market 3カラム→Gate 3失敗、1カラムに絞ると成功）。1-2カラムが最適。",
      "recommendation": "temporal_context出力は1-2カラムに厳格に制限"
    },
    "hmm_success_pattern": {
      "key_learning": "Gate 3をPASSした8サブモデル全てがHMM regime_probを使用。MAE改善が最も安定した指標（DA/Sharpeは変動が大きい）。",
      "implication": "Transformer出力もregime-like（連続的な状態確率）の形式が望ましい。0-1正規化されたスコアが最も安定。"
    }
  },

  "meta_model_attempt_6_context": {
    "status": "Kaggle training中（submitted 2026-02-16T04:38:48）",
    "changes": "SHAP confidence scoring, OLS output scaling, 強化正則化",
    "expected_outcome": "HCDA改善が主目標。DA/Sharpeは維持。MAEは改善困難。",
    "relevance_to_this_task": "attempt 6の結果に依存せずtemporal_contextサブモデルの設計・開発を並行して進める。attempt 6完了後にtemporal_contextを組み込んだattempt 7を構築する。"
  },

  "execution_plan": {
    "step_1": "researcher: 上記research_questionsを調査",
    "step_2": "architect: fact-check → アーキテクチャ設計（入力特徴量選択、ウィンドウサイズ、モデルサイズ、学習目標、HP探索空間）",
    "step_3": "builder_data: 既存データパイプラインから入力ウィンドウデータセット生成",
    "step_4": "datachecker: 7-step check",
    "step_5": "builder_model: self-contained Kaggle Notebook生成",
    "step_6": "Kaggle submission → training",
    "step_7": "evaluator: Gate 1/2/3評価"
  },

  "created_at": "2026-02-16T14:00:00",
  "created_by": "entrance"
}
