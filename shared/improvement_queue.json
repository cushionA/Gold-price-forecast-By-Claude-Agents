{
  "feature": "meta_model",
  "attempt": 9,
  "items": [
    {
      "priority": 1,
      "type": "architecture_revert",
      "description": "Revert to single XGBoost architecture from attempt 7. Use the same 24 base features (no regime features). The stacking approach was a proven failure -- Ridge meta-learner collapsed predictions to constant. Single XGBoost with attempt 7 hyperparameter strategy (100 Optuna trials, max_depth 3-6, reg:squarederror) is the validated foundation.",
      "reason": "Attempt 7 single XGB: DA 60.04%, HCDA 64.13%, Sharpe 2.46. Attempt 8 stacking: DA 58.73% (=naive), HCDA 61.96%, Sharpe 2.06. Stacking was strictly worse on all metrics. Do not repeat.",
      "resume_from": "architect",
      "research_needed": false
    },
    {
      "priority": 2,
      "type": "loss_function",
      "description": "Replace reg:squarederror with a custom directional-aware loss function. Penalize wrong-direction predictions 2x more heavily than correct-direction errors. This directly targets the DA metric and should help the model learn when gold will decline (the key skill that differentiates from naive always-up). Implementation: custom XGBoost objective with gradient/hessian that weights sign-mismatched errors higher.",
      "reason": "Attempt 7 DA=60.04% beats naive by only 1.31pp. The model predicts 87.3% positive but actual positive rate is only 58.7%. Model needs to learn negative returns better. An asymmetric loss that heavily penalizes predicting positive when actual is negative should improve negative-day DA without destroying positive-day DA.",
      "resume_from": "architect",
      "research_needed": false
    },
    {
      "priority": 3,
      "type": "feature_engineering",
      "description": "Add 4-6 continuous interaction features (NOT binary regime indicators). Examples: real_rate_change * vix_level, dxy_change * yc_curvature_z, inflation_exp_change * xasset_recession_signal. These capture how feature effects vary with market conditions using continuous values (not sparse binary flags). Keep total feature count under 30 to avoid curse of dimensionality.",
      "reason": "Attempt 8 binary regime features failed catastrophically: high-vol active in 0% of samples, 5/6 had zero importance. Continuous interactions provide information across ALL samples instead of only during rare regime activations. The key is multiplicative interactions between base features and submodel regime indicators.",
      "resume_from": "architect",
      "research_needed": false
    },
    {
      "priority": 4,
      "type": "hpo_strategy",
      "description": "Use multi-objective Optuna (DA + Sharpe as dual objectives) with NSGA-II sampler. Select from Pareto front: choose the trial that maximizes DA while maintaining Sharpe > 1.5. This allows principled exploration of the DA-Sharpe tradeoff space instead of a single composite score.",
      "reason": "Attempt 7 used composite objective. Attempt 8's 280-trial HPO found suboptimal base models (DA 50-52%). Multi-objective Optuna may find trials with higher DA at acceptable Sharpe cost.",
      "resume_from": "architect",
      "research_needed": false
    }
  ],
  "previous_feedback": {
    "attempt": 8,
    "gate1_passed": false,
    "gate3_nominal_passed": "3/4",
    "gate3_substantive_passed": "0/4",
    "root_cause": "Ridge meta-learner with alpha=90.37 collapsed stacking predictions to near-constant output (std=0.000107, 100% positive). Ridge intercept dominated; all base model coefficients near-zero. Model has zero skill beyond naive always-up strategy.",
    "key_evidence": [
      "Prediction std 0.000107 (211x smaller than attempt 7's 0.0226)",
      "100% positive predictions (vs 87.3% in attempt 7)",
      "DA exactly equals naive: 58.73% = 58.73%, delta = 0.0pp",
      "Ridge alpha 90.37 (extreme regularization), coefficients: XGB=0.0004, LGBM=0.006, CB=0.005",
      "Base models all near coin-flip: XGB 52.09%, LGBM 50.77%, CB 52.53%",
      "5 of 6 regime features had zero importance",
      "High-vol regime active in 0.0% of samples (threshold too aggressive)",
      "Base model correlation high: XGB-LGBM r=0.775, XGB-CB r=0.570, LGBM-CB r=0.733"
    ],
    "what_failed_in_attempt_8": [
      "Stacking architecture: base models too correlated for ensemble benefit",
      "Ridge meta-learner: extreme L2 penalty collapsed signal",
      "Binary regime features: sparse activation (0% high-vol, 0.4% trend) rendered useless",
      "Feature expansion from 24 to 30: added noise without information"
    ],
    "anti_patterns": [
      "DO NOT use stacking/ensemble meta-learners (Ridge collapse proven)",
      "DO NOT use binary regime indicators with sparse activation (<5%)",
      "DO NOT try prediction calibration (proven failure in attempts 3-4)",
      "DO NOT expand features beyond 28 (curse of dimensionality with 2132 training samples)"
    ]
  }
}
