"""
Fetch and engineer real_rate features for Attempt 2
Reuses validated logic from Attempt 1
Generated by builder_data agent
"""

import pandas as pd
import numpy as np
from fredapi import Fred
import yfinance as yf
import os
from scipy.stats import percentileofscore
from datetime import datetime


def fetch_real_rate_features():
    """
    Self-contained data fetching and feature engineering.
    No external file dependencies.
    Returns: DataFrame with 8 engineered features
    """
    # === Load API key from environment ===
    api_key = os.environ.get('FRED_API_KEY')
    if api_key is None:
        try:
            # Try loading from .env file if not already loaded
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.environ.get('FRED_API_KEY')
        except ImportError:
            pass

    if api_key is None:
        raise RuntimeError(
            "FRED_API_KEY not found in environment. "
            "Please set it in .env file or environment variables."
        )

    fred = Fred(api_key=api_key)

    # === Fetch DFII10 from FRED ===
    print("Fetching DFII10 from FRED...")
    # Fetch from 2013-06-01 to allow 252-day rolling windows before schema start (2015-01-30)
    series = fred.get_series('DFII10', observation_start='2013-06-01')

    df = pd.DataFrame({'level': series})
    df.index = pd.to_datetime(df.index)
    df.index.name = 'Date'

    # Drop NaN rows (holidays)
    df = df.dropna()
    print(f"FRED data shape after dropping NaN: {df.shape}")

    # === Align with gold trading days ===
    print("Fetching gold trading days from yfinance...")
    gold = yf.download('GC=F', start='2013-06-01', progress=False)
    gold_dates = pd.DatetimeIndex(gold.index)

    # Reindex to gold trading days and forward-fill (max 5 days)
    df = df.reindex(gold_dates)
    df['level'] = df['level'].ffill(limit=5)
    df = df.dropna()
    print(f"Data shape after alignment to gold trading days: {df.shape}")

    # === Feature Engineering ===
    print("Computing 8 hand-crafted features...")

    # Feature 2: Daily change
    df['change_1d'] = df['level'].diff()

    # Rolling std for normalization (60-day window)
    rolling_std_60d = df['change_1d'].rolling(60, min_periods=60).std()

    # Feature 3: Velocity 20-day (normalized)
    df['velocity_20d'] = (df['level'] - df['level'].shift(20)) / rolling_std_60d

    # Feature 4: Velocity 60-day (normalized)
    df['velocity_60d'] = (df['level'] - df['level'].shift(60)) / rolling_std_60d

    # Feature 5: Acceleration (change in 20-day velocity)
    df['accel_20d'] = df['velocity_20d'] - df['velocity_20d'].shift(20)

    # Feature 6: Rolling std 20-day
    df['rolling_std_20d'] = df['change_1d'].rolling(20, min_periods=20).std()

    # Feature 7: Regime percentile (252-day rolling percentile rank)
    def percentile_rank(x):
        if len(x) < 2:
            return np.nan
        return percentileofscore(x, x.iloc[-1]) / 100.0

    df['regime_percentile'] = df['level'].rolling(252, min_periods=252).apply(percentile_rank, raw=False)

    # Feature 8: Autocorrelation of daily changes (60-day window, lag 1)
    def rolling_autocorr(x):
        if len(x) < 2:
            return np.nan
        try:
            return pd.Series(x).autocorr(lag=1)
        except:
            return np.nan

    df['autocorr_20d'] = df['change_1d'].rolling(60, min_periods=60).apply(rolling_autocorr, raw=False)

    # Drop rows with NaN from rolling calculations
    initial_rows = len(df)
    df = df.dropna()
    print(f"Dropped {initial_rows - len(df)} rows due to rolling window NaN")

    # === Align to schema date range ===
    schema_start = '2015-01-30'
    schema_end = '2025-02-12'

    # First check if we have data covering the schema range
    if df.index.min() > pd.Timestamp(schema_start):
        raise ValueError(f"Insufficient data buffer. First valid date: {df.index.min()}, "
                        f"but schema starts at {schema_start}")

    df = df.loc[schema_start:schema_end]

    # Verify NO NaN in final output
    if df.isna().any().any():
        nan_count = df.isna().sum().sum()
        print(f"WARNING: {nan_count} NaN values remain in schema range")
        print("Dropping rows with NaN...")
        df = df.dropna()

    print(f"Final shape after schema alignment: {df.shape}")
    print(f"Date range: {df.index.min()} to {df.index.max()}")
    print(f"NaN count: {df.isna().sum().sum()}")

    # Verify we have 8 feature columns
    feature_cols = ['level', 'change_1d', 'velocity_20d', 'velocity_60d',
                    'accel_20d', 'rolling_std_20d', 'regime_percentile', 'autocorr_20d']
    assert all(col in df.columns for col in feature_cols), "Missing feature columns"

    return df[feature_cols]


if __name__ == "__main__":
    print("=" * 60)
    print("Real Rate Feature Fetching - Attempt 2")
    print("=" * 60)
    print(f"Started: {datetime.now().isoformat()}\n")

    # Fetch data
    df = fetch_real_rate_features()

    # === Quality Checks ===
    print("\n" + "=" * 60)
    print("Quality Checks")
    print("=" * 60)

    print(f"\n1. Shape: {df.shape}")
    expected_rows = 2523
    if abs(df.shape[0] - expected_rows) > 10:
        print(f"   WARNING: Expected ~{expected_rows} rows, got {df.shape[0]}")
    else:
        print(f"   [OK] Row count within expected range")

    print(f"\n2. Date range: {df.index.min()} to {df.index.max()}")
    schema_start = pd.Timestamp('2015-01-30')
    schema_end = pd.Timestamp('2025-02-12')
    if df.index.min() <= schema_start and df.index.max() >= schema_end:
        print(f"   [OK] Date range covers schema requirements")
    else:
        print(f"   WARNING: Schema requires {schema_start} to {schema_end}")

    print(f"\n3. NaN count: {df.isna().sum().sum()}")
    if df.isna().sum().sum() == 0:
        print(f"   [OK] No NaN values")
    else:
        print(f"   WARNING: NaN values present!")
        print(df.isna().sum())

    print(f"\n4. Feature count: {len(df.columns)} columns")
    if len(df.columns) == 8:
        print(f"   [OK] Expected 8 features")
    else:
        print(f"   WARNING: Expected 8 features, got {len(df.columns)}")

    print(f"\n5. Data types:")
    print(df.dtypes)
    if all(df.dtypes == 'float64'):
        print(f"   [OK] All numeric (float64)")
    else:
        print(f"   WARNING: Non-numeric types found")

    print(f"\n6. Sanity check (level statistics):")
    level_mean = df['level'].mean()
    level_std = df['level'].std()
    print(f"   Mean: {level_mean:.4f}%")
    print(f"   Std:  {level_std:.4f}%")
    if 0.0 < level_mean < 3.0 and level_std > 0.1:
        print(f"   [OK] Reasonable TIPS yield range")
    else:
        print(f"   WARNING: Unexpected yield values")

    # === Display Sample Data ===
    print("\n" + "=" * 60)
    print("Sample Data")
    print("=" * 60)

    print("\nFirst 5 rows:")
    print(df.head())

    print("\nLast 5 rows:")
    print(df.tail())

    print("\nDescriptive statistics:")
    print(df.describe())

    # === Save ===
    print("\n" + "=" * 60)
    print("Saving")
    print("=" * 60)

    output_path = "data/processed/real_rate_features.csv"
    df.to_csv(output_path)
    print(f"\n[OK] Saved: {output_path}")

    # Verify file exists and is readable
    if os.path.exists(output_path):
        file_size = os.path.getsize(output_path) / 1024
        print(f"  File size: {file_size:.1f} KB")
        print(f"\n[OK] All checks passed!")
    else:
        print(f"\nERROR: File was not created!")

    print("\n" + "=" * 60)
    print("Complete!")
    print("=" * 60)
    print(f"Finished: {datetime.now().isoformat()}")
